{"pages":[{"text":"Motivation In some of my other projects , I've needed to make extensive use of the Stack Exchange API. I built a small library - StackAPI - to assist in this task and released it on Python's PyPI repository . This post is going to cover some of the technical decisions and issues I ran into while going through this process. This was my first project being released to PyPI. My goals when releasing this were: Clean up my own code so that it is usable by others Improve the documentation and host the documentation on Read the Docs Automatically release it to PyPI, if it passes basic tests Those goals sound simple. In the future, they probably will be, but for this first release it wasn't as simple as I was hoping. Project Layout Before this project, I'd written modules and libraries that were used by myself (for personal projects) or as part of a larger application (for work). In both cases, though, I had a directory structure that looked something like this: /project_root /mymodule __init__.py mymodule.py __init__.py This was close to the end goal, but lacked some files in the project_root that were needed for a proper install via pip . The important file that was missing was setup.py . I needed this file to ensure that everything would install with a simple pip install stackapi setup.py My setup.py is pretty basic and available on GitHub. There are a couple important things though. version : This is needed, but I didn't want to have to constantly remember to update this when pushing a version to PyPI. This was one of my first criteria when starting the project. I wanted to automate as much as I could, and versioning was at the top of that list. It's small, but easy to forget and keep syncronized across all the files in the project. I decided to utilize bumpversion and Fabric to manage this specific field (both here and elsewhere in the project). install_requires : StackAPI is built in the fantasitc Requests library. To ensure this was installed when StackAPI was install, it was needed in this field. tests_require : The testsuite I build utilizes the mock library. I don't want that to be installed if the developer isn't running the tests, so it is added to this field. test_suite : I wanted developers to be able to run python setup.py test to execute the test suite. To do so, I had to point to the where the tests were being executed from. The rest of setup.py seemed to be fairly standard when compared to other Python libraries. Bumpversion and Fabric As I mentioned, I wanted to automate any versioning that was required. To do so, I used the bumpversion library and wrote a small Fabric script to handle it automatically. bumpversion uses a config file , to determine what it is going to do. I configured it to automatically create a commit and a new git tag for each version. I then pointed to a couple files where the current version is listed. When bumpversion is executed, it will change the version in each of those files to the new version. It will then create a single commit to the git repository with a commit message similar to Bump version: 0.1.6 â†’ 0.1.7 That is nice and clean. It tags the commit for me, which is useful later, when I want to push the change to PyPI. To make running bumpversion a bit easier, I utilized a Fabric routine I found online and adjusted it for my purposes. When I run fab release , all of the bumpversion 'stuff' occurs. Then I just have to push the commit (and new tag) to GitHub. Final Project Layout The final project layout I settled on was this: /stackapi /docs ... /stackapi __init__.py stackapi.py /tests .gitignore .travis.yml fabfile.py setup.cfg setup.py Read the Docs Configure the project In the final project layout, you can see there is a docs directory. One of my goals was to make this library usable and understandable by other developers. A good part of that means having decent documentation. I spent way more time than I expected cleaning the documentation in the code and creating documentation with examples. Most of that time was spent learning the sphinx documentation style and ReStructuredText, which Read the Docs utilizes. The first step in this process was installing and setting up the initial configuration for the documentation: pip install sphinx sphinx-autobuild Then, I created the docs directory and switched to it and ran: sphinx-quickstart This starts a short, interactive, wizard. Fill out the questions. At the end of this, it creates a conf.py file in the docs directory. The rest of the documentation is ReStructuredText files. To see how the documentation looks, from the docs directory, run: make html This creates a _build directory. If you open _build/html/index.html , the documentation can be browsed locally. I do not commit this directory to git, though. It is ignored in .gitignore , as a user can regenerate it at will. Configure Read the Docs Once I was satisfied with how the documentation looked, I had to configure Read the Docs to read my GitHub repository. To repeat those steps: Sign up (or log in) at Read the Docs (part of this will be associating the account to a GitHub account) Visit your dashboard and click \"Import a project\" Fill out the form, but in my case the defaults were all appropriate. Do note that URLs are case sensitive. Click \"Create\". This is the first version of your documentation. To keep the code updating as you update GitHub, log into GitHub and go to the repository's \"Settings\" page. Click \"Webhooks & Services\" Click \"Add Service\" Select \"ReadTheDocs\" and add the service At this point, each time you push a change to the repository, a new set of documents will be built. I then added the Read the Docs badge to my README.rst for a simple link to the detailed documentation. .. image :: https://readthedocs.org/projects/stackapi/badge/?version=latest :target: http://stackapi.readthedocs.org/en/latest/?badge=latest :alt: Documentation Status Force rebuild of docs Toward the very end of this project, Read the Docs had a minor hiccup and failed on building my documentation. I didn't want to force a build by making a fake commit. Instead, Read the Docs provides the information needed to force a rebuild. It requires a very simple POST to the Post Commit Hook they provide. In my case, this was as simple as running this command (provided from the Dashboard): curl -X POST https://readthedocs.org/build/stackapi PyPI Nearing the end of the journey, it was time to see what exactly PyPI required. The first step was setting up an account on both the Test and Production instances of PyPI. PiPY Test: http://testpypi.python.org/pypi?%3Aaction=register_form PyPI Live: https://pypi.python.org/pypi?%3Aaction=register_form Having one on both was important while testing. It meant that I didn't have to send broken versions to the live PyPI server, and I could adjust ReStructuredText formatting issues without requiring another release to PyPI. Each time a version is pushed to PyPI it must have a new version number. By using the test instance, I could use as many of these fake versions as needed to fix things. Hooray for test environments! Before we perform this step automatically, we need to test that the PyPI accounts work. By following portions of a \"First Time with PyPI\" tutorial, I focused by steps down to these: Create a .pypirc file in your home directory - not your project directory. This won't be required once Travis CI is set up and configured, so having the passwords in this, temporarily, wasn't an issue because I eventually deleted the file. [distutils] index-servers = pypi pypitest [pypi] repository=https://pypi.python.org/pypi username=your_username password=your_password [pypitest] repository=https://testpypi.python.org/pypi username=your_username password=your_password Register the package on PyPI Test: python setup.py register -r pypitest Register the package on PyPI Live: python setup.py register -r pypi Upload the project to Test: python setup.py register -r pypitest Upload the project to Live, if you're ready for your first release. Remember, once a version is released to PyPI, it can't be used again (or overwritten): python setup.py register -r pypi If all of the above passed to your satisfaction, you can remove the .pypirc file and move on to configuring Travis CI. Travis The last step in this process will be using Travis CI to perform some basic tests and, if this was a new release, push the changes to PyPI. The Travis config file is available on GitHub . My goal is to support 'modern' Python with this library. I've configured Travis to test against multiple versions of Python, ranging from 2.7 to 3.5. StackAPI is installed using python setup.py -q install . Then the test suite is run. The important bits are in the deploy section. on : tags : true branch : master If there is a new git tag pushed to GitHub, and the tests pass, Travis CI will push the code to PyPI. Since bumpversion makes a new git tag with each new version, this works perfectly. This does require that my password be included in the yml file. To keep this secure, I utilized the Travis Command Line Client ( gem install travis ). In my local directory, I then ran: travis encrypt --add deploy.password This added the password to the YML file. Conclusion This was the first time I've released something to PyPI. It took a lot more set up than I expected it would take. However, now that I've gone through the process, gotten used to the ReStructuredText format that Sphinx and Read the Docs require, and set up PyPI for one project, I think it'll be fairly simple to do in the future. Most of the work is getting the other services to talk with GitHub and practicing good developer habits (documentation...). All StackAPI Links All of these links are to the various places that StackAPI lives on the internet GitHub : https://github.com/AWegnerGitHub/stackapi Read the Docs : http://stackapi.readthedocs.org/en/latest/ TravisCI : https://travis-ci.org/AWegnerGitHub/stackapi PyPI : https://pypi.python.org/pypi/StackAPI","tags":"Technical Solutions","loc":"http://andrewwegner.com/my-experiences-releasing-a-package-to-pypi.html","title":"My experiences releasing a package to PyPI"},{"text":"Motivation Since I shut down Vipers early this year, I've been itching to do something web related. Web technologies aren't my best technical skill, but I like trying out new things and learning something in the process. I use Python at work. I like Python a lot. With Christmas and New Years coming up, I want to have a project during my down time. My goal is to get a Flask application built and then deployed to OpenShift . Part of this deployment is to utilize TravisCI . I'm planning on using pytest and hypothesis for my test suite. Finally, I want to use my own (sub)domain, instead of the provided rhcloud one. Of these three technologies, I've used only Flask before. The comment flagging bot I built has a dashboard built in Flask. I've never used OpenShift or TravisCI. I selected OpenShift because it has a couple features I want that Heroku doesn't. The biggest one, according to the previous link, was that OpenShift has support for MySQL and Heroku doesn't (surprisingly). I want to use TravisCI and automated testing, because one of my goals for next year at work is to introduce automated tested to our development. (I work with Engineers, not coders...that's my excuse and it's a bad excuse, so I'm going to try and fix it.) To get ready for that goal, I want to test out a system that does continuous integration/automated testing. Both OpenShift and Travis CI provide me with free services. Hypothesis and py.test provide me with a way to generate comprehensive test conditions. OpenShift set up Signing up for OpenShift was easy. Fill out the form, provide an email address - though they don't like email addresses with + signs, which is disappointing - and then click the link they email back to you. Next, the rhc OpenShift client tools are needed. This is a Ruby package. I have no experience with Ruby, so I needed to install Ruby as well. I ran into a problem almost immediately. The page for installing these tools says: OpenShift rhc can be run on any operating system with Ruby 1.8.7 or higher assuming you have the requisite user permissions to install programs. Instructions for specific operating systems are provided below. Based on that, I figured I'd install the latest version of Ruby . At the time I tested this, that was 2.2.3. Unfortunately, when I ran the command to install the rhc tools, I received an error. After a bit of Googling, I found that it doesn't like 2.2x. So, I installed 2.1.7 instead. Next, I ran: gem install rhc This installs several gems and took a few minutes to complete. Next, rhc setup This started the OpenShift setup wizard. It consisted of filling out the few prompts and letting it generate an SSH key and then connecting to my account. Remember the Namespace you select. Again, this took a few minutes. Flask setup The next step was to set up my first \"Gear\". This would be the Flask application. I'll work on the database next. First, I just want Python and Flask to function properly. Fortunately, this is very easy, as OpenShift has a Flask template. rhc app create testapp python-2.7 --from-code=https://github.com/openshift-quickstart/flask-base.git A few notes: I am utilizing Python 2.7, because that is the recommendation from the Flask team. testapp can be any alphanumeric string. This is the name that will appear in the Web Console. A specific note, _ is not alphanumeric. I'm getting the feeling that OpenShift doesn't like \"special\" characters. The --from-code parameter will download that repository and use it as the base of your application. At this point, Flask can be run locally using: python app.py The application can be pushed back to OpenShift at this point and there should be a functional page on your OpenShift domain. In your command line, from the directory of your project: git add --all git commit -m \"Adding Flask application\" git push This will take a moment. At the end, you should see these lines in your command prompt: remote : Git Post - Receive Result : success remote : Activation status : success remote : Deployment completed with status : success If all three are a success, then you should be able to visit your URL. Your URL is a combination of your selected Namespace and the application name you created. http://<namespace>-<testapp>.rhcloud.com/ This should show a \"Welcome to Flask on OpenShift\" page. If you append /test to your URL, you'll get a message that says \"It's Alive!\" If it doesn't, you can check your error logs by running: rhc tail -a <testapp> MySQL setup Next, I set up my application to utilize MySQL. It's the database I have the most experience with, so I decided to keep that aspect of this project simple for myself. The first step was to add a MySQL 5.5 cartridge to my test gear (OpenShift terminology). I did this in the OpenShift web console. The UI provided me with the option to install various databases and one of those was MySQL. Clicking the link caused a few second delay as it was set up, and then I was presented with login credentials to my database. Step one...done. The next step is installing the correct Python modules to utilize MySQL. I selected PyMySQL (again, experience) and SQLAlchemy . I added these to both requirements.txt and setup.py . The idea behind doing it in both places is to make life easy for myself in the future. Additionally, the quick tutorials I've looked at for TravisCI encourage the usage of requirements.txt , while it seems OpenShift uses the setup.py . I'll fix that eventually, but getting it set up initially, this will be fastest. Add these to requirements.txt sqlalchemy==1.0.9 pymysql==0.6.7 Flask-SQLAlchemy==2.1 Add this to the install_requires list in setup.py 'sqlalchemy==1.0.9','pymysql==0.6.7','Flask-SQLAlchemy==2.1' The nice thing about OpenShift is that the credentials to the database are placed in environment variables , so I don't need to embed the passwords, connections strings, or anything potentially sensitive in my code. For MySQL these are available as: Variable Name | Purpose ------------------------------------------------ OPENSHIFT_MYSQL_DB_HOST | The host name or IP address used to connect to the database. OPENSHIFT_MYSQL_DB_PORT | The port the database server is listening on. OPENSHIFT_MYSQL_DB_USERNAME | The database administrative user name. OPENSHIFT_MYSQL_DB_PASSWORD | The database administrative user's password. OPENSHIFT_MYSQL_DB_SOCKET | An AF socket for connecting to the database (for non-scaled apps only). OPENSHIFT_MYSQL_DB_URL | Database connection URL. Utilizing the database Setting up SQLAlchemy and MySQL is fairly easy. I tested this with a simple table and ensured that it appeared in the database as expected. class User ( db . Model ): __tablename__ = 'users' id = db . Column ( 'user_id' , db . Integer , primary_key = True ) name = db . Column ( db . String ( 60 )) A few adjustments were made to the import statements of the Flask application: from flask_sqlalchemy import SQLAlchemy A couple variables were created and loaded: app.config.from_pyfile('flaskapp.cfg') db = SQLAlchemy(app) Finally, the flaskapp.cfg file was modified to include these two lines: SQLALCHEMY_DATABASE_URI = os.environ['OPENSHIFT_MYSQL_DB_URL'] + os.environ['OPENSHIFT_APP_NAME] SQLALCHEMY_ECHO = False Remote MySQL Access I like to use MySQL Workbench while building and testing to watch what's happening in the database. To use that with OpenShift, I had to jump through a few small hoops. Open MySQL Workbench and create a new connection Give the connection a name In \"Connection Method\", select \"Standard TCP/IP over SSH\" The SSH Hostname is the full name of your OpenShift gear where MySQL is installed. It should look like namespace-appname.rhcloud.com The SSH Username is the gear's Unique Identifier. This can be found by looking at the OPENSHIFT_GEAR_UUID environment variable. It can also be found in the web console, but looking at the \"remote access\" section. It shows a connection string. You need the username portion. This is the part that appears before the @ in the ssh longuniquestring@namespace-appname.rhcloud.com command. Set the SSH key file. On Windows this is in \\Users\\<username>\\.ssh\\id_rsa by default Set MySQL Hostname equal to the value in OPENSHIFT_MYSQL_DB_HOST Set Username equal to the value in OPENSHIFT_MYSQL_DB_USERNAME (this was also provided to you when you installed MySQL) See Password equal to the value in OPENSHIFT_MYSQL_DB_PASSWORD (again, this was provided to you when MySQL was installed) TravisCI setup I want to play with automated testing. The idea behind this is to get a jump start on a goal for next year at work and to learn something new. I'd like to utilize Travis CI to perform the tests and if they pass, deploy to OpenShift. If the tests fail, I don't want to push a broken build to OpenShift. That's the goal...we'll see how it turns out. But, the first step is getting Travis CI and OpenShift talking to one another. Travis CI integrates with GitHub, so what I'm going to do in reality is push to GitHub and let Travis CI pick up the changes. From there, it will perform it's tests. If the tests pass, it will push the commit to OpenShift. On GitHub, create a new repository for your source. This is where you will be pushing your code for Travis CI to pick up. From your OpenShift directory (which is already a git repository): git remote rename origin openshift git remote add origin https://github.com/<USER>/<repositoryname>.git git push -u origin master This resets the origin to point to your new GitHub repository and sets up a new remote. Then it pushes the changes to GitHub. Log into your Travis CI profile page . Make sure you are logged into GitHub first, as this will create the profile automatically. Press the \"Sync now\" button at the top of the page to pull a list of all of your repositories. Once that is done, find the repository you just set up, and enable integration with that repository. Next you need to set up Travis CI and the .travis.yml file. gem install travis This will install a Ruby script that assists in this process. If you get an error when running this, you need to create an empty .travis.yml file first and then run the command again. travis setup openshift Fill out the prompts. Defaults should be fine in most cases, but do pay attention to \"OpenShift application name\". If your GitHub repository is named differently than your OpenShift application name, the default for this prompt will be incorrect. One last note, for a quick test you can change the script section to script : true This forces the tests to pass. Once you've written tests, you can do something like: script: - py.test This will run your test scripts, utilizing py.test Deploy these changes to GitHub: git add .travis.yml git commit -m \"Deploying Travis\" git push origin master This will commit and push the changes to GitHub. A few seconds later, if you are watching Travis CI, you'll see it notices the new commit and starts running tests. If you tests complete with a status code of 0 (successful), it will deploy the changes to OpenShift. If the tests fail (any other status code), it will not deploy to OpenShift. py.test setup Setting up the testing frame work involves a few Python modules. These need to be added to both requirements.txt and setup.py . pytest>=2.8.0 hypothesis==1.16.0 pytest-runner==2.6.2 The next step is setting up some quick integration with setup.py , so that users can run python setup.py test and execute your tests. In setup.py , add (or edit) the setup_requires list to include pytest-runner . Add (or edit) the tests_require list to include pytest . I also added the following to my setup.cfg [aliases] test = pytest I modified my setup_requires list a bit, so that it's conditional. Since this would install the pytest-runner on every call to setup.py , even when the module wouldn't be called, I wanted the runner to only be required when pytest is utilized. import sys needs_pytest = { 'pytest' , 'test' , 'ptr' } . intersection ( sys . argv ) pytest_runner = [ 'pytest-runner' ] if needs_pytest else [] setup ( setup_requires = [ #... Other requirements here ] + pytest_runner , ) I wanted to test that my tests were working correctly. I created a tests directory, which is where I plan on storing all of my test cases. pytest will find any files that start or end with test and execute them. I created a very simple test_tests.py file with the following simple test (taken from the Hypothesis Quickstart ) @given(st.integers(), st.integers()) def test_ints_are_commutative(x, y): assert x + y == y + x Finally, Travis CI needs to be told what to do. Modify the script key to include py.test script: - py.test After a successful run through Travis, you'll see something like this: tests/test_tests.py . =========================== 1 passed in 0.26 seconds =========================== The command \"py.test\" exited with 0. Custom Domain I have my own domain name. I want to utilize OpenShift with one of those domains, instead of the default one provided. Since I've using the free tier, that will rule out using the SSL certificate that is wildcarded to the whole rhcloud.com domain. I can live with this. If I need SSL on my domain, I'll upgrade. To set up OpenShift to use your domain, log into the web console. Go to the gear you are configuring. At the top, where the full domain is displayed, is the option to \"Change\". Select that option. Input the full domain (and subdomain) you want to utilize and click \"Save\". After a few seconds, you'll get a notification that the alias was created. The next step is to configure the DNS records. I utilize CloudFlare for my domains, so the instructs will be specific to that, but should apply to any DNS system. Login to your management system and go to the area where you can specify DNS records. For my test, I set up a subdomain of one of my domains as the alias I wanted to use. In your DNS system, set up a CNAME that points to the original hostname on OpenShift. The CNAME should be the subdomain you told OpenShift about. Save the record. CloudFlare recognized this immediately and redirected me to my Flask application. Hooray! Conclusion With this, the set up is complete. You have a Flask application, connected to MySQL, that is integrated with a CI system which automatically deploys to OpenShift when all tests pass and uses CloudFlare (because I already was doing so), to provide a CDN. On to building something!","tags":"Technical Solutions","loc":"http://andrewwegner.com/how-i-set-up-openshift-travisci-and-flask.html","title":"How I built a Flask application that integrates with Travis CI and OpenShift"},{"text":"Introduction In April, I ran for moderator on Stack Overflow and didn't make it through the primaries. That's ok though, there were several very good users that did get elected . In a surprise announcement, though, Stack Overflow is running a second election this year. This is the first time this has happened since 2011. I'm still interested in a position and I'm still active in the community, so I'm going to run again. This post will follow the process. Nomination Phase Like last time, the nomination phase began with users throwing their hat into the ring. Nominations were slower and fewer this time. Only 19 nominees, so no one was eliminated due to low reputation. Several users from the last election are rerunning too. My Platform My platform hasn't changed much since the previous run. Below is my nomination post. This time, I tried to pull emphasis off the automated script by putting it lower on the list of things I've done and instead focused on the moderation tasks I do on Stack Overflow and the work I've done on Community Building . We'll see if it works. Hi Everyone, I'm Andy and I'd make a great moderator for Stack Overflow. Why vote for me? I'm active in the review queues (currently holding 10th in Low Quality Post reviewers of all time), provide edits to posts, answers and enjoy the moderation aspect of Stack Exchange I've been a moderator on CommunityBuilding for nearly a year and a half. I know the moderator tools and have worked with several of the current moderators. This interaction will continue as a new moderator here. I've built an automated script that continues to handle noisy comments very accurately . I have a history on Meta.SO that shows I'm involved in the meta aspect of the site as well. I have a history of good community moderation already. I enjoy the moderation aspect on Stack Overflow (and Stack Exchange in general). I deal with users with respect, even if our opinions on an issue differ. With this, I received my \"candidate score\". It was 33/40. Not the highest, but better than last time. The score wasn't mentioned in April. I am not expecting it to be an issue this time either. Primary Phase Updated November 21, 2015 The primary phase is in the third day. In day 1, I was hovering around 9th/10th place. Overnight, between days 1 and 2 though, I dropped down to 11th. I've been sitting here consistantly for a full day now and, while still gaining votes, I'm not gaining as fast as 10th position. It appears I may not make the cutoff by Friday's deadline. While disappointing, there are a few things that I came away with that I'm very happy about. In the last primary, I revieved 1,492 positive votes. I've surpassed that already. I've over 2,100 currently. I'm pleased with that upswing. I was also more prepared for the questionnaire portion of the primaries this time. I've gotten the second highest number of upvotes on my responses . Several of the questions were similar to last time, but there are a few that I think should be included in the future elections. Questionnaire This first question is a great post for candidates. It allows them to show off their involvement in Meta and show their best work. For users, it gives them a sense of how a candidate interacts with the community. I am very surprised that several candidates list only one or two posts. This seems to be doing a disservice to themselves. Do you have any Meta posts that you're particularly proud of, or that you feel best demonstrate your moderation style? My response to this question: I'm proud of several of my posts both here on Meta.SO and on other network sites I participate in. Here on MSO, I have two questions that I am proud of: Can a machine be taught to flag comments automatically? I estimate 10% of the links posted here are dead. How do we deal with them? In both of these, you can see that I care about quality on Stack Overflow. I've spent time analyzing the problem, as I see it, and present my findings to the community. I participated in the discussions that both posts generated and continue to run the bot to this day. Elsewhere on the network, my participation in meta has helped to shape communities. For example, on Hardware Recommendations , my meta post about \"What type of hardware is allowed\" helped to set the scope of what the community accepts as on topic hardware. I've also helped to set up the high quality guidelines for questions and argued against certain types of tags and hardware . With all of these, I've presented my arguments and logic and strived to remain professional. I believe the community on HardwareRecs has seen that as well. As a moderator on Community Building , I've been involved in many discussions . I was involved in the discussions to rename the community from Moderators.SE to CommunityBuilding.SE. I've been involved in discussions about slow growth of the community . I've also presented arguments that go against other moderators, and walked away still feeling like a moderation team. (Go communication!) Finally, on OpenSource, I made a post about how moderators had implemented a policy to watch the reviewers. It was similar to the long removed \"flag weight\" option that used to exist. I believe the post was presented in a way that questioned the decisions of the moderators, yet remained professional. With all of these meta posts, across the network, I think you can pick up on my moderation style and personality. I like data and I try to present my thoughts in a way that is understandable to all. I'm also willing to speak my mind. This second question I struggled with for a bit. I've had ideas on how Stack Overflow/Stack Exchange could improve, but what did I want to present in this response. If you could add/revise one Stack Overflow policy/guideline, what would you change? Why would you change it, and what would it mean for the community? My response to this question: At the risk of talking myself out of a position, I think more community moderation would help the problem that Stack Overflow has with scaling moderators. There are a couple areas that I think would work well in opening this to the higher reputation users Comment flagging: Comments can be removed if enough users flag a comment. If not, a moderator needs to handle the flag. Instead, opening this as a review queue can remove a lot of this burden from the moderators. Users could handle all but the \"Other...\" flag. There may be guidance needed on the \"Obsolete\" one due to the difference between \"obsolete comment\" and \"obsolete code block\" differences. Audit Review reviews: On Stack Overflow, we get a decent number of disputed audit review posts on meta. There may be a way to get users with a history of passing both audits and good reviews involved in dealing with these disputed audits. The idea would be to say whether an audit is good or not. These changes, and other areas where the community could be leveraged for moderation tasks, helps to remove the burden on moderators. Handling 2,000 (and growing) flags a day means that something needs to change. Moderators are exception handlers. They should be handling the cases that are exceptional - not comments that are no longer relevant. For the community, this would be more involvement with the moderation aspect. Users would be able to more quickly clean up a comment thread. Flag it and it appears in the review queue. From here, the moderators don't need to be involved. The downside of this is that it adds another queue for users to be involved with. Primary Results With the primaries over, I ended with 2483 positive votes. This put me in 11th place. Sadly, this was not enough to get into the election. I was 185 votes shy of over taking 10th. Good luck to the candidates that made it. One of the tools that came out of this election was a way to visualize various data points to compare candidates. I provided a couple notes about outliers various candidates show regarding aspects on the site. I found it interesting to see what each user had \"specialized\" in. Election Updated December 8, 2015 The election is over and the new moderators have settled in. We've had our first bout of public drama over one of these moderators actually moderating a chat room too. gasp Final thoughts I was closer to the top 10 this time, but still missed it. Even more surprising was that the user that ended up in 3rd in the primaries didn't even come close to getting elected. He was eliminated in the 5th round of final STV votes. I still think I'd make a great moderator for Stack Overflow, but I need to figure out the best way to promote myself in the next election.","tags":"Side Activities","loc":"http://andrewwegner.com/i'm-running-for-moderator-on-stack-overflow-again.html","title":"I'm running for moderator on Stack Overflow again"},{"text":"Introduction A few days ago, on Meta.StackOverflow, a product manager asked for feedback on how they could make first contact between employeers and potential employees more valuable to the potential employee. I provided my feedback . I've reposted it below, with some minor changes to further explain my ideals. The summary of this entire post is: If I have to spend time to make my resume jump out and catch your attention, I expect you to spend just a little bit of time telling me about your company and the position you are recruiting for. I don't think a few sentences with these details is that much ask. What I want to see I want to know what the company does and I want to know who you are. I also want to know what positions you are looking to fill and a few details about the position are also important. Messages I've recieved Message 1 This is the start of a good message: It has the following good elements: Who the recruiter is are Position they are recruiting for A company name It does not offer the following: What the company does (other than \"build great software\"). Explanation which of my skills or experiences they are interested in. Are you interested in my Python answers on Stack Overflow? My projects on Github? My PHP experience from years ago? This is important to know. If the company is looking to recruit me for knowledge I shared years ago (ie. PHP in my case), I'd probably be less useful right now because I haven't used PHP in years. I have not kept up with recent changes to the language or various frameworks. Explanation of what the position will do, projects I'll be involved in, or challenges they are facing now that I'll help solve. I did not respond to this position. Message 2 This has the following useful elements: I know your name, position and company. Due to who the company was in this particular message, I am even vaguely aware of what the company does. Name recognition helps, but is not very high on my list of \"important things\" What this does not have: A description of the position. \"Looking for top talent...to make a significant impact on our systems\". Ok. How? What will I be doing? How did you find me? I actually talked to this recruiter and I have to say that I'm both disappointed in the company and myself for not noticing some red flags in this message. This wasn't an interview for a specific position. This was a general recruiting email and I suspect it was sent to multiple users. Flags that I missed in the message: Lack of any details at all and use of buzzwords (\"top technical talent\", \"fast paced\", \"start-up like\", \"eCommerce industry\"). Discussion about personal career goals and technical background. Some of my back ground should be obvious from my Careers profile, SO profile and links on both. More importantly, I missed the \"personal career goals\" flag. The details they wanted were to see if my goals aligned with any open positions they had at the time. When the discussion took place, the recruiter knew little more than my name. There were no questions about my careers profile (at the very least) or other projects I mentioned on my profile. Shame on me. Message 3 This message isn't useful to me at all. Who is this company and what do they do? I have no experience in mobile apps. How did they find me? What is Chad's relationship to the company? I suspect this message showed up because I work for one of their largest clients, live 10 miles from their office and work, literally, right across the street from them. I didn't know that at the time though. I had to ask a co-worker if they'd ever heard of the company. They pointed out their office at lunch. I did not respond this this message. Message 4 This message is worthless. You are seeking employees who are willing to relocate. Great. Good for you. Are you interested in my skills or just a warm body to keep your chairs warm? No details about the company, other than a link to a web site No name at all. Who am I talking to? No details about the job. I hope they found someone to hold their seats to the floor 8 hours a day. If not, I recommend large rocks. Summary of the messages I need details and I need details more than what the salary is going to be. Obviously, I need to do some research about your company, if I've never heard of you. But, throw me a bone. Tell me a bit about your self. I had to make a fancy resume to get you to reach out to me. I did something that caught your eye. Now, do something that will catch mine. I don't need to work at a company that is the next Google or Apple or start up flush with cash. But, I do want to know about the company before I talk to you. If you can't spend a few seconds explaining a bit about the company, I don't want to talk with you. I don't think a few sentences is that much to offer: \"Here at COMPANY NAME, we are looking for a POSITION with skills in LANGUAGE or experience in INDUSTRY. I see you have both and think you'd be able to help TEAM NAME with their on going project of MAKING THE WORLD BETTER. Your PROJECT ON GITHUB looks like you've dealt with aspects of this problem before. We've gotten some press recently about our innovations in this area (check them out HERE).\" How to get my attention I think the post above makes it clear what doesn't work. The companies that I've responded to (other than message 2, above) have all done the following: Indicate they've at least looked at my resume by commenting on some aspect of it. I've had recruiters mention projects I've done, jobs I've held, or individual bullet points that caught their attention. This is great. It means you aren't reaching out to me because I happened to hit all of your keywords when the resume passed through your Human Resources department. Provide a short description of the job they want to fill. I don't need a full job posting. A link to such a posting is sufficient. However, if you could summarize it in a sentence or two, that'd make both of our lives easier. Reading full job descriptions isn't the most fun thing in the world and often have industry (or even company) specific acronyms. That isn't helpful to either of us, because I may not know them. Just tell me what your team does within the company. That's good enough to get my attention. Talk to me like a person, not some number that a database search returned. As a hint, if you can figure out that I like to go by \"Andy\", instead of \"Andrew\", you've already done a far better job than most recruiters I've spoken with.","tags":"Jobs","loc":"http://andrewwegner.com/how-not-to-recruit-me.html","title":"How not to recruit me"},{"text":"Introduction In my last two posts, I've discussed the number of rotten links on Stack Overflow and a proposal to fix said links . In this post, I'm going to discuss how I performed this analysis. Set up The database The process began by downloading the March 2013 data dump . I loaded the posts into a [MariaDB] instance on my local machine. This was accomplished with a very simple script and patience, as the script took a while to run. load xml local infile '/path/to/posts.xml' into table posts rows identified by '<row>'; The data Once this was done, the next step was selecting my random sample of data. I did this by randomly selecting 25% of the days in a year and then pulling all posts for those days across all years of Stack Overflow's existence. The Python script I used to do this was fairly simple: from datetime import timedelta , datetime from random import randint from math import ceil def random_date ( start , end ): return start + timedelta ( seconds = randint ( 0 , int (( end - start ) . total_seconds ()))) percentage = 0.25 days = 366 dayslist = [] for d in xrange ( int ( ceil ( days * percentage ))): dayslist . append ( random_date ( datetime ( 2008 , 1 , 1 ), datetime ( 2008 , 12 , 31 ))) At the end of this run, the days that I cared about are in the dayslist variable. I used that to pull questions and answers from the database that were created on that month/day combination. In the end, this resulted in just over 25% of the total posts being selected. To ensure that I could replicate the results, I also saved the dates that were selected. Parsing the data The next step was to parse out links from the data. I used the following script to extract anchor text and links from a post. def links_in_post(post): \"\"\" Returns a list of all links found :param posts: A list of dictionaries with a 'body' key containing HTML strings [ { 'body': \" <b> This is HTML </b> \" }, ] :return: A list of tuples containing anchor text and URL [ ('Display Text', 'http://example.com') ] \"\"\" logging.debug(\"Extracting links...\") links = [] images = [] regexp = \" &.+?; \" list_of_html = re.findall(regexp, post) for e in list_of_html: if e in invalid_entities: h = HTMLParser.HTMLParser() unescaped = h.unescape(e) post = post.replace(e, unescaped) doc = html.fromstring(post) for link in doc.xpath('//a'): links.append(Link(text=link.text_content(), link=link.get('href'))) for image in doc.xpath('//img'): images.append(Link(text=image.get('alt'), link=image.get('src'))) all_items = links + images seen = set() unique_items = [item for item in all_items if item[1] not in seen and not seen.add(item[1])] return unique_items The regular expression being utilized, is to strip out HTML entities. This was needed due to weird parsing issues with non-ASCII characters. Fortunately, I wasn't the first to encounter oddities like this . The list comprehension at the end of the function is returning only unique tuples of anchor text/link. I was surprised how often I'd end up with tuples such as ('this', 'http://google.com') in the same post. This uniqueness saved a lot of processing time later. After all links in a post had been extracted, this information and information about the post itself, was saved to the database. If a post had no links, it was not saved. The database consisted of three tables. Links - This table contains the base URLs seen in all posts. URLs are distinct. It also contains an ID that will be utilized for linking to the other tables. Post Links - This table contains information about links in a post. This includes the specific anchor text/link combinations Link Results - This table contains the results of link status checks Processing the posts was fairly time consuming, but was able to be parallelized easily. That significantly cut down on processing time. Checking the links The most time consuming portion of this entire project was actually checking link status. Each link that appeared in the Links table was checked. As I mentioned in my first post, the original idea was to simply send a HEAD request to each URL. The idea was to save myself and the end point a tiny amount of bandwidth. I had over a million links to process. I figured a little saved bandwidth wouldn't hurt. I turns out this isn't a good idea. When I started seeing larger sites as not being accessible, I go suspicious that something was wrong. These sites were returning status 405 errors. This indicates that the method is not allowed. So, I switched to GET for every link. The next problem I ran into was that many sites didn't like the default user agent of the spider. They rejected requests with 404 and 401 errors. In the end, I got around this by mimicking Firefox on every request. With those kinks worked out, every link was sent a GET request that looked to be from a Firefox browser. The process would allow 20 seconds per link. If the link didn't respond in that time limit, it was declared inaccessible. A week later, I repeated the process with anything that hadn't returned a status code less than 400. Once more, on the third week, I repeated this with the failed links. At the end of three weeks, I had a list of sites that were inaccessible to me - on a residential connection - three times over a period of three weeks. Results The SVG image that I created for the write up was generated with Pygal. The tables were the result of various break downs of the data via queries to the status results. Wrap up I am rather proud of how the results turned out for this project. I went into it expecting about 15% of links to be broken, but I didn't really realize what the meant. Fifteen percent of 21 million total posts is over 3 million. That's a large number. BUT, it also ignored that a large percentage of posts don't contain links. I failed to consider that in my original hypothesis. Less than half of my sample had links (2.3M out of 5.6M). Of the 2.3M with links, only 1.5M were unique links. The final result of 10% failed links makes much more sense in this context. Ten percent of 1.5M links means that there are 150K links that are bad.","tags":"Side Activities","loc":"http://andrewwegner.com/link-analysis---technical-explanation.html","title":"Link Analysis - Technical Explanation"},{"text":"This post was published by me on Meta Stack Overflow on August 7th, 2015. I've republished it here so that I can easily update information related to recent developments. If you have questions or comments, I highly encourage you to visit the question on Meta Stack Overflow and post there. This is a follow up to yesterday's post about how many links on Stack Overflow are starting to rot. The proposal I propose another hybrid of the previous broken link queue (as was mentioned above in comments and other answers ) and an automated process to fix broken links with an archived version (which has also been suggested ). The broken link queue should focus on editing and fixing the links in a post (as opposed to closing it). It'd be similar to the suggested edits queue, but with the focus intended to correct links not spelling and grammar. This could be done by only allowing a user to edit the links. One possibility, I envision is presenting the user with the links in the post and a status on whether or not the link is available. If it's not available, give the user a way to change that specific link. Utilizing this post, I have a quick mock up of what I propose such a review task looks like: The Queue All the links that appear in the post are on the right hand side of the screen. The links that are accessible have a green check mark. The ones that are broken (and the reason for being in this queue) have a red X. When a user elects to fix a post, they are presented with a modal showing only the broken URLs. The Automation With this queue, though, I think an automated process would be helpful as well. The idea is that this would operate similarly to the Low Quality queue, where the system can automatically add a post to the queue if certain criteria are met or a user can flag a post as having broken links. I've based my idea on what Tim Post outlined in the comments to a previous post . Automated process performs a \"Today in History\" type check. This keeps the fixes limited to a small subset of posts per day. It also focuses on older posts, which were more likely to have a broken link than something posted recently. Example: On July 31, 2015, the only posts being checked for bad links would be anything posted on July 31 in any year 2008 through current year - 1. Utilizing the Wayback Machine API , or similar service, the system attempts to change broken links into an archived version of the URL. This archived version should probably be from \"close\" to the time the post was originally made. If the automated process isn't able to find an archived version of the link, the post should be tossed into the Broken Link queue When the Community edits a post to fix a link, a new Post History event is utilized to show that a link was changed. This would allow anyone looking at revision history to easily see that a specific change was only to fix links. Actions performed in the previous bullets are exposed to 10K users in the moderator tools. Much like recent close/delete posts show up, these do as well. This allows higher rep users to spot check (if they so desire). I think this portion is important when the automated process fixes a link. For community edits in the queue, the history tab in /review seems sufficient. If a post consists of a large percentage of a link (or links) and these links were changed by Community, the post should have further action taken on it in some queue. Example: - A post where X+% of the text is hyperlinks is very dependant on the links being active. If one or more of the links are broken, the post may no longer be relevant (or may be a link only post). One example I found while doing this was this answer. I don't think that this type of edit from the Community user should bump a post to the front page. Edits done in the broken link queue, though, should bump the post just like a suggested edit does today. By preventing the automated Community posts from being bumped, we prevent the the front page from being flooded, daily, with old posts and these edits. I think that the exposure in the 10K tools and the broken link queue will provide the visibility needed to check the process is working correctly. Process flows Queue Flow: Automated process flow: Potential pitfalls The automated link checking will likely run into several of the problems I did. Mainly: Sites modify the HEAD request to send a 404 instead of a 405. My solution to this was to issue GET requests for everything. Sites don't like certain user agents. My solution to this was to mimic the Firefox user agent. To be a good internet citizen, Stack Exchange probably shouldn't go that far, but providing a unique user agent that is easily identifiable as \"StackExchangeBot\" (think \"GoogleBot\"), should be helpful in identifying where traffic is coming from. Sites that are down one week and up another. I solved this by spreading my tests over a period of 3 weeks. With the queue and automatic linking to an archived version of the site, this may not be necessary. However, immediately converting a link to an archived copy should be discussed by the community. Do we convert the broken link immediately? Or do we try again in X days. If it's still down then convert it? It was suggested in another answer that we first offer the poster the chance to make changes before an automatic process takes place. The need to throttle requests so that you don't flood a site with requests. I solved this by only querying unique URLs. This still issues a lot of requests to certain, popular, domains. This could be solved by staggering the checks over a period of minutes/hours versus spewing 100s - 1000s of GET requests at midnight daily. With the broken link queue, I feel the first two would be acceptable. Much like posts in the Low Quality queue appear because of a heuristic, despite not being low quality, links will be the same way. The system will flag them as broken and the queue will determine if that is true (if an archived version of the site can't be found by the automated process). The bullet about throttling requests is an implementation detail that I'm sure the developers would be able to figure out.","tags":"Side Activities","loc":"http://andrewwegner.com/a-proposal-to-fix-broken-links-on-stack-overflow.html","title":"A proposal to fix broken links on Stack Overflow"},{"text":"This post was published by me on Meta Stack Overflow on August 6th, 2015. I've republished it here so that I can easily update information related to recent developments. If you have questions or comments, I highly encourage you to visit the question on Meta Stack Overflow and post there. TL;DR: Approximately 10% of 1.5M randomly selected unique links in the March 2015 data dump are unavailable. To be more precise, that is approximately 150K dead links. Motivation I've been running into more and more links that are dead on Stack Overflow and it's bothering me. In some cases, I've spent the time hunting down a replacement, in others I've notified the owner of the post that a link is dead, and (more shamefully), in others I've simply ignored it and left just a down vote . Obviously that's not good. Before making sweeping generalizations that there are dead links everywhere, though, I wanted to make sure I wasn't just finding bad posts because I was wandering through the review queues. Utilizing the March 2015 data dump, I randomly selected about 25% of the posts (both questions and answers) and then parsed out the links. This works out to 5.6M posts out of 21.7M total. Of these 5.6M posts, 2.3M contained links and 1.5M of these were unique links. I sent each unique URL a HEAD request, with a user agent mimicking Firefox 1 . I then retested everything that didn't return a successful response a week later. Finally, anything that failed from that batch, I resent a final test a week later. If a site was down in all three tests, I considered it down for this test. Results 2 By status code Good news/Bad News: A majority of the links returned a valid response, but there are still roughly 10% that failed. (This image is showing the top status codes returned) The three largest slices of the pie are the status 200s (site working!), status 404 (page not found, but server responded saying the page isn't found) and Connection Errors. Connection errors are sites that had no proper server response. The request to access the page timed out. I was generous in the time out and allowed a request to live for 20 seconds before failing a link with this status. The 4xx and 5xx errors are status codes that fall in the 400 and 500 range of HTTP responses. These are client and server error ranges, thus counted as a failure. 2xx errors (of which was are in the low triple) are pages that responded with a success message in the 200 range, but it wasn't a 200 code. Finally, there were just over a hundred sites that hit a redirect loop that didn't seem to end. These are the 3xx errors. I failed a site with this range if it redirected more than 30 times. There are a negligible number of sites that returned status codes in the 600 and 700 range 4 By most common There are, expectedly, many URLs that failed that appeared frequently in the sample set. Below is a list of the top 50 3 URLs that are in posts most often, but failed three times over the course of three weeks. http://docs.jquery.com/Plugins/validation http://www.eclipse.org/eclipselink/moxy.php http://jackson.codehaus.org/ http://xstream.codehaus.org/ http://opencv.willowgarage.com/wiki/ http://developer.android.com/resources/articles/painless-threading.html http://valums.com/ajax-upload/ http://sqlite.phxsoftware.com/ http://qt.nokia.com/ http://www.oracle.com/technetwork/java/codeconv-138413.html http://download.java.net/jdk8/docs/api/java/time/package-summary.html http://docs.oracle.com/javase/1.4.2/docs/api/java/text/SimpleDateFormat.html http://watin.sourceforge.net/ http://leandrovieira.com/projects/jquery/lightbox/ https://graph.facebook.com/ https://ccrma.stanford.edu/courses/422/projects/WaveFormat/ http://www.postsharp.org/ http://www.erichynds.com/jquery/jquery-ui-multiselect-widget/ http://ha.ckers.org/xss.html http://jetty.codehaus.org/jetty/ http://cpp-next.com/archive/2009/08/want-speed-pass-by-value/ http://codespeak.net/lxml/ http://www.hpl.hp.com/personal/Hans_Boehm/gc/ http://jquery.com/demo/thickbox/ http://book.git-scm.com/5_submodules.html http://monotouch.net/ http://developer.android.com/resources/articles/timed-ui-updates.html http://jquery.bassistance.de/validate/demo/ http://codeigniter.com/user_guide/database/active_record.html http://www.phantomjs.org/ http://watin.org/ http://www.db4o.com/ http://qt.nokia.com/products/ http://referencesource.microsoft.com/netframework.aspx https://github.com/facebook/php-sdk/ http://java.decompiler.free.fr/ http://pivotal.github.com/jasmine/ http://api.jquery.com/category/plugins/templates/ http://code.google.com/closure/library http://www.w3schools.com/tags/ref_entities.asp http://xstream.codehaus.org/tutorial.html https://github.com/facebook/php-sdk http://download.java.net/maven/1/jstl/jars/jstl-1.2.jar https://developers.facebook.com/docs/offline-access-deprecation/ http://www.parashift.com/c++-faq-lite/pointers-to-members.html https://developers.facebook.com/docs/mobile/ios/build/ http://downloads.php.net/pierre/ http://fluentnhibernate.org/ http://net.tutsplus.com/tutorials/javascript-ajax/5-ways-to-make-ajax-calls-with-jquery/ http://dev.iceburg.net/jquery/jqModal/ By post score Count of posts by score (top 10) (Covers 94% of all broken links): | Score | Percentage of Total Broken | |-------|----------------------------| | 0 | 36.4087% | | 1 | 25.1674% | | 2 | 13.4089% | | 3 | 7.2806% | | 4 | 4.2971% | | 5 | 2.7065% | | 6 | 1.8068% | | 7 | 1.2854% | | -1 | 1.1935% | | 8 | 0.9415% | By number of views Note, this is number of views at the time the data dump was created, not as of today Count of posts by number of views (top 10): | Views | Total Views | |--------------|-------------| | (0, 200] | 24.4709% | | (200, 400] | 14.2186% | | (400, 600] | 9.5045% | | (600, 800] | 6.9793% | | (800, 1000] | 5.2574% | | (1000, 1200] | 4.1864% | | (1200, 1400] | 3.3699% | | (1400, 1600] | 2.7766% | | (1600, 1800] | 2.3477% | | (1800, 2000] | 1.9550% | By days since post created Note: This is number of days since creation at the time the data dump was created, not from today Count of posts by days since creation (top 10) (Covers 64% of broken links): | Days since Creation | Percentage of Total Broken | |---------------------|----------------------------| | (1110, 1140] | 7.2938% | | (1140, 1170] | 6.7648% | | (1470, 1500] | 6.6579% | | (1080, 1110] | 6.6535% | | (750, 780] | 6.5535% | | (720, 750] | 6.5516% | | (1500, 1530] | 6.3978% | | (390, 420] | 5.8508% | | (360, 390] | 5.8258% | | (780, 810] | 5.5175% | By Ratio of Views:Days Ratio Views:Days (top 20) (Covers 90% of broken links): | Views:Days Ratio | Percentage of Total Broken | |------------------|-------------| | (0, 0.25] | 27.2369% | | (0.25, 0.5] | 18.8496% | | (0.5, 0.75] | 11.4321% | | (0.75, 1] | 7.2481% | | (1, 1.25] | 5.1668% | | (1.25, 1.5] | 3.7907% | | (1.5, 1.75] | 2.9310% | | (1.75, 2] | 2.4033% | | (2, 2.25] | 1.9788% | | (2.25, 2.5] | 1.6850% | | (2.5, 2.75] | 1.4080% | | (2.75, 3] | 1.1879% | | (3, 3.25] | 1.0654% | | (3.25, 3.5] | 0.9391% | | (3.5, 3.75] | 0.8334% | | (3.75, 4] | 0.7165% | | (4, 4.25] | 0.6634% | | (4.25, 4.5] | 0.5789% | | (4.5, 4.75] | 0.5508% | | (4.75, 5] | 0.4833% | Discussion What can we do with all of this? How do we, as a community, solve the issue of 10% of our outbound links pointing to places on the internet that no longer exist? Assuming that my sample was indicative of the entire data dump, there are close to 600K (150K broken unique links x 4, because I took 1/4 of the data dump as a sample) broken links posted in questions and answers on Stack Overflow. I assume a large number of links posted in comments would be broken as well, but that's an activity for another month. We encourage posters to provide snippets from their links just in case a link dies. That definitely helps, but the resources behind the links and the (presumably) expanded explanation behind the links are still gone. How can we properly deal with this? It looks like there have been a few previous discussions: Utilize the Wayback API to automatically fix broken links. Development appeared to stall on this due to the large number of edits the Community user would be making. This would also hide posts that depended on said link from being surfaced for the community to fix it. Link review queue . It was in alpha , but disappeared in early 2014. Badge proposal for fixing broken links Footnotes This is how it ultimately played out. Originally I sent HEAD requests, in an effort to save bandwidth. This turned out to waste a whole bunch of time because there are a whole bunch of sites around the internet that return a 405 Method Not Allowed when sending a HEAD request. The next step was to sent GET requests, but utilize the default Python requests user-agent. A lot of sites were returning 401 or 404 responses to this user agent. Links to Stack Exchange sites were not counted in the above results. The failures seen are almost 100% due to a question/answer/comment being deleted. The process ran as an anonymous user, thus didn't have any reputation and was served a 404. A user with appropriate permissions can still visit the link. I verified a number of 404'd links to Stack Overflow posts and this was the case. The 4th most common failure was to localhost . The 16th and 17th most common were localhost on ports other than 80. I removed these from the result table with the knowledge that these shouldn't be accessible from the internet. There where 7 total URLs that returned status codes in the 600 and 700 range. One such site was code.org with a status code of 752. Sadly, this is not even defined the joke RFC. Follow up I posted a proposal on how I think this could be fixed.","tags":"Side Activities","loc":"http://andrewwegner.com/analysis-of-links-posted-to-stack-overflow.html","title":"Analysis of links posted to Stack Overflow"},{"text":"Introduction In a previous post , I described why I moved from Wordpress to Pelican for my blog. This one goes a step further and describes how I eliminated the need for the dedicated server I'd been utilizing as a part of Team Vipers . By eliminating that server, I reduced my costs to zero but kept control over the DNS of my domain (thanks to CloudFlare ) and had an easier method of updating the site using GitHub Pages . GitHub Pages Setup To utilize GitHub Pages, I needed to create a new repository that followed the format GitHubUsername.github.io . This repository would house the content that is this site. I also set up a second repository which contains the source for the blog. This repository includes the templates, plugins and markdown version of the pages. The first repository was set up as submodule. git submodule add https://github.com/AWegnerGitHub/awegnergithub.github.io.git output I ignored the output directory in .gitignore on the source repository. Finally, I had to adjust publishconf.py slightly to DELETE_OUTPUT_DIRECTORY = False Without this, I was constantly destroying the output repository and had to reinitialize it. This prevents that from occuring. Now, a new post consists of writing up the Markdown page , generating the page with the command below (or the batch script ) and then committing and pushing the changes to the submodule to GitHub. # Generates HTML files without debugging information pelican content --output output --settings publishconf.py The new content is available immediately. Custom Domain You may notice that the URL for this site isn't awegnergithub.github.io , but instead andrewwegner.com . To accomplish this, I added a directory to the content named extra . In this directory is a single file named CNAME (no extension). In the file is my domain name. Next, I had to modify pelicanconf.py to add the extra/CNAME to the static path and then on generation move the CNAME file from this subdirectory to the root. I could have put it in the root of content by default, but Pelican provides a way to do this and it keeps content clean. One very important note , the EXTRA_PATH_METADATA is operating system sensitive. Since I am generating the content on a Windows machine, I had to use a backslash instead of the forward slash the documentation shows. I found this after posing a question on Stack Overflow on why it wasn't working as the documentation suggested. The two important fields to add or edit are: ... STATIC_PATHS = ['images', 'extra/CNAME'] ... EXTRA_PATH_METADATA = {'extra\\CNAME': {'path': 'CNAME'},} Cloudflare Setup The final thing I needed in order to get rid of my server was control over DNS. I could revert back to GoDaddy, but after a little research found that CloudFlare's additional CDN and security was a \"good thing\" (because, you know, I'm such a highly traffic'd blog these days). Step one was signing up to CloudFlare. This was a 3-5 minute thing. Once signed up and signed in, I went to set up DNS. This was as simple as adding my domain name and waiting for CloudFlare to import my existing DNS records. With this, I kept by Google Apps email intact (which is what I was most concerned with). Next, I went and removed the A records. I replaced these with CNAME records pointing to my GitHub Pages URL. I also added a www CNAME pointing to the same location. Since I have Pelican configured to strip it with the setting below, it doesn't matter other than people expect to enter www dot domain dot com in their URL bar. SITEURL = 'http://andrewwegner.com' Last, I had to point by name servers to CloudFlare instead of my dedicated server. They provide a list of registratars to choose from. Select your registrar and follow the instructions. My biggest issue here was remembering my GoDaddy password. After I made it into my account, the steps to change name servers were very simple. Once those are saved, you wait for the changes to propagate and enjoy your new GitHub Pages / CloudFlare web page for free.","tags":"Side Activities","loc":"http://andrewwegner.com/how-i-set-up-this-site-with-github-pages-and-cloudflare.html","title":"How I set up this site with GitHub Pages and CloudFlare"},{"text":"Introduction For years, I maintained a Wordpress blog covering various things I've done or created. Most of these revolved around things I created to make administering Team Vipers easier for me and for the rest of the admin team. It was my way of documenting what I'd done (in case I ever needed to do it again) and providing a way to update the Team Vipers community about new plugins or applications that would be deployed to the community. My Issues with Wordpress The problem I had with Wordpress was that it was just to bulky for the simple posts I was making. I needed a database, a full web server (or a hosting provider), and either time to hunt for the \"perfect\" plugin(s) or PHP knowledge to do it myself. Early in my development career, I used PHP a lot. That was part of the reason I chose Wordpress. Oh! I know that language. If I ever need something, I can just whip it up myself. -Me, before the real world ambushed me and beat me with a stick Spam I spent time setting up Wordpress. I picked out a theme, plugins, and started saving future me with documentation. Then life happened. For whatever reason, I stopped updating Wordpress. My blog sat out there for weeks or months unvisited by anyone. Then, one morning, my phone vibrated and told me that I had a new comment on my site. Woo! Except it was spam. Boo! I marked it spam and moved on with my day. Later that morning, I glanced at my phone again. 32 emails. I am just not that popular. Something was wrong. Turns out, a spam bot found me. I sighed and then removed all the comments and checked the box indicating that users had to be registered to post. That solved my problem for a few months. Then the bots got smarter. They started registering. They started posting legitimate looking messages, except for that associated URL their name would link to in the comments. They pulled keywords out of the post and formulated a somewhat passable English question using those words. The spam prevention plugins I installed would slow the tide for a few weeks. The bots would adapt and then I'd be awash with spam posts again. Eventually, the solution was to completely disable comments. I'd spent way to much time dealing with spam on a blog that received very little legitimate traffic. Since I don't utilize the comments, Pelican provides a nice simple page that I can post my thoughts and not worry about getting hit by a spam bot. It also provides plugins so that I can include comments should I ever choose to do so in the future. For the time being, though, I have a nice simple page with no comments. That's exactly what I was looking for. Security If you watch any technology web sites, you'll notice that there are vulnerabilities found in Wordpress frequently. These require patches, which requires me to do something. It may be as simple as logging in and clicking a button to update, but it is still something I need to remember to do for a relatively minor site. When I'd log in to clear the spam backlog, I'd frequently also install updates for 10-20 plugins, themes or Wordpress itself. It was mostly painless, but I didn't like the idea of the site sitting there vulnerable for weeks at a time because I didn't visit and login. The dynamic nature of Wordpress and the underlying database exposed a fairly sizable target for a web page so small. Pelican generates static HTML pages. I don't have to worry about SQL injections, unauthorized logins, or anything else. I host a basic set of HTML, CSS and JavaScript files. That's it. PHP vs Python As I mentioned before, I used to use PHP frequently. It was my go to language. I picked Wordpress with the idea that I'd be able to hack together features I needed. The reality, it turns out, was that I wasn't actually interested in doing that. Instead, I picked out plugins that were close enough to the exact functionality I wanted. I transferred to a job where I used Python. Instead of having a language I used on the side (PHP) and a job where I was a glorified project manager, without the actual title of \"Project Manager\", I now had a job where I used a language (Python) for 8 hours a day. My usage of PHP plummeted. I found I could get what I wanted done in my side projects faster and easier with Python. At work I used Python to build tools for engineering problems. At home, I started using it for every day tasks. Soon, I realized I hadn't used PHP for several versions of the language. My knowledge of the language was outdated. The biggest reason I'd chosen Wordpress was no longer relevant, because I couldn't write anything complicated in PHP without glancing at documentation to do even simple things. It's sad that I lost the intimate knowledge of a language, but I feel that I've been more productive with Python anyway. Pelican is written in Python. Even more importantly though, it generates HTML files which are hosted. I don't need to run a Python environment on a server. I just need to host HTML files. Markdown Finally, I've fought with Wordpress's text editor countless times. This happened most often when attempting to add code blocks. It was a pain to do. It was a pain to fix when the blocks broke. Pelican supports Markdown. Markdown is supported by large organizations like GitHub, reddit and Stack Exchange. I use all three of those. I know how to utilize Markdown to create code blocks, headers, insert images, create bulleted lists. All without needing to fight how the text editor is going to actually save the data.","tags":"Side Activities","loc":"http://andrewwegner.com/why-i-moved-from-wordpress-to-pelican.html","title":"Why I moved from Wordpress to Pelican"},{"text":"Introduction Stack Exchange has over 130 sites in its network. Each of those sites has at least 3 moderators that are glorified janitors : A lot of the moderation work is extremely mundane, almost janitorial. It's deleting obvious spam, closing blatantly off-topic questions, and culling some of the worst rated posts in various dimensions. The ideal moderator does as little as possible. But those little actions may be powerful and highly concentrated. Judiciously limiting your use of moderator powers to selectively prune and guide the community â€” now that's the true art of moderation. This works so well in the Stack Exchange network because the users are able to handle most moderation tasks. As users gain reputation, they gain privileges. With these privileges, they are better able to maintain and cultivate the content on the site. Not all activity can be performed by users though. This is where moderators come in. I already have a diamond (â™¦) on the Community Building site. I was appointed one of the Pro Tempore moderators during the beta phase of the site. This is my first experience moderating on Stack Exchange, beyond what my reputation on Stack Overflow gets me. It is not, however, my first time moderating . Community Building is focused on various aspects of building communities - both online and off. It caters to users from all aspects of a site (owners, moderators, users, advertisers and any thing else). My experience here led me to pursuing a moderator position on Stack Overflow. This site is orders of magnitude larger than Community Building . It has millions of users and millions of questions. With all this traffic, there are still only 18 diamond moderators. The idea of community moderation shows that it scales well. Nomination Phase The election cycle begins with the nomination phase. Users nominate themselves for the position. In 1200 characters or less, you lay out your platform. In this phase, nearly anyone can nominate themselves. However, there are only 30 positions available. If more than 30 people nominate themselves, then the users with the lowest reputation get bumped. With only 11K reputation, I am at the lower end of the spectrum of other nominees, but I am well above the lowest. I have the 8th lowest reputation. I've looked at previous election cycles. It seems unlikely that there will be 30 or more nominees. I think that I am safe. My Platform I posted my platform shortly after the nomination phase opened. I stressed the work that I've already done on Stack Overflow and previous moderation experience on another Stack Exchange site (Community Building). I'm Andy and I want to be one of your moderators. Why vote for me? I've built an automated script to handle noisy comments . It runs daily, probably to the chagrin of the current moderators, but it helps keep the comments noise down and it is incredibly accurate. In addition to the automated flagging, I participate in the review queues, provide edits and post answers . As a moderator of \"Community Building\" , I know the tools used by the Stack Exchange sites. I've used this position to interact with the existing moderators for advise and questions. This interaction will not only continue but be helpful as a new moderator to SO. I have the flagging history and experience to make the appropriate judgement calls. I feel that is a positive attribute that will help me in helping you to have the best experience you can on Stack Overflow. I'll continue to help improve the site regardless of whether I am elected or not. I will be able to do a much better job at that, though, as one of your moderators. With this, I received my \"candidate score\". It was 29/40. Not the highest, but not the lowest either. I expect, if any one is concerned about this score, it will be mentioned it in the comments. Nomination Phase Comments Updated April 11, 2014 The comments on my nomination post have focused mostly on the pros and cons of the automated script I built to flag comments. I was expecting this discussion. I'd classify most of the comments about the script as \"cautious\". Users seem concerned about something completely automated used to moderate or that I focus on low priority content. My response to this concern was: [...] by automating the removal of this lower priority content it is easier to focus on the actively harmful stuff. I'd like to point out that I have done more than \"build a script\". I've actively participated in review queues on Stack Overflow for years. I've flagged appropriate content (with a high percentage marked as accepted). I have also been moderator on Community Building. All of this, in combination with the automation, makes up my \"platform\". I have the experience of moderation, the knowledge of how the system works and the drive to improve the community. It was also mentioned a few times that I'd be unlikely to utilize such an automated tool as a moderator. I promised to bring this concern up when I moved further into the election process. Finally, I received a few comments supporting either me or the work that I've done. The automation is very impressive - thanks for that, it's a real contribution to the quality of the site. That said - as a privileged user I suspect you won't be able to run these sort of scripts on your account for obvious reasons. â€“ Benjamin Gruenbaum Apr 6 at 21:34 I can vouch for Andy's track record as a pro tem mod on Community Building. I want to note as well for the record that many of the main site questions and answers on Community Building are explicitly about moderation, and his contributions there also provide insight into his approach to the job. â€“ Air Apr 7 at 16:54 Huh. I have to say I'm impressed. Putting you on my list of candidates to watch. â€“ Qix Apr 7 at 19:23 I'd just like to point out that your response to my concerns is fair and reasonable, as well as polite, even if it isn't quite exactly how I see things. It certainly alleviates my concerns of misusing moderator power. I will be seriously considering voting for you. â€“ jpmc26 Apr 8 at 6:41 Overall, I'm pleased with how the discussion has gone. It has been a few days since there were comments left though. Hopefully a little interest picks up during the primary phase. I am also very impressed by the other candidates. There are many very good candidates that I'd be happy to work with (or be unashamed to lose to). Primary Phase Updated April 17, 2015 The nomination phases has ended. I have advanced to the primary phase. The purpose of this phase is to narrow the list of 30 candidates to 10. Those 10 will be the ones that can be voted on to be the next moderators. In the primary phase, users can vote nominees up or down. As part of the election process, one of the other candidates created several tools to help users see nominee activity. He also created a live vote monitor . It was exciting to hang out in the Election Chat room and watch votes roll in. The position of 10th was highly contested toward the end of the primary. I was very pleased to see that everyone remained civil in the chat room too. I ended this phase with 1492 positive votes. This put me in 15th. Sadly, not enough to advance to the election phase. However, there are several excellent candidates that did advance. Good luck to them. Other Primary Data Part of the primary phase involves answering questions that users have posed during the nomination phase. These questions were voted upon and the highest were included in the questionnaire. My response was removed at the end of this phase because I did not advance. I was prepared with answers about my moderation style and my expectations for the position. One of the other tools that was created during this cycle showed nominee activity on the site over time. This was a quick way to compare nominees based on the number of activities they have performed on undeleted posts (because data related to deleted posts is removed from public view). It provided, at a glance, a way to see who is and is not active. Below are the charts of my activity that the tool created. The green vertical bar is account creation date. The orange/brown vertical bars are previous election windows. This is my activity chart for Stack Overflow itself. It shows that I've had an account since 2009, but didn't really start utilizing the site, more than occassionally, until 2013. Since then, it shows that a large majority of my activity onsite is in the review queues. There are posts, revisions, comments and more along the bottom, but most of my activity is in the review queues. My Meta Stack Overflow and Meta Stack Exchange charts show a fairly low activity level. Unfortunately, I think this is obscuring the posts that I do make simply because of the large scale on the y-axis. Finally, the last chart shows the combined activity across all three areas (SO, MSO, and MSE). It looks very similar to the Stack Overflow one because of the lower activity levels on the two Meta sites. Election Phase Updated April 21, 2015 The election is over. Three great candidates were elected. One thing I am like about Stack Exchange's elections is their usage of OpenSTV and the Meek STV method . The results of each round of the election cycle are available at opavote.org . Final Thoughts Like most elections, there are known and unknown winners before the election even begins. The response that Martijn Pieters received from the time he nominated through the final moments of the election all but guarenteed a victory for him. It is a well deserved victory. I hope that he is able to keep up his frequent, high quality, answers in the python tag. If not, his drop in activity will be felt. The surprise, to me, was Jeremy Banks . He ended in 9th place in the primaries. That doesn't mean he can't do the job. I've interacted with all three moderators on site and in chat prior to their victory. I've been very happy with those interactions. With their promotion, I look forward to working with them as a moderator on the Stack Exchange network in the future. Congratulations to the winners!","tags":"Side Activities","loc":"http://andrewwegner.com/i'm-running-to-be-a-moderator-of-stack-overflow.html","title":"I'm running to be a moderator of Stack Overflow"},{"text":"Introduction Stack Exchange receives thousands of questions per day across all of their sites. Not all of these are high quality posts. Fortunately, users of the Stack Exchange network are given tools to help keep that low quality stuff to a minimum. One of these tools is the chat network that spans the Stack Exchange sites. In the chat rooms, a convention has arisen to tag a message as cv-pls for questions that need to be closed for one reason or another. Over time, this evolved to include other tags such as: del-pls for a deletion request spam for notification that spam made it through the already impressive spam filters reopen for a reopen request a few others to cover specific flag types (eg. Not an answer, Very Low Quality or Offensive) Introducing Zephyr The problem with these is that the requests are only seen by users active in the specific room where it was posted. Other users across the network miss the request. Zephyr was built to resolve this problem. Zephyr monitors several rooms where these types of requests are frequent. These requests all all posted into a single chat room . This provides users with a single room to monitor to see requests for multiple questions and sites across the network. Here is an example of what Zephyr's chat activity looks like during a spam wave: How it works Zephyr utilizes the ChatExchange package to join and read the chat rooms. To do this, Zephyr required a dedicated account. I decided to run Zephyr with a dedicated account to completely separate the bot that would sit and watch multiple chat rooms 24/7 from my account. Zephyr maintains a small SQLite database of all the posts that it records. The idea behind this, is that eventually this data will be utilized to train other systems on unwanted content. This information is pulled via the API . Zephyr watches the chat rooms for specific string patterns . If these patterns are matched, a message is posted if should_post is True for the matched pattern. Overall, a nice simple application. It performs some pattern matching and a couple API calls. Other bots In addition to watching user activity, Zephyr also watches two other quality bots that patrol Stack Exchange for low quality content: SmokeDetector and Phamhilator . If either of these bots detect spam, Zephyr takes note of the information by recording it to the database, but not reposting. Since both of those bots post their reports, it didn't make sense for Zephyr to add a second (or third, if both of the others detected spam) message to the chat room. The information is recorded, though, to help future training for other systems. Updates Updated May 8, 2015 Over time Zephyr has been updated to include new rooms to monitor or new patterns to match. Those changes are small (and simple). There are, however, a few larger changes that I'd like to note below. Commands The other bots that Zephyr monitors respond to user input. Zephyr has very little that requires user interaction since all of it's posts are generated by user input. However, there have been times where I, as the bot owner, would like to be able to issue certain commands to it. My most common desire is to see a report of how many spam posts Zephyr has seen. Thus, Zephyr now responds to the command spamreport from me. It then prints out a nice summary of information. This information has been utilized in SmokeDetector to watch for commonly spammed domains. Upgrade from SQLite to MariaDB Zephyr was originally built against an SQLite database. This worked, but was getting slower as more data was being added. This slow down was beginning to affect performance. I started seeing this error more and more frequently: Traceback (most recent call last): File \"H:\\python-virtualenvs\\zephyr-se-voterequests\\lib\\site-packages\\sqlalchemy\\pool.py\", line 255, in _close_connection self._dialect.do_close(connection) File \"H:\\python-virtualenvs\\zephyr-se-voterequests\\lib\\site-packages\\sqlalchemy\\engine\\default.py\", line 418, in do_close dbapi_connection.close() ProgrammingError: SQLite objects created in a thread can only be used in that same thread.The object was created in thread id 4824 and this is thread id 4660 After spending a lot of time troubleshooting and not resolving it to my satisfaction, I decided to upgrade to a more robust database. I'd used MySQL/MariaDB before and I happened to have another application utilizing MariaDB at the moment so that is the solution I picked. The first step was transferring data. I learned that there isn't a decent utility to do a straight migration. So, I took these steps to transfer the data: Export table structures and data from SQLite Convert the SQLite dump to MySQL format. Though both systems use SQL, there are slight differences in dialect. I utilized this Python script as a starting point. It got me most of the way there, but not completely. Data clean up. Ugh. The dreaded part of the job for anyone who handles data. Fortunately, the script above did most of the work. I ended up fixing a couple stray back ticks that didn't convert properly, escaping a very extra quotation marks, and replacing a few \"smart quotes\" (of both the left and right variety). I wish data at the office job was this easy to clean... Import into MariaDB Since the transfer to MariaDB, I've noticed no performance degradation. The error about threads has been eliminated as well. Upgrade to utilize web sockets Originally, Zephyr used the watch method when monitoring a room. This method would long poll the room. It turns out that this is pretty unreliable. I'd get multiple errors through out the week, ranging from Connection Aborted errors to random 404 messages. The solution has been to switch to watch_socket . The only time I've had problems since this switch is when the Stack Exchange web sockets go down. This saves a lot of restarts to get everything up and running again.","tags":"Programming Projects","loc":"http://andrewwegner.com/zephyr-the-bot-that-watches-for-low-quality-vote-requests.html","title":"Zephyr - The bot that watches for low quality vote requests"},{"text":"Introduction This post was originally published by me on Meta Stack Overflow on December 14, 2014. I've republished it here so that I can easily update information related to recent developments. If you have questions or comments, I highly encourage you to visit the question on Meta Stack Overflow and post there. TL;DR: Yes it can. Background On June 27, 2014 Skynet awoke. It looked at Stack Overflow and thought \"Why are all these people being so chatty and talking about obsolete things? I should nuke them all!\" Fortunately, Skynet was a baby and only had access to my 100 comment flags a day. Prior to this activation date, the system was fed with 10,000 \"Good Comments\", \"Obsolete\" comments and \"Too Chatty\" comments. These comments were taken from the Stack Exchange Data Explorer . The \"Obsolete\" and \"Too Chatty\" comment types had to meet the following criteria: Total comment length of less than 100 characters Comment has a 0 score Had variations of the following phrases: Phrases '%mark%answer%' '%mark%accept%' '%accept%answer%' '%lease%accept%' '%mark%answer%' '%thank%you%' '%thx%you%' '%.....' '+1%' '-1%' \"Good Comments\" were assumed, initially, to be anything that didn't fall into the above criteria This provided a base of 30,000 comments that were roughly categorized into 3 distinct groups. Manually scanning the classifications took several weeks, and through this some of the groupings were changed to reflect a more appropriate classification. Not all comments less than 100 characters starting with \"Thank you\" are \"too chatty\", just as not all comments over 100 characters are good comments. I reclassified these comments as if I had encountered them on Stack Overflow. My next step was to train a classifier. I had initially assumed that I'd start with a Naive Bayes to get a baseline and then work to something more complicated from there. Perhaps, extract text features, user information, etc. and build a fancy classifier. My initial tests showed that the Naive Bayes was accurate 80-90% of the time with test data. I combined the classifier's certainty of classification with an acceptable threshold of when I'd allow a flag to be issued in my name. Tuning these threshold took a few weeks but eventually I determined the following thresholds were appropriate for my use: Type | Threshold | Flagging Enabled -------------------------------------------------- too chatty | 0.9997 | True obsolete | 0.99 | True good comment | 0.9999 | False When a comment is classified, if it exceeds the threshold for one of the above, it is recorded into my database for future retraining. If flagging is enabled, the API is utilized to issue an appropriate flag. Obviously, I don't want to flag good comments, but I do want to record them so that I can reuse the data in a later training step. Results What have the results of this experiment been? From my point of view, I'd venture that it's been successful. I have automatically flagged over 17,000 comments. As of December 17, 2014, the process has been running for 173 days. My comment flagging stats are currently: 26885 comments flagged 26714 deemed helpful 171 declined Started at (approximately): 9885 comments flagged 9847 deemed helpful 38 declined This gives me an overall accuracy of 99.36%. Down from 99.61% when no automated process was involved. There are pictures that help tell this story too. In this first one, we see that the rolling 10 day average for the number of declined flags has stayed below two flags a day. In October, there was a two week period where the rolling average was 0 and nearly a month long period where the system did not make any mistakes. Since November, the number of mistakes has climbed slightly. The biggest number of mistakes it has made was the opening day of Winter Bash 2014. Purely speculation, but I believe this was the moderators being protective of content and not wanting people to farm the Resolution hat . Of course, I don't know this. Another theory I have about this uptick since November is the adjustment to day light saving time. My process starts 10 minutes after UTC. It is possible that this earlier hour has caused my flags to be processed by a different moderator, or a moderator that is more awake/less hungry/in a different mood than previously at this point in the daily rotation cycle or because they lost their keys that day. Except for 3 days, since June 27th, the process has flagged 100 comments a day. In this chart, you can see the number of declined comment flags along the bottom. Finally, this chart shows the number of comments that the system wanted to act on (and a rolling 5 day average). When the system was brought online, it was acting on 700-800 comments a day (saving to my local database). Many of these were being classified as \"Good Comments\". You can see the day that I adjusted the threshold for \"Good Comments\" to be acted upon (saved). The drop in the number of comments the system saved is dramatic. Instead of saving 700-800 comments daily, the system now averages about 150 comments to save. Since I don't flag \"Good Comments\", I feel this is the appropriate action to take. Flagged but declined As shown above, I've had comments flags declined. Some of these obviously should have been and required a retraining or threshold adjustment on my part. Others, in my opinion, should have been removed as noise. Below is a small sampling of both types of comments. Recent comments that I feel are noise: yes thank you so much for you help it works sorry for the late reply Wow it works. Thank you very much! wow that works!Thanks so much for your advice! Ok, the works great, thank you so much! Thank you very much for your explanation, you rock dude !!! Here are some comments that were incorrectly flagged: @Spina: yes. Check my answer. You can simply point MONGO_URL to an invalid URL. Sorry, my error. I was: \"position\", not \"display\". Check it: jsfiddle.net/hvfku99c I believe UI.registerHelper is, being deprecated. Please check my updated answer. Other comments are flagged but then edited prior to a moderator seeing the comment. The edit adds information to the post, thus the declination is justified: Yes, I have indexes. Let me show my schema was edited to the much more useful: Yes, I have indexes for UUID and Permission. In fact rlationship is a variable length here (e)-[rp:Has_Pocket|Has_Document*0..]->d Here is the question i had posted first using FIleStorage issue was edited to include the link to the referenced post. It's also worth noting that despite getting flags declined, some comments do eventually disappear. This is due to either flags raised by other community members putting the comment back in front of a moderator or by simply accumulating enough community flags for the system to act automatically. In either case, the desired result of removing noise has been accomplished. Oh, derr. good point. Edited. You're right! Hopefully you see my point anyways. Lessons and Observations Replication to other sites would depend on site culture As a (fairly) non-subjective site, Stack Overflow made a good test case for this. On a site like Community Building , Pets , Parenting or other site that accepts subjective answers, \"too chatty\" would be much harder to classify. +/-1 has been discouraged The observation I made on my own that comments with this type of content were distracting has been noticed by others as well. This was actually a very nice validation of my own process and some of the results posted on that thread show many such comments continue to be noise. Of course, this change did also force users to modify their content and may have added new patterns that can be utilized in future training. Ability to automatically check flags would be great so that automated runs could be paused if it goes crazy The process of checking that my flagging history remains accurate is time consuming. The status of a flag can't be acquired via the API. I've submitted a feature request for this information to be added to the API. With this information, flagging can be paused or stopped if X number of flags are declined. Stack Overflow's volume of comments is a crutch. Due to the high volume of comments and limited number of comment flags my account has available, I can afford to be picky on which comments I want to act on. The classifier itself is about 85% accurate in determining the type of comment. However, I artificially increase my accuracy by only acting on comments that have a very high classifier certainty by forcing this certainty level to meet or surpass my threshold values from above. Smaller sites, with a lower volume, don't have the benefit of having enough comments to be this picky. It is on these sites that a more feature based classifier would be important. The human element is still unpredictable. My classifier was trained utilizing my idea of how comments should be flagged. Prior to automating this, I was not 100% accurate. Additionally, moderators are not 100% accurate in their processing of flags. Users disagree on how these rules should be implemented, but are willing to assist in keeping the site clean. With more than 175K comments a week, every little bit helps. Discussion As my title states, my original question was whether or not I can teach a machine how to flag comments as I would. The answer to that is yes. The next question is whether this type of system would be helpful in cleaning up comments across Stack Overflow. My system works only on new comments created around each new UTC. Once my 100 flags are hit (or the API tells me to stop), it shuts down for the day. Having something automated go through historical comments or that can run all day would be beneficial. Finally, now that I've admitted that I've been automatically flagging comments, can I continue to do so? Update This section has been updated multiple times since the original post. Most recently, it was updated May 3, 2015 As I mentioned in the introduction, this was originally published in December 2014. How is the system behaving now? It is performing very well. Process Changes In January 2015, another user was using a basic query to look for invalid comments. This caused a high number of moderator flags, many of which were declined. My process was caught in this mass decline. This resulted in 49 declined flags for a single day. This is, by far, the largest number of declined flags the process has generated in a day. It did, however, prompt a process change after consultation with the Stack Overflow moderators. The process will no longer flag comments newer than 48 hours old. This provides users with a two day window to see a comment before the system will flag it. This single change has provided a huge improvement in terms of flag acceptance. May 2015 (11 Months) After nearly a year of running, these are my flagging statistics: 39938 comments flagged 39659 deemed helpful 279 declined This provides a helpful rate of 99.3%. This is down just slightly from 99.36% in December. I attribute a large part of the dip to the issue mentioned above. Here is an updated chart showing the rolling 10 day average for number of declined flags. I've had several stretches of multi-week time frames with no declined flags. This is a busy chart, so I've narrowed it down to show just the last 90 days. From here you can see that in the past 90 days there have been only 10 declined flags. Sept 2015 (15 Months) It has been almost 15 months since the process started. In that time, the model has gotten more accurate. Since the last update in May, I've had only 3 declined comment flags: 52351 comments flagged 52069 deemed helpful 282 declined This provides a helpful rate of 99.46%. Here is an updated chart showing the rolling 10 day average for number of declined flags. The 90 day window is not even worth showing. It has three days where a single flag was declined. Summary of 2015 I processed comments 359 days out of the year. I missed three in January due to stopping it after a mass decline of flags (more later), I can't account for a missed day in July and August. I don't recall stopping it, but I missed July 3rd and August 19. I also missed December 28th due to a power issue. I flagged 35,960 comments. Of that, 111 were declined. By month, this is the break down of rejected flags. The blip at the end of November is due to new moderators being elected and adjusting to what other moderators consider \"good\" versus \"bad\" comments. I didn't see the spike in the April election which is interesting, but after a couple days in November it's back to normal. The January spike I mentioned above. Interesting note: The longest stretch in the year with no declined flags was from August 13th through November 24th.","tags":"Programming Projects","loc":"http://andrewwegner.com/can-a-machine-be-taught-to-flag-comments-automatically.html","title":"Can a machine be taught to flag comments automatically"},{"text":"Introduction I've been the owner of Vipers for almost 5 years. In that time I've help the community grown and get stronger. I've helped members begin their own communities and I've maintained the game servers the players utilize 24 hours a day. I've been concerned about low activity, excited about new events and updates, and watched the community ebb and flow through out the years. I've dealt with trolls, spammers, and new community members. I think I've gained valuable experience in managing a community. I'm also a member on Stack Overflow and over the years gotten to know others that enjoy the \"Meta\" aspect of Stack Exchange. That's the \"behind the scenes\", \"how does this site work?\" discussions and activities. Moderators A few days ago, I found a way to merge my experience with Vipers and the knowledge of the Stack Exchange platform. A new community started recently: Moderators . The idea behind this site is for people building, administering, managing or cultivating digital communities to have a place to ask others with similar experiences questions. I love it! One of the things a new site needs on the Stack Exchange network is a set of pro tempore moderators. While Stack Exchange believe in community moderation , there are still some things that require more access. These pro tempore moderators are those people. I nominated myself for one such position. The Nomination This is the nomination I presented to the community. In it, I discuss my experience with Vipers and some of our challenges. One in particular that I mention is the automated language filter we have on the game servers. I am an administrator/site owner of a medium sized gaming community that runs on a PHPBB3 board. We host multiple game servers as well. I've got a team of moderators that help keep the forum and game servers clean. I've run this site for 5 years, after taking it over from the original creator of the community who wanted to move on. In my time as admin, we've seen the number of participants on the forum increase. We've seen our game server population increase as well. I attribute this to getting the community involved in change discussions. One of our biggest changes occurred several years ago. Community members complained that our game servers would be over run with trolls at hours when moderators weren't available and spewing filth. The community wanted a cleaner game server experience. Users wanted these players gone immediately. Previous community leaders felt that trolling of this kind was part of the game and did nothing. After some discussions regarding what was and wasn't appropriate, we decided to be (for lack of a better term) \"family friendlier\". Certain 'extreme' phrases were no longer tolerated at all. A technical solution was built to automatically remove players that violated these rules. This solution allowed users to swear, but once it became excessive (again, defined by the community) they, too, were removed. The tool we have (PHPBB3) may not have the reputation, badges, or increasing privileges used here on Stack Exchange but for my community that has not been a negative. Engaging with the community in discussions and letting the members provide input that me and my team utilize has been extremely beneficial. I have no experience moderating a Stack Exchange site. I don't feel that's a down side though. I can provide the \"outside\" perspective in a Stack Exchange heavy group. That does mean, though, that I'd depend on and expect the community to provide feedback on how moderation in being handled. Much like my existing gaming community, input from the community to the moderation team is important and the moderation team should be listening to that input. Appointment Updated on August 12, 2014 I have been appointed to a moderator position on Moderators. I now \"moderator the moderators\", as the running joke has been on meta and the chatroom. I am looking forward to helping this community grow and to providing my experience of community management outside of Stack Exchange to this position. Renaming to Community Building Updated on December 4, 2014 Almost since the beginning of Moderators, there have been conversations about whether the name is limiting our scope. We aren't a community for only moderation questions. We are a community about how to build communities and moderation happens to be a part of community building. (How many times can you say \"community\" before it sounds weird?) Discussions began in August about possible name changes. These discussions continued into October as we worked on the scope of our site. Finally, on December 2, we received our new name: Community Building . It's the same great site but with a much more relevant name.","tags":"Side Activities","loc":"http://andrewwegner.com/moderation-postion-on-moderators.html","title":"Moderation postion on Moderators"},{"text":"Introduction In last week's post , I showed three ways that appealing bans on the Vipers forums would fail. Everyone makes mistakes and we try to recognize that when someone comes to the forums and makes a good appeal. We usually offer an unadvertised \"last chance\". This is the chance to prove you've changed. If you fail, you are gone for good. If, however, you have changed behaviors, this allows you to play on the Vipers servers again. To get this chance though, you have to make an appeal. Super secret formula for getting unbanned The secret to a successful appeal is to do all of the following in your appeal thread: Post in complete sentences, using mostly correct english. I'm not going to mark you off for simple spelling mistakes, but I'm not going to read your post if it looks like you typed it from your phone to your teenage buddy. Stay polite. If you lose your cool, it's much less likely we are going to want to work with you. Remember, this is a game. You not being able to play on one server is not the end of the world. Explain why you think your ban was inappropriate. Make this short and to the point. Don't lie. I have logs. I know how to read logs. I even have ways of quickly searching through the logs for specific times, if you provide that information. If your story doesn't match what I see in the logs, I'm not going to engage with you. Answer questions from the admins. It's entirely possible you didn't provide a crucial bit of information or we need to wait for input from the banning admin. In either case, if an administrator asks you a question, you should probably answer it. When you do so, follow the first two bullet points above. Summary The sad thing is most of the appeals fail at step 1. These posts are to facilitate communication between the admin team and the player that wants to return to the server. In these appeals I try to educate a player on why they were banned in the first place. If it's difficult to understand what is being said, it's very hard to have this conversation.","tags":"Vipers","loc":"http://andrewwegner.com/how-to-appeal-a-ban-effectively.html","title":"How to appeal a ban effectively"},{"text":"Introduction I've been running Vipers for about four and a half years. In that time I've seen my share of players get banned. The \"good\" ones realize they screwed up and come back when the ban has expired and never have an issue again. These types of players are common if they are first time offenders. The minute someone gets a second ban, the likelihood of them getting another ban shoots way up. Our Rules Vipers has five very simple rules. These are presented every time someone joins any of our game servers. They have to click \"Continue\" to move past them. We have a very simple structure for how bans work. Unless you are incredibly egregious in your behavior, or are out right cheating by using hacks of some kind, you get a 4 strike policy. The ban length for the first three increases for each, but you are allowed back. On the forth, we show you the door. Most of our bans are automated , thus are violating the very first rule. Bad appeals Playing on our servers is not a right. It is not a guarantee. We are not obligated to provide a gaming environment for you to spew your filth. If you can't meet our simple rules, you can leave. If you want to get back into the servers after your forth offense, you have to come to us and ask. You should think about how you get someone to do what you want in \"the real world\". Method 1: Freedom of Speech I think this one is my favorite. Last I checked, I'm not the government of the United States. I'm a private citizen providing a private server for others to play on. My server, my rules (technically, the community's rules, but you understand...) I am not bound by the First Amendment to allow you to say whatever you want. An example: Let's break down some of the flaws: First, and most importantly, we do not have a member named \"DarkWolf\", let alone an admin DarkWolf was banned, automatically, 2 minutes prior to the person making this appeal. The chat logs show that the person appealing even noticed the ban and made a comment. It was this comment that triggered the ban. Not seen in this image is the response when this was pointed out and the appeal denied. The response managed to hit all of the trigger words we ban for on the servers. Just in case we were reconsidering the denial, at this point we're not. Method 2: Admin Abuse Admins are evil and are holding a grudge! They hate me and ban for inappropriate language. Honest, I never said anything bad. It's amazing how quickly that tune changes when I can produce exact, timestamped, logs with their Steam ID attached to it. It's even more amusing when the admin being accused is \"Zephyr\", the automated process that watches for such language. An example: Let's break down this one too: Zephyr is a bot. It is not holding a grudge and it is not stalking you Swearing, unless very very excessive, isn't going to trigger a ban Chat logs show this is the 4th automated ban. In each ban, the user has managed to slip in multiple offensive phrases befor being automatically kicked Method 3: Ranting A rant isn't helpful to anyone. It doesn't endear the poster to the community. The administrators don't want to read through a rambling, unformated, exposition. An example: Our break down: Several logic errors and contradictions to their own arguments Doesn't actually mention they are looking to be unbanned, just that they want to complain about the rules. (It's mentioned in a later post that they'd like to be unbanned) Chat logs clearly indicate the player was fond of certain slurs. This is the reason for the bans Method 4: Threats to the server Threats to attack the server in some way are not going to get you unbanned. There are only two possible outcomes to this appeal type. Either you are successful and the server is off line (now how are you going to play even if unbanned) or we ignore your tantrum and leave you banned. Break down: I have no reason to unban you. You've threatened the servers and made an ultimatum that effectively holds us hostage: \"Either I get to play with you, or I'll take down your servers\". You offer no proof of your claims (unsurprisingly, none existed) Summary Above I've posted three of the most common methods people use to appeal their ban on the Viper servers. Every time one of these types of appeals is posted, it is rejected and the player is told they aren't welcome. All I (and the community) ask is a little bit of respect. You've already proven that you can't follow our rules if we reach this point. If you have truly changed, why don't you demonstrate some of that change while requesting the ban be removed.","tags":"Vipers","loc":"http://andrewwegner.com/how-not-to-appeal-a-ban.html","title":"How NOT to appeal a ban"},{"text":"Introduction On July 11, 2013, Valve released a patch to Team Fortress 2 to limit idling. Idling is the process of launching Team Fortress and then letting it run for a few hours and collecting item drops. It used to be simple to do by utilizing the -textmode game parameter . This would \"launch\" the game without graphics. From there, you just let it run and come back in a few hours with 8-10 new items (your weekly limit). The idea behind this was to then trade or craft these into metal. The metal could be used to trade for keys, hats, etc. and all you to be \"rich\". The quotes are there because I do realize that being rich in a virtual game does not make one rich in the real world. But, since the Mann-Conomy Update introduced this massive meta game to Team Fortress, I've tried to stay out of it. I failed. I didn't just idle one account. I idled 17 accounts. That is between 136 and 170 items a week. That's between 68 to 85 scrap metal a week. That's between 7.55 and 9.44 refined metal a week. That is between 3 and 4 keys a week (depending on who I trade with). Keys are the backbone of the economy. Get enough of those and you can trade for anything else. I honestly don't know what my goal was. I wasn't interested in the fancy hats (ooooh...shiny pixels). I didn't open crates with the keys. I did utilize the keys to get Tour of Duty tickets for me and some friends. That's honorable, right? How did this happen? This all started with the bot I built for Vipers to handle the raffles . I realized that I could automate much more than trading with players. I could automate trading between bots. I could automate crafting - which is relatively time consuming, especially when you have 136 items to go through. I could automate trading with established scrap bank bots. These bots will take any two weapon drops and give you one scrap, even if the two items can't be crafted together. The idea is that they get enough weapons that eventually they'll be able to craft it down to metal. The set up To idle 17 accounts I needed to figure out what my computer could handle. I had to figure out how to run multiple instances of Steam and Team Fortress at a time. Enter an application I'd utilized before: Sandboxie . This application provides isolated sandboxes for applications to run in. Normally Steam won't run more than once on a machine. But, if you launch it via Sandboxie, the host OS (and other Sandboxie environments) can't see that Steam is running in another instance. A bit of experiementation showed that I could handle 6 accounts idling at a time and have the computer remain just barely usable. I created new accounts and split them into which day of the week they'd be run. I had three days out of the week designated for idling. A batch script was built to launch the Sandboxie environments of the day that was to idle. Then it'd launch Team Fortress in each of those environments. Then I'd go to bed. When I woke up the next morning, I'd shut down the environments. I'd repeat this for two more nights. At the end of the third night, I had all the items I could get for the week. Accounts could not be free accounts. This meant that I needed to either buy an item for each account, or trade for an \"Upgrade to Premium Gift\" . Getting items to a single account The next step in the process was to convert all the items to metal. Normally, this would take place by either logging into each account and crafting there and then trading everything to a master account, or trading everything first then crafting. In either case, 17 accounts is a lot to handle and trading/crafting is rather boring. My solution was to modify the raffle bot. I'd designate one account as a master account. This would be the account that received items. All others would dump items to it. I'd log into the account that was receiving items and initiate a trade with each other bot in turn. I'd issue a command add all and that bot would dump it's inventory into trade. Making it through all the bots would take 5 minutes. Previously it would take me an hour or more to log into each account and manually add items to the trade windows and then confirm the trade on both sides. yawn Crafting The next step was crafting items to metal. The rule was that any two weapons utilized by the same class could be crafted to scrap metal. That sounds simple enough. After lots of trial and error, I'd built a set of commands that would select compatable weapons and craft them together. This particular aspect wasn't documented by Steam (or the revese engineered SteamKit2 library I was utilizing). Crafting would take about 15 seconds to get all compatible items to scrap and then combining 9 scrap to get a refined metal. This saved inventory space. With the simple command craftall , all those new weapons would be crafted into metal and then combined into refined metal. This would be done in less than a minute. Previously, I'd have to initiate each crafting session, add all items per craft, wait for the craft to complete and then repeat. This was another 15-30 minutes saved. Left overs After crafting, there would still be left over weapons. There were weapons that had run out of same class items to craft with. The solution was to trade these away in groups of two to ScrapBank.me and get scrap metal back. This could be further combined to refined metal again wasting space. Metal to Keys At this point, human intervention was required again. I had to convert my metal to keys. I did this via trading. The easy way to do this was to find someone running a keybot, that would take metal for a key. Many exist, but all of them overcharge. The other option was to look for a trader that was getting rid of a bulk set of keys and then trade them. In either case, I'd end the week with 3-4 keys. I'd trade these for Tour of Duty tickets and go play a game with friends. The actual work required on my part was starting the idling for the night, shutting down idling in the morning, starting the automated trade, crafting and trade left overs processes and finally trading metal for keys. What used to take me hours to do each week, I could not do in less than an hour (the bulk of which ended up being trading metal for keys). What changed? Valve released a patch to limit how effective idling was. Their \"Mann-Conomy\" was seeing rapid inflation. The price of keys (which they sold for physical money) in metal was rapidly increasing. From the time I started this to the time I ended it, it jumped from 2-3 refined per key to 9-10 per key, with no sign of stopping. Casual players were complaining they couldn't get enough items to trade for this stuff. The patch also required that you click to confirm each new received item. I'm ok. Honest. I tried for a few days to find an effective work around. I didn't. When I wasn't able to find one, I transfered all items to the master account and went through the process of converting to keys. With that, I purchased the final set of Tour of Duty tickets. The idle bots were shut down. It's been a month now. The bots are still down. I'm still around. Looking back on this, I'm happy Valve broke this. It wasn't doing anything for me other than providing me with something to do: \"Need to idle tonight\", \"Need to craft and trade today\", etc. Now I have that time back. I am pleased with the technical challeges I over came to get this done though. Crafting was the biggest, but I think I'm most proud of the automated transfer of items to the master. Since the bots had to communicate via Steam and not via a local application, working out how I was going to do that took some time. That, my friends, was my addiction to the meta game of a hat similator in a first person shooter. I'm over it now and looking forward to some other project.","tags":"Side Activities","loc":"http://andrewwegner.com/i-had-a-secret-addiction-to-the-tf2-idling-economy-but-i'm-better-now-(honest).html","title":"I had a secret addiction to the TF2 idling economy but I'm better now (honest)"},{"text":"Introduction Financially, Vipers is supported by donations from the community. When the community doesn't cover the cost, I end up covering the difference. This isn't my favorite thing to do in the world, but we've been pretty successful in the past. In recent months, though, we've been coming up short more frequently. This has motivated me (and the rest of the admin team to find ways to cover costs). Now we have one. Welcome to the new raffle bot I've built a Raffle bot based on SteamBot , which is the base of scrap.tf . Entries to raffles will be one entry per refined metal. You can have an unlimited number of entries. I will convert the refined metal to various prizes (with the goal being keys most of the time). Then we'll have the system select a set of winners from the entries. A user can only win once per raffle, so even if you you have a gigantic number of entries, your odds of winning more than one position are zero. Only one win per steam id. Using the prices from backpack.tf , the bot will determine the \"value\" of the items within the trade. How does this off set costs? Items that are received that are of high value or any additional keys we can get based on raffle entries will be sold on various TF2 trading markets. The profits from these trades will be used to cover some community costs. Keep high value items for future raffles? This question was added based on feed back from the community. It was added on March 20, 2013 The original plan was to sell any such items. However, due to community feedback, I've changed my mind. We will utilize \"high value\" items as prizes for future raffles. These future raffles will not be announced until any running raffles are complete. It is also possible that such a raffle will run separately from the planned monthly ones. Our first winners Updated on April 3, 2013 Our first raffle completed at Midnight on April 1st. I was surprised by the number of entries we received. I'm even more surprised that the second one has begun and is already three quarters of the way to the number of entries it took a month to receive. People want those keys, and I saw mention of those Bill's Hats too. The number of entries we received allowed us to completely cover the community costs that donations didn't cover. Thank you to all our players that entered! Our first winners are: Cashprizes: Winner of 10 keys Popinfresh: Winner of 7 keys Iamthebaron: Winner of 4 refined metal That Guy From That Thing: Winner of 1 refined metal","tags":"Vipers","loc":"http://andrewwegner.com/give-some-refined-win-some-prizes.html","title":"Give some refined, Win some prizes"},{"text":"Stupid soldier spam The appeal of the crit server is fast game play, overpowered shots, and nearly instant death if you aren't paying attention. The down side is the soldier spam. Lots of it. It's not unusual to have a team of soldiers spamming rockets. This is part of the reason we stuck a class limit on Soldiers. Pyro is a common way to counter a soldier firing at long range. The problem with pyro is that it has limited long range weapons in return. Unless you can sneak up on an enemy (not easy with spam and some of the maps), the pyro is stuck taking pot shots with either the Flare gun or the shotgun. Two weeks ago, I added a plugin to the server that made Pyro much more effective at helping the team without needing to advance to the front line constantly. Reflectiles Reflected Projectiles - Reflectiles, if you will - becoming homing projectiles when a Pyro air blasts them away. These newly tracking projectiles will track an opposing team member and hunt them down. If the player being tracked dies before the projectile hits them, the projectile will select a new target. Well, that seems unfair. How do you defend against a homing projectile as a soldier? It is called Team Fortress 2. You have team mates. Utilize them. That homing projectile can be reflected again by a Pyro on your team. Each time a projectile is reflected it gets just a bit faster. Source Code Updated May 20, 2015 with link to GitHub instead of the old SVN, my apologies for missing that link when migrating to this blog It is important to note that this version hasn't been updated in a LONG time but was still functioning when Vipers shut down. If it doesn't work, the first thing to try is updating SourceMod's gamedata. This was the fix every other time it didn't work The source code is released on Github. The repository is: https://github.com/AWegnerGitHub/Vipers-Server-Plugins","tags":"Vipers","loc":"http://andrewwegner.com/homing-projectiles-are-awesome!.html","title":"Homing projectiles are awesome!"},{"text":"Introduction My admin tool of choice for the TF2 servers is HLSW . It's decent at allowing me to manage a server without ever needing to log to the server. My biggest complaint about it is that I can only watch the game chat of one server at a time. Sometimes, it's helpful to see an ongoing conversation to resolve minor problems before they become big ones. For example, claims of \"hacking\" usually turn out to be completely baseless. But, if multiple users (and more importantly, multiple trusted users) suddenly start mentioning a hacker, I can step in and resolve the problem without entering the server. HLSW is good for this. A hacker is confined to one server. The biggest problem is when there are reports of lag across all of the game servers. Vipers has a dedicated machine that runs 5 game servers. If all five suddenly report lag, there is a problem somewhere. With HLSW, though, I can't see all of the servers at once. Thus, I've built a tool... Chat Monitor All chat that occurs on the servers is logged. I've used this to resolve complaints of unfair bans and reports of hackers. I built in a hook to these logs from the application template to quickly pull known aliases of users. It's been invaluable in solving problems of \"what happened\" on the servers. I've expanded it's usefulness. Now I can load a single page and see all chat activity occurring on all active game servers on a single screen. It provides, at a glance, a quick way to see if there are problems on the servers. It also allows me to step back from picking which server I think will be \"bad\" and monitor that. Now I can monitor all of them at once. Inappropriate words As a community, we've chosen to set a higher standard for our players. As such, we have a restriction on a total of 3 words and a few derivatives of those words. This system is in place because the community stepped forward and wanted to clean up the experience on the servers a bit. The problem with these higher standards is that we don't have admins on the game servers (or watch chat logs) 24 hours a day. Thus, while admins sleep, a troll can wander through and spew garbage. Unless a user reports this behavior, we will never be aware of it. I've built a system to handle this automatically. The system will monitor chat logs across all servers. If a user hits the threshold for banning, they will be removed from the server and banned for a day. The logic to the system is this: Automated removal logic Inappropriate terms are configured with a \"weight\". This \"weight\" will be used to calculate whether or not a user surpasses a threshold set for being banned. System monitors chat logs for configured terms. If a term is found, the offending message is saved. The term weight is added to the user's current threshold value. If this is the user's first time saying one of these terms, they start at 0 and this weight is added. If user exceeds the threshold, the system issues a ban to Sourcebans. The user is then kicked from the game server. The ban length will be 1 day. The system keeps messages for a total of 5 minutes. If a message is older than that, the system forgets it. Currently, the three inappropriate terms all have a threshold of 1 . This means they saying the words results in a ban. Homophobic, racist remarks aren't welcome on the Viper servers. We can't prevent it, but we can deal with offences swiftly. The 5 minute window is added because the community requested that excessive swearing also be limited. We don't want to outright ban it, but they don't want a swear filled rant to occur after every match. Thus, I built in the 5 minute window and the thresholds. The system is configured to catch common swear words, but the words have a low weight. It'd take repeated spamming of the words in a 5 minute window to reach the threshold and be removed from the server. Updated removal logic Updated May 17, 2012 The automated system has been active for almost a month. I'm finding that the system has been removing the same set of players every other day. They aren't learning. This is despite the message they are shown when removed from the server. I've made a change to the logic in how long a ban will last. It provides a 4 strike system: First offence: Weight of term(s) said times 1. This means, for most cases, they are issued a single day ban. Second offence: Weight of term(s) said times 3. This means, for most cases, they are issued a three day ban. Third offence: Weight of terms said times 21. This means, for most cases, they are issued a three week ban. Forth offence: Permanent removal from the game servers. The community has been very enthusiastic about how quickly users of inappropriate terms are removed. I've seen a few minor complaints about the permanent removal of users on the forth offence. I've told the community that if a user protests the ban and if they can show they've learned our rules, I will provide one additional chance after the user has waited a minimum of a month from from last time they were banned. If they return to their previous activities, they will be re-banned and they will not be able to return in the future. Update at shutdown Updated January 20, 2015 In January 2015, Team Vipers shut down . With that shutdown, all chat monitoring also shut down. The system was active from April 23, 2012 to January 4, 2015, for a total of 987 days. In that time, 4457 bans were issued for inappropriate language. That is over 4 users a day being removed from our player base because they couldn't maintain a respectful attitude. I consider that a success. I believe Viper community members did too.","tags":"Vipers","loc":"http://andrewwegner.com/monitoring-language-on-the-game-servers.html","title":"Monitoring Language on the game servers"},{"text":"OSI PI is a historian database. I had a task to connect a python application to this database. I asked a question on Stack Overflow about whether this was a simple problem to solve. After two weeks I still hadn't gotten a viable response, so I had to build by own solution. I did reach out to the vendor first for help. Their response back was not helpful. Looks like pyodbc is written against ODBC 3.x. The OSI PI ODBC driver is using ODBC 2.0. The python ODBC driver manager will convert most ODBC 3 calls on the fly to ODBC 2 ones. Anything added to 3, however, will obviously fail. You would need to find some way to make sure that your only using 2.0 compliant ODBC calls. Currently their is not a PI ODBC driver that is compliant with ODBC 3.0. So, it looks like the vendor doesn't support Python (odd, they are named \"PI\", but I digress). Additionally, the drivers provided by the company initially didn't work. The code below shows how I was able to finally connect python to OSI PI. It may not be the most elegant, but it functions for the purposes of my application. Initially I was attempting to connect using the pyodbc module. Unfortunately, OSI PI would return a message like this: pyodbc . Error : ( ' IM002 ' , \"[IM002] [OSI][PI ODBC][PI]PI-API Error <pilg_getdefserverinfo> 0 (0) (SQLDriverConnectW); [01000] [Microsoft][ODBC Driver Manager] The driver doesn't support the version of ODBC behavior that the application requested (see SQLSetEnvAttr). (0)\" ) They vendor mentioned that using OLEDB instead may prove more fruitful. Thus, the code below is how I got connected using the vendor provided OLDEB driver. The downside is that I also had to do this all through COM objects using win32com . I'm not knocking the module, because it is extremely useful and I've done some great things with it. from win32com.client import Dispatch oConn = Dispatch ( 'ADODB.Connection' ) oRS = Dispatch ( 'ADODB.RecordSet' ) oConn . ConnectionString = \"Provider=PIOLEDB;Data Source=<server>;User ID=<username>;database=<database>;Password=<password>\" oConn . Open () if oConn . State == 0 : print \"We've connected to the database.\" db_cmd = \"\"\"SELECT tag FROM pipoint WHERE tag LIKE 'TAG0001%'\"\"\" oRS . ActiveConnection = oConn oRS . Open ( db_cmd ) while not oRS . EOF : #print oRS.Fields.Item(\"tag\").Value # Ability to print by a field name print oRS . Fields . Item ( 0 ) . Value # Ability to print by a field location oRS . MoveNext () oRS . Close () oRS = None else : print \"Not connected\" if oConn . State == 0 : oConn . Close () oConn = None I followed up on my Stack Overflow post about 2 months after posting my solution with the following note: Just following up on this after using it for a couple months. This is still the only way I've found to do this with python, but it seems to be very slow when I need to run a large number of queries. I suspect it is because I have to open/close the database connection for each query, but OSI PI/ADODB complains if I do not. Performance has not reached a point where I am forced to rewrite this yet. If/when I do I will follow up again. In the meantime others using this solution should be aware that it is slow when running many queries.","tags":"Technical Solutions","loc":"http://andrewwegner.com/connect-python-to-osi-soft-pi.html","title":"Connect Python to OSI Soft PI"},{"text":"What happened? Over the Christmas holiday, the Team Vipers forums were hit by \"multiple\" users who decided it was a good idea to use our community as a battle ground. A Viper member stepped in to \"help\" resolve the problem by attempting to play mediator. (Notice the air quotes. These are important later) The battling users turned on this member and escalated the matter to threats of violence. When the member stopped responding, the users got bored and began spamming the board. Due to a combination of it being Christmas and the admins doing Christmas-y things with their families, this went on for hours. There were several rounds of clean ups, but the battle continued over nearly a 48 hour period. Why is this troll different? After Christmas festivities had ended and I had time to sit down in front of a computer to investigate what happened, I began piecing together a time line of events. It turns out that all of these \"users\" were coming from the same IP address. On top of that, they shared the same IP address as the member that attempted to \"help\". Odd... A little more background The member that was \"helping\" had recently been approached by the admin team in an effort to clarify some questions about their age. One of the admins had been presented evidence that the user was under the age of 16, which we all know if the minimum age to be a Vipers member. The user took this as a threat that we were going to remove them from the community entirely via bans on the servers and forums. They posted a rant on their \"sub-community\". To mean this seems much more like a place where people that didn't like our rules go to complain about us in an unproductive manner. I'm still not sure why they set up this community in the first place, because they have always expressed a very keen interest in Vipers. Follow up with the member When I started seeing a pattern between the battling users and the member, I sent a Steam message to the member asking if they were around so that we could chat. The conversation began with me asking about the rant they posted and whether or not they were aware that we allowed players under 16 to play on the servers, but not be members. They said they were not aware of this and seemed pleased they'd still be around. I thanked them for their attempt to help over the holiday. Then I mentioned that it was odd that all of the users were coming from the same IP address. It was agreed that was odd. When I said that there was another user that used that IP in the past, they stopped responding. A few hours later I got a message saying they were back and around to talk again but they'd closed the Steam chat window and lost the conversation. A few minutes later they began complaining that their own forum had been spammed. I'll admit I was curious, so I went to take a look. There were multiple \"users\" having a very similar battle across the other forum. The member had posted messages as the voice of the community that it had to stop or they'd issue bans. I was sick of the game at this point. I mentioned that I knew they were all of the users involved. I'd issued bans to the user accounts on the forum. I revoked their Viper membership and banned their main account. I also made a note that they could appeal the ban in 6 months time. I figured they'd forget about us. It takes a special kind of troll to spam their own site to try and convince someone you aren't a spammer. After effect of the bans At this point the member was less than pleased. There were vague threats made against me, the server, and anything they could think of that would offend me. I simply ignored the user on Steam. This didn't sit well, apparently. The user attempted a denial of service attack on the game servers, which our hosting provider mitigated. Part of this mitigation involves sending me a list of IP addresses that were attempting to connect to the server at the time of the attack, and part of the traffic that automatically triggered their counter measures. Unsurprisingly, there were only a few IP addresses. Geolocation says they are all in the same city. One of those IP addresses is the IP of the user. What is going to change? The biggest problem I see out of all of this is that we didn't have active admins during the initial wave of spam. I can't fault the admin team, after all it was Christmas. However, there have been a few requests recently for a couple more game server admins. I feel this incident just confirms that we need a couple more. In the coming days, we'll announce who our new admins will be. The member that has been removed has been banned from the servers and the forums. We've also destroyed two new user accounts they created since the spam wave. This is business as usual. I did offer them the chance to appear at the end of June. I suspect they will not. The appeal This section was added on July 7th, 2011 after the user appealed their ban It seems I was wrong. They filed an appeal to their ban. Of course it took place over another holiday (4th of July). Fortunately, our two European admins were still around while the rest of us were out enjoying some fireworks. The appeal was filed on the forums, as is usual. Community members stepped in to experess their unhappiness that the user would consider returning after what they did over Christmas. One of the admins that were around put a message in the admin area and asked for feedback from other admins. When I returned home after some amazing fireworks, I found I had an email from our hosting provider. They'd blocked another attempted denial of service attack. I logged into the forums to see what was going on and noticed that the appeal thread was not going well. The user were getting more and more upset at the community members who were rallying against their return. It eventually ended with a note saying that they hope Vipers never recovers from the DDOS. At this point one of the other admins locked the thread. A check of the IP address the user was utilizing to post the messages and the list of IPs involved in the attempted DDOS showed that the IP was on the list, and again they were all in the same city (and the same city as in December). Their appeal was rejected. They were informed they would not be welcome back. After this appeal, I don't believe anything will need to change on our side. The appeal process concluded faster than normal. Attempts to DDOS the community server makes the decision rather easy for the admins.","tags":"Vipers","loc":"http://andrewwegner.com/a-special-kind-of-troll.html","title":"A special kind of troll"},{"text":"There are times when a server can be allocated more than one IP Address even though it contains only one physical network card. To associate these IP addresses with the server some manipulation of networking settings will need to be performed. The steps outlined in this walk-through are for RedHat based systems. This tutorial is for statically assigned IP Addresses (as a server generally will have). For this walk through we are going to add one additional IP address to eth0 . Navigate to cd /etc/sysconfig/network-scripts Copy ifcfg-eth0 to ifcfg-eth0:0 cp ifcfg-eth0 ifcfg-eth0:0 Now we need to modify the new file slightly so that it gets it's own IP address. Open ifcfg-eth0:0 in your favorite editor DEVICE=eth0:0 <-- Change this to match the new eth0:0 file we just created BOOTPROTO=none BROADCAST=x.x.x.x <-- This is the broad cast address for the subnet the new IP is on DNS1=x.x.x.x <-- This is the main DNS server you are using (example: 64.120.14.26) GATEWAY=x.x.x.x <-- This is the gateway address for the subnet the new IP is on HWADDR=<DO NOT CHANGE> <-- Don't change this from what is existing. The Hardware address is the same as the physical one IPADDR=x.x.x.x <-- This is your new IP address NETMASK=x.x.x.x <-- This is the netmask for the subnet the new IP is on ONBOOT=yes <-- Leave to yes OPTIONS=layer2=1 TYPE=Ethernet PREFIX=29 DEFROUTE=yes NAME=\"System eth0:0\" <-- Change to reflect new name of device Save your file with the new settings. Now we need to restart the networking service: service network restart When the network components come back up you should see your new device in the ifconfig command. To add more IPs, copy and replace values as specified above.","tags":"Technical Solutions","loc":"http://andrewwegner.com/multiple-ip-addresses-on-the-same-physical-network-card.html","title":"Multiple IP addresses on the same physical network card"},{"text":"For various reasons, MyISAM tables are known to crash. When this happens, the following message will be displayed: INVALID SQL: 145 : Table '{something}' is marked as crashed and should be repaired I've found this error occurs when MySQL is unexpectedly shut down - whether from a power failure to the entire server or if MySQL itself has issues and you use the kill command to stop it. Unexpected shut downs, especially while these tables are being used, do not make MyISAM tables happy. To fix this, you need the ability to stop MySQL in a controlled manner, and you need to know where the database files are stored. locate *.MYI This will return where all the MYI files are stored. In this example, I am using /var/lib/mysql/mysql/ Go to the directory of the crashed table using the cd command. Next, stop MySQL. This is to ensure the tables are not accessed while we perform our repair functions. If you don't perform this step, the repair may not succeed. service mysqld stop Next we are going to perform two repair functions. The first one may take a while depending on the size of your tables. myisamchk -r --force --safe-recover *.MYI The second repair step is used to ensure all table states are updated correctly and repair any minor indexing issues. It is likely that this step is not needed after performing the previous step, but it should only take a few seconds now. myisamchk --force --fast --update-state *.MYI Finally, restart MySQL service mysqld start","tags":"Technical Solutions","loc":"http://andrewwegner.com/fixing-myisam-crashed-tables.html","title":"Fixing MYISAM Crashed Tables"},{"text":"Introduction Not to long ago, applications for new membership in Team Vipers consisted of someone wandering onto the site, posting a few words and an administrator saying they were accepted as a member. This worked when we were a small community. We aren't small any more. We have 5 servers and hundreds of players a day. Each server has their own sub-community. There are players joining the forums than entire groups of people have never met because they play exclusively on one game server. Admins were also inconsistent in how (or if) they voted. Some admins didn't realize they could have a say, thinking it was a privilege granted to only the senior administrators. We've built a system to resolve many of these problems and to make the administration side easier. What's new? The new system presents all users with an application template. They fill out the form and the system handles the rest. The New Users and Applications subforum has been modified so that no one, except the bot, can create topics. They topics will only be created when the form is submitted. When a user applies to join Vipers, we will automatically include relevant information about the user as we know them: HLStats information: This helps us get a sense of where they play and how often they play. It will help admins identify users they should recognize based on the servers they frequent (because we all know certains servers are better than others ;) cough Vanilla Nest vs Crits cough ) Ban information: This will check if the user has any recorded bans in Sourcebans . It's important to know if the user has been banned previously. Known aliases: Pulling information from our chat logs and Valve's profile page, we can build a list of known aliases. This helps identify users that frequently change names but have been around a while. There may be other features we add in the future as well. Updated January 2012 I've removed HLStats from the servers and removed it from new application information. We have added a check of a user's Steam Reputation instead. After a user applies, they are put into a two week hold. During this week it is expected they will stick around the forums and learn about the community they just applied to. Even better would be that they had done this before applying. While this two week hold is in place, the administration team will be able to cast they votes in a separate sub-forum. They can hold administration specific discussions - usually details that are important for admins to know, but don't need to be public. Once voting is complete, if they become a member, the system automatically grants appropriate forum and server related permissions. Voting rules Three admin \"Yes\" votes and zero \"No\" votes grants the user membership immediately after the two week window has expired One or two \"No\" votes places a message on the user's application that the administrators are still considering the application Three or more \"No\" votes rejects the application Possible Responses Accepted by admin team If you reach a total of three or more admin \"Yes\" votes, and do not get three or more admin \"No\" votes, you will be accepted as a member of Vipers and automatically have your forum access modified. You will receive a message similar to this: Denied by admin team If you reach three or more \"No\" votes your application will be denied. This will occur even if you receive more \"Yes\" votes than \"No\" votes. Vipers is not a majority rule community. The decision has been made that if three admins do not feel comfortable accepting your application, you will not be granted membership. You will receive a message similar to this: Denied due to lack of votes To be accepted, your application requires a minimum of three \"Yes\" votes. If this can not be reached (and you also can't reach three \"No\" votes), your application will be rejected due to lack of votes from the admin team. This means that the admin team does not feel strongly either way about your application. Post on the forums. Play in the servers. Get to know our players and the community at large and then try again in a month. You will receive a message similar to this: Denied because of age Repeat after me: \"Age does not equal maturity.\" However, it has a very strong correlation. Over time we have learned that younger players tend to bring a lower maturity level that most of the community does not care for. As such, we've decided to set a minimium age requirement of 16. If a user indicates they are less than that, the system will reject their application with a message similar to this: Updated January 2010 This process has been in place for a few months now. It has gone very well. We've reduced the clutter in the applications forum. We've also seen the number of \"forgotten\" applications drop dramatically. Original Announcement The original announcement is posted here for future reference. Zephyr is an automated robot designed to improve our new member application process. The current process, which involves copying a template to a new thread and the potential for applications to become misplaced, is cumbersome and inefficient. The goal of Zephyr is to remove as much of the manual process as possible. Now, our new applicants will fill out an actual application online. The same information will be requested, but it will be in a more reliable format and will not require an applicant copy and paste anything between threads. This new application will be posted in the same forum as you're used to, and members will be freely able to comment and discuss the applicant via that thread. It will also display which date admin voting will be open, which is two full weeks after the original application post. This will hopefully cut down on any confusion related to the delay between application and voting. As another note, from now on Zephyr will be the only user capable of creating new threads in the New Member Application forum. As previously stated, members will be able to post comments on existing threads, but the only new threads will come from the application process. This will keep the forum cleaner and help prevent applications from becoming lost or forgotten about. Once again, Zephyr is an automated robot. It is not programmed to respond to comments or questions. Doing so will not get your question answered. As always, if you have questions or comments about the application process, Zephyr, or anything else, you're welcome to send them to any admin. We'll be more than happy to help. You may all bow to your Robotic Overlord now.","tags":"Vipers","loc":"http://andrewwegner.com/automated-template-for-membership-applications.html","title":"Automated template for membership applications"}]}