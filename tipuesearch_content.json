{"pages":[{"tags":"Review","url":"docker-swarm-beginner-advanced.html","title":"Review of Udemy's Docker Swarm: Beginner + Advanced","text":"Introduction I recently found short two courses about Docker on Udemy there were listed as free by the author as they get ready to update the course. This is a review of Docker Swarm: Beginner + Advanced by Luke Angel. This is the second course I've reviewed by this author. The previous course was reviewed here . I have a very basic idea of what the container/microservice architechure is designed to solve. I've never used it professionally or on personal projects. My experience is limited to reading articles about technologies such as Docker or Kubernetes and thinking that I'd like to try those out at some point in the future. Course This is another hour long course and promises \"foundational knowledge\" by the end of the course. I disagree with this. The course starts with a lecture that is essentially an ad (almost in the form of a TV episode trailer) for the course. There are duplicate lectures, which seems to be a recurring problem based on the same issue in the previous course. Another problem that has carried over from the previous course is audio issues and inconsistances. This is not a beginner course, despite the name. The author assumes knowledge of Docker and does not explain concepts when the \"demo\" starts. Speaking of the \"demo\", this isn't a true demonstration of the product. Instead, the author provides a ZIP file of commands that are run and outputs of those commands. In the lectures, a few elements of the output are highlighted and explained. I didn't find this to be a useful demonstration. Final Thoughts This is a course that is several years old. It is not well put together, with multiple lectures being duplicated. Sound issues plague the presentation throughout the hour. These are all similar complaints to the previous course . This course also includes a demonstration, that is really just looking at the output of an attached text file. For a demonstration, I was expecting to be able to follow along, as as this is \"Beginner + Advanced\", I was expecting some kind of instructions on how to set up my environment. There is none. I don't recommend this course (or the previous one ) because of how little \"useful\" content is fit into these two hours."},{"tags":"Review","url":"docker-essentials-course.html","title":"Review of Udemy's Docker and Containers: The Essentials","text":"Introduction I recently found short two courses about Docker on Udemy there were listed as free by the author as they get ready to update the course. This is a review of Docker and Containers: The Essentials by Luke Angel. There is a third free Docker course by the same author as well. I have a very basic idea of what the container/microservice architechure is designed to solve. I've never used it professionally or on personal projects. My experience is limited to reading articles about technologies such as Docker or Kubernetes and thinking that I'd like to try those out at some point in the future. Course This course is a one hour, very high level, overview of what containers are. The course description mentions that there won't be detailed developer examples, and this is true. My issue with the course is that it is to high level. This course is not for anyone with even a tiny idea of what a container is. The author has basically created an hour long, narrated Power Point presentation. Even worse, there are at least two instances of lecture that are duplicated (and are pointed out in the community feedback and Q&A section). The narration is obviously edited from multiple microphones and the sound consistency is uneven. The author also spends lectures typing on their keyboard while talking. This is loud and distracting. Motivational speaking makes it into one of the final lectures, explaining that we are winners for being on the right track by studying Docker. Final Thoughts This is a course that is several years old. It is not well put together, with multiple lectures being duplicated. Sound issues plague the presentation throughout the hour. The content of the course is accurate - as of a few years ago - but has areas that are outdated at this point. I don't know how long the author is spending on updating their courses, but there is a lot that needs to be done in just this one hour course."},{"tags":"Review","url":"git-45-minute-crash-course.html","title":"Review of Udemy's renamed The 2018 Git Complete: 45 minute crash course using Angular.","text":"Introduction I use Git extensively in both my personal and professional work. I've set up GitLab for my personal work. I know enough about Git to be effective in a small environment with a handful of developers. I found a 30 minute crash course about Git on Udemy and decided to take it with the goal of learning a little bit more. The course I found was The 30 minute crash course to learning GIT 2 by Ricardo Morales. Spoiler : I rated this course poorly on Udemy and since that rating, the course name has been changed to \"The 2018 Git Complete: 45 minute Crash Course using Angular\". The course has been updated slightly to add 15 more minutes of content since I completed the course. I have watched those as well and have not changed my rating. Course When I took this course it was definately incomplete. The very first lesson starts with \"In our previous lecture...\". Obviously, as the first lecture, there isn't a previous one. The 15 minutes of new content does add a few introduction lectures before this, so it's not as jarring, but the order of lectures is still out of order. There are next instances of \"In the next lecture I'll cover...\" and then the next lecture is something else, or \"In the last lecture we talked about...\" and the mentioned topic was a few lectures ago. The course is a very quick run down of common Git commands. Very quick. Most lessons are under a minute in length. What this means is that there is almost no explaination about the command you're about to learn. In most of the lectures, it's a quick reading of the command's \"help\" sentence, then typing the command in a terminal window to show the syntax. Unfortunately, there isn't an over all project and the order of the lectures is out of order, so it's difficult to see how a particular command actually works since there isn't any set up done ahead of time. One example of how this is poorly demostrated is in the lecture on renaming a file. The example uses mv to rename a file, talks about how it's been renamed, and then uses mv to rename it back to the original name to that there \"won't be problems with Git.\" The entire point of the lecture is to show how this should be accomplished and this lecture misses it's mark entirely. Another example is in the lessons on handling merge conflicts. This is an important topic because conflicts need to be handled when multiple developers are working on the same thing. The lecture on handling these though talks about the theory of handling it but doesn't provide a demo. The same thing is done in the topic on rebasing. Minute long lectures are not a good format. The lessons are rushed to the point where there is almost no content. Reading the \"help\" sentence and then showing the syntax doesn't make this a crash course. It makes it an audio/visual help document. Final Thoughts If you know anything about Git, avoid this course. If you don't know anything about Git, I'd avoid it too, unless you prefer listening to help documents instead of reading them. There isn't any novel content here. Examples are lacking, at best. Lectures are out of order and when I took it, missing entirely. The newly renamed course to include \"using Angular\" is incredibly misleading. Angular is used only to generate a project that can be committed to Git. Nothing done here uses Angular other than generating that project. If you know anything about Git, there is nothing here you don't know. The completion award below reflects the name of the course before it was renamed."},{"tags":"Technical Solutions","url":"ubuntu-backup-rsync.html","title":"Backing up Ubuntu laptop to Ubuntu Server with passwordless rsync","text":"Introduction The server has been running for almost nine months. It's been backing up family data and pictures from phones without any problems. Now it's time to back up the laptop because I have the space and really should make sure the stuff that isn't work related (ie. the stuff that is in the work git repositories) is also backed up. Enter rsync . How To My goal is to automatically back up my home directory from the laptop to the server on a daily basis. This will provide a once a day backup and if I need to do more than that in the future, it will be as easy as modifying the final cronjob that I'll use. SSH Key The first step is setting up an SSH key so that I don't have to manually provide a password. I can, in the future, add restrictions on the server side as to what this particular key will be able to do too. I'm not doing that today though, because I don't open SSH to the outside world. The first thing to do is generate a new key. I already have an SSH key configured, but it has a password. On the laptop, run the follwoing: ssh-keygen -t rsa -b 2048 -f ~/.ssh/laptop-rsync-key When asked to enter a passphrase, simply press enter and then enter again to confirm the empty passphrase. This will put laptop-rsync-key and laptop-rsync-key.pub in my user's .ssh/ directory. Copy the public key to the server Next, we need to copy the public key that was just generated to the server. scp ~/ . ssh / laptop - rsync - key . pub andy @192.168.140.187 :~/ . ssh Once it's been copied, log into the server. Now you need to add this key to the authorized_keys . cd ~/.ssh cat laptop-rsync-key.pub >> authorized_keys rsync command The final command to back up my home directory is pretty simple. This command is going to tell rsync to use the new SSH key that was just created, to exclude all dot files and directories, and to delete anything that has been removed on the laptop from the server. The backup will go in ~/backup/laptop on the server. rsync -a -e \"ssh -i ~/.ssh/laptop-rsync-key\" ~/ andy@nas:~/backup/laptop --exclude=\".*\" --exclude=\".*/\" --delete Once I confirmed this worked, I added it to my user's crontab on the laptop. It will run once a day now. Next steps The next steps I'll take be taking are to restrict the new SSH key on the server to only allow it to perform rsync tasks. This can be done by slightly modifying the appropriate line in authorized_keys . I'll see how this daily, single, back up works for a while. If I need to, I may change it to a rotating weekly backup. I don't forsee that right now, but I need a few weeks of seeing how this works and if the single day is good enough."},{"tags":"Review","url":"learn-davinci-resolve-15-from-scratch.html","title":"Review of Udemy's Learn DaVinci Resolve 15 from scratch","text":"Introduction It's been a while since I've done a course review. I wanted to try something new in an area that seems interesting, but I'm not sure if I want to do anything with it hobby-wise. Learn DaVinci Resolve 15 from scratch caught my attention, and I found a coupon code that allowed me to take the course for free. Video editting sounds interesting and I always have the \"do something with family videos\" on my very distant TODO list. The course is two and a half hours long and it seems to be a basic class. DaVinci Resolve is a very powerful, professional piece video editting software. The free version has everything I'd ever need for home use or even YouTube content. The paid version adds in multi-user collaboration features, additional 3D tools and new Resolve FX. Course The course focuses heavily on learning the interface of DaVinci Resolve. There are a few basic tutorials and the author promises more in the future. This isn't exactly what I was expecting with a course description that includes Learn how to use DaVinci Resolve to create beautiful and stunning videos for the world to see. That said, it does provide a good overview of each of the tabs in DaVinci Resolve. The explaination of what each tab is used for and the very basic examples of how to use each give users an idea of how powerful DaVinci Resolve can be. Thoughts on the lessons As an overview of the interface for DaVinci Resolve, this course is good. As a \"learn from scratch\" course, the interface overview quickly shows that two and a half hours isn't enough time to learn the software beyond the very basics. The title is a bit misleading and as someone who didn't know anything about DaVinci Resolve and only took this course because of a potentially budding hobby, I was hoping for more than a two hour tutorial on where to click in the interface to do the basics. Alas, my ignorance of the topic meant I wasn't able to judge the course accurately. I had several nit-picks with the content too. For one, I was immediately turned off to the course creator when they begged in the first lecture to not give them bad reviews. That isn't the type of thing I expect from someone who's put time and effort into creating the course. I was worried when this was done, but it turned out to be better than the begging implied. My second complaint is about how the content is presented. This is a course on video editting . I expect a certain amount of care to be put into the presentation of such a topic, but the course wasn't well paced. There are very abrupt endings to several lectures. There are other lectures that have long periods of silence at the end. There are also more than a few occassions where the audio and video seem to be out of sync. You never see the instructor, but when they are presenting a new tab and say something like \"Click on this tab\" and then the mouse doesn't move to the tab for a few seconds it makes it hard to follow along. My final complaint is how incomplete the course is. There is one five second lecture that is just \"This lecture will be uploaded very soon\". The last bonus lecutre ends with \"See you at the next lecture\", but there are no more lectures. I realize the course was released this month, but it's incomplete. The full price of the course is $80. I expect more to be completed for that price. This is a good overview of the DaVinci Resolve environment. I understand what each tab is and how I can use them for basic edits. I feel comfortable enough to make a very simple video with some home videos now. Final Thoughts If you are paying for the course, you should avoid it for a while. It's not complete. There is no guarentee that additional lectures will be added and the promise of \"more to come\" isn't worth the full price. If you are looking for an overview of the most important buttons in DaVinci Resolve, this is a good course. If you are looking for details on what every button or menu option is going to do, this course doesn't cover that. Finally, if you are really looking to \"learn DaVinci Resolve\" and not \"learn the DaVinci Resolve interface\", this isn't the best course either. There are some tutorials here, but they are not the majority of content."},{"tags":"Technical Solutions","url":"slack-app-google-cloud.html","title":"Deploying a Flask Slack app on Google Cloud Platform","text":"Background and goals At work I am the software QA team lead (I haven't given myself a fancy title, but I should). As such, I spend a lot of time in JIRA tracking our bug and feature requests and in Slack working with every aspect of the company to ensure the new features work as expected and bugs as appropriately squashed. As new releases approach their release date, I start running more queries to ensure everything will be done on time. Mini-rant: I hate JIRA's UI. It's slow, clunky and makes rolling things up as I need them unnecessarily complicated. Despite that complaint, JIRA is good because it has so much flexibility on the web UI and, even better, it has an API I can use to automate the queries I use. So, that's what I did. Our last release was larger and more complex than the ones we've done in the last year (since I started). The reason for this complexity was that we needed to coordinate our updates with those of our third party billing platform. Messing up how we bill customers is a great way to get into a \"discussion\" with the higher ups at any company. In this release, I started poking around JIRA's API . With very little work, I'd managed to automatically run the queries that were taking a significant amount of time in the UI. I formatted these nicely and started posting the results in Slack during our update calls so that all of the developers were on the same page. From my point of view, these calls were more efficient. After the release was pushed out, I decided to see what it'd take to make these queries available to everyone via a Slack slash command. This article will talk about the process I went through, give a small tutorial for a basic command, explain how I tested locally, provide a few tips that deal with pitfalls I encountered and explain how I deployed this to Google's cloud platform. Writing the Flask app Slack's slash apps do not run on Slack's platform. When a slash command is issued, it calls a predefined URL and awaits a response. My experience is with Python. I've used both Flask and Django web frameworks. These commands will be small and don't need any of the back end batteries that Django includes, so I chose to use Flask to handle the commands I wanted to create. Slash Pitfall 1: Timeouts The first pitfall that I encountered was before I even started writing code. Slack only allows a slash command 3000 milliseconds to respond, before it times out. Unfortunately, connecting to JIRA and running the series of queries I need takes a minimum of 5 seconds. Fortunately, the workaround for this was simple: Use delayed responses by responding to the initial command with a confirmation message of some kind, then perform the work and respond again using the Slack passed response_url . Sample Application The application code below is a simple toy example. It will respond to the command \"/hello-world\" and then reply again after a few seconds, to simulate the delayed responses I needed. from functools import wraps from threading import Thread import asyncio from time import sleep import requests from flask import abort , Flask , jsonify , request app = Flask ( __name__ ) SLACK_VERIFICATION_TOKEN = \"\" # Put your token here SLACK_TEAM_ID = \"\" # Put your team ID here def validate_request ( f ): \"\"\"Decorator to validate request is from slack\"\"\" @wraps ( f ) def check_request_validity ( * args , ** kwargs ): if not is_request_valid ( request ): abort ( 400 ) return f ( * args , ** kwargs ) return check_request_validity def is_request_valid ( request ): \"\"\"Validate a request is from Slack\"\"\" is_token_valid = request . form [ 'token' ] == SLACK_VERIFICATION_TOKEN is_team_id_valid = request . form [ 'team_id' ] == SLACK_TEAM_ID return is_token_valid and is_team_id_valid def slack_command_response ( response_url , response_text , response_type = 'ephemeral' ): \"\"\"Respond to a Slack command\"\"\" if response_type not in ( 'ephemeral' , 'in_channel' ): response_type = 'ephemeral' data = { 'response_type' : response_type , 'text' : response_text } requests . post ( response_url , json = data ) def start_command_worker ( loop ): \"\"\"Switch to new event loop and run forever\"\"\" asyncio . set_event_loop ( loop ) loop . run_forever () command_loop = asyncio . new_event_loop () command_worker = Thread ( target = start_command_worker , args = ( command_loop ,)) command_worker . start () def hello_world ( response_url ): \"\"\"Sends \"Hello World!\" to Slack after 5 seconds\"\"\" sleep ( 5 ) slack_command_response ( response_url , response_type = 'ephemeral' , response_text = \"Hello World!\" ) @app.route ( '/hello-world' , methods = [ 'POST' ]) @validate_request def command_hello_world ( ** kwargs ): command_loop . call_soon_threadsafe ( hello_world , request . form [ 'response_url' ]) return jsonify ( response_type = 'ephemeral' , text = \"Waiting to greet you...\" , ) if __name__ == '__main__' : # This is used when running locally. Gunicorn is used to run the # application on Google App Engine. See entrypoint in app.yaml. app . run ( host = '127.0.0.1' , port = 5000 , debug = True ) Sample Application Walkthrough The only interesting thing in the imports here is the inclusion of asyncio . Since I need to fire off an immediate response and then do the \"real work\", I'll funnel that work into worker threads. I'm also including functools.wraps because I'm making a decorator for validating a request is coming from Slack. For a single command, this type of decorator isn't needed, but I have multiple Slack slash commands in the real application. I figured it'd be helpful to show here too. This application will also need the requests library. Speaking of that decorator, the first function encountered in the code is validate_request . This will be the decorator that ensures a request came from Slack. It calls is_request_valid , which compares the passed token and team_id to the values we've previously saved. If they match, the request is valid. If they don't match, the request is invalid. This application is only for my team and won't be distributed elsewhere. Next up is slack_command_response , which is used to send text back to Slack. It will respond to the response_url parameter. This is passed by Slack and is part of the request.form object Flask receives. This can be found at request.form['response_url'] . It will reply either ephemeral (default) or in_channel . The first will reply only to the user and will hide the slash command that was used. The second will reply to the entire channel and will leave the slash command visible to all. Starting the worker thread is done in start_command_worker and the next three lines. This will fire up a thread that listens forever. It will not take place on the main thread, which allows Flask to respond immediately and then perform work in the background. Remember, this is a small application and will work for the scale me and my team will be using this on. This is most certainly not designed for a huge number of users constantly using it. Now it's time to get to the real work. hello_world and command_hello_world . If you've used Flask before, you can see that command_hello_world will be the function associated with a user hitting http:\\\\server.tld\\hello-world with a POST request. Slack only sends POST requests, so I care about GET methods. In command_hello_world , we send a call to the command worker thread, telling it to call hello_world and then pass the response_url as a parameter. The function immediately returns a response to Slack telling the user to wait. In hello_world , the function sleeps for a few seconds before sending a response back to the passed response_url . This sleep is to emulate \"real work\" being done. In my case, it's five seconds of queries to JIRA to gather and format all of the data I want to return. Finally, this can run locally by firing up Flask. I tested with this command: FLASK_APP=jira-slack-integration.py flask run When deploying to Google App Engine, the main function won't be utilized. I cover that below. Testing the application Now it's time for everyone's favorite part of development: TESTING! Slack set up - Part 1 To test a Slack application, though, some set up within Slack is needed: create a Slack Application, set up and gather tokens, and set up slash command end points. First, create a new Slack App . Fill out the name and select the appropriate workspace. After submission, it redirects to a basic information section about the new application. Scroll down to \"App Credentials\". Copy the Verification Token and put it in the SLACK_VERIFICATION_TOKEN variable in the Flask application. Open Slack in the browser, sign in, and then open the web console. In Chrome, do this with CTRL + SHIFT + I or with F12 in FireFox. View the page source and search for team_id . It will look something like this: \"T083XXXX\" . Copy this value to SLACK_TEAM_ID in the Flask application. ngrok set up Before slash commands can be set up in Slack, you need a development environment and an easy way to access our development server. One option is to punch holes in the router's firewall to point to your development machine. This works if you are on a home network and you'll be the only machine running the development server. It's no so easy if your set up is more complicated or infrastructure is outside of your control. I choose to use ngrok instead. This application provides you with a free, secure and public URL to your local development environment without worrying about your NAT or firewall settings. Sign up. After that the four steps to complete setup are shown Download ngrok. There are downloads for a variety of operating systems. This includes Ubuntu, which I use for my work related development work. Unzip ngrok to any location: unzip /path/to/ngrok.zip This places an ngrok binary in the selected location. Set up the authentication token. This is a one time step. This will create a ~/.ngrok2/ngrok.yml file. Start ngrok . If you're using the script from above, Flask should run on the local machine on port 5000. The command to start ngrok to point to the Flask server is: ./ngrok http 5000 . In another command prompt start the Flask application. Slack set up - Part 2 ngrok provides a public URL. In the screenshot below, my URL is https://1eed8eae.ngrok.io . Important Note: This changes every time ngrok is stated. At this point, I can visit https://1eed8eae.ngrok.io/hello-world in my browser and get an error message because I didn't configure it to support GET requests. Go back to Slack and the management area where the new application was set up. Select \"Slash Commands\" Select \"Create New Command\" Put in the command users will use within Slack. This can be anything. Enter the request URL. This will be https://1eed8eae.ngrok.io/hello-world with this example Provide a description of the command Add a usage hint. This is useful if you are passing parameters to the command. Press save The slash command is now set up. The last step is installing the application. Go back to \"Basic Information\" and expand \"Install your app to your workspace\" then press the green \"Install App to Workspace\" button. You'll be presented with an oAuth Access Token. For this example application, it's not needed. Now go into any channel in Slack and use the new /hello-world command. Important Note: If/when you shut down and restart ngrok , you'll get a new end point. The slash command will need to be modified to point to this new request URL to continue to function. These changes will not be required once the application is deployed to Google's App Engine. Deploy application to App Engine This project requires the use of the flexible app engine environment (vs. standard environment). The biggest reason for this is due to the network requirements. It seems that anything other than Node.js has networking restrictions, and the sample application needs to connect to Slack and my application also needed to connect to JIRA. Another downside of the standard environment is that it only supports Python 2.7. I don't believe there is anything in the example application that would break on Python 2, but there are a few Python 3 specific things I used in my real application (f strings, are one). The flexible environment isn't free though. It's always on. The sample application and my real application are so small and used by so few people that it costs less than fifty cents a day. This isn't a huge deal when the rest of our Google cloud bill exceeds that by a couple orders of magnitude, but it is something to consider if you are just running this as a small side thing. It's not free. Set up gcloud SDK Due to the size of this application, the quick start tutorial that Google provides is perfect. Using the Google Cloud Platform console, create a new App Engine project and enable billing (billing must be enabled). This can be done from here Download the Google Cloud SDK Extract this to any location. To add it to the path, run ./google-cloud-sdk/install.sh . If this isn't done, the full path needs to be in all gcloud commands. Initialize the SDK by running gcloud init and follow the prompts on screen. You'll need access to a browser for this step as you'll be authorizing your account using oAuth. Set up app.yaml With gcloud set up on your development machine, there is one last step to do: Configuring the app.yaml file. This file contains information on the type of environment you'll be deploying to. Create and save an app.yaml file in the same directory as the Flask application. For this example, the Flask application is in a file saved as example-script.py runtime : python env : flex entrypoint : gunicorn - b : $PORT example - script : app runtime_config : python_version : 3 manual_scaling : instances : 1 resources : cpu : 1 memory_gb : 0.5 disk_size_gb : 10 Other than the entrypoint line, this is the example app.yaml provided by Google. example-script is the name of the file that contains the Flask application. Deploy to Google Finally, it's time to deploy this application to Google. From within the same directory where example-script.py resides, run: gcloud app deploy Wait a few minutes for the deployment to occur. When it's complete, the command prompt will say so and provide a URL where the application is accessible. The last thing that needs to be done, is repointing the slash commands to this new location. With it deployed to Google's Cloud Platform, the ngrok provided URLs need to be changed. The endpoints remain the same though. Once the slash commands are changed and saved, test them out and enjoy the new slash commands hosted on Google's App Engine."},{"tags":"Technical Solutions","url":"setup-lets-encrypt.html","title":"Set up Dynamic CloudFlare IP with Let's Encrypt","text":"Introduction In the two previous articles, I installed NextCloud and GitLab . These are running on the server, inside my local network, with no firewall rules set up to allow it to be accessible from the internet. That's great if I plan on sitting at home all the time and never accessing anything from the outside. However, I do plan on that. That means I need to make this server accessible from the internet. On top of that, I want to secure the connection to the server with SSL, so that I'm not uploading pictures or code in a way that everyone can read. Setting up CloudFlare This new server sits in my house, which sits on a residential ISP network. Obviously, this isn't going to have 24x7 uptime, but that's fine with me. One thing that I will need, is a way to access this server regardless of the IP address my ISP has given me. This can (and does) change frequently enough that it'd be annoying to keep track of my current IP manually. My solution: set up a DNS entry. In the two previous articles, I set up the Apache virtual hosts with subdomains: ServerName nas.example.com and ServerName gitlab.example.com It's time to utilize those. Then I will only need to visit those URLs and Apache will handle routing to the correct application. I use CloudFlare to handle DNS for this blog. I described the process to set up CloudFlare a few years ago and never looked at it again. \"It just works.\" Hooray! For this, we're going to add two new A entries to reflect the subdomains I want to use. I'll point it at my IP address initially too. Automating the IP adddress updates The initial set up of the A entry/IP address takes a minute. The trick is automating that process every time your IP address changes. I am doing that with a small Python script called cloudflare-ddns . Clone this to the server. git clone https://github.com/ethaligan/cloudflare-ddns.git Next, we need to set up zone information. This is the configuration file that will be used to update your A records. Copy example.com.yml to the name of your domain. For example: cd zones cp example.com.yml andrewwegner.com.yml Now we need to edit the newly copied file to contain appropriate zone information, CloudFlare API information and your domain. %YAML 1.1 # Your Cloudflare email address cf_email : 'your_cloudflare_email_address' # Your Cloudflare API key # https : // support . cloudflare . com / hc / en - us / articles / 200167836 - Where - do - I - find - my - Cloudflare - API - key cf_api_key : YOUR_CLOUDFLARE_API # Cloudflare zone name # If you ' re updating 'ddns.example.com' set this to 'example.com' cf_zone : example . com # List of records # If you ' re updating 'example.com' record , set its name to '@' . # Only write the subdomain ( 'ddns' for 'ddns.example.com' ) cf_records : - 'nas' : type : A log : ERROR - 'gitlab' : type : A log : ERROR # This is the method used to discover the server ' s IP address # The faster one is 'dig' but it may not be available on your system # Available methods : 'http' or 'dig' cf_resolving_method : 'dig' In this case, I am updating two subdomains ( nas and gitlab ) that are part of the example.com domain. Those should be changed to reflect your set up. Last, we need to schedule this to run on a regular basis so that CloudFlare always points to the correct IP address. I did this with a crontab entry: */30 * * * * python3 /path/to/cloudflare-ddns.py -z example.com Again, change example.com to your domain, and it will use the appropriate YML file. With this entry, my DNS entries are updated every 30 minutes. That is frequently enough for my needs. Let's Encrypt (SSL) With the subdomains set up and working, it's time to install some SSL certificates. In previous articles, I had entries in my Apache virtual hosts that pointed to SSL certificates. This is where we'll set those up. Let's Encrypt certificates are valid for 90 days. Renewing certificates, though, can be easily automated. Since I need my certificates to work through CloudFlare, because it provides my DNS services, I use a hook in Let's Encrypt's ACME client dehydrated to handle everything. cd ~ git clone https://github.com/lukas2511/dehydrated cd dehydrated mkdir hooks git clone https://github.com/kappataumu/letsencrypt-cloudflare-hook hooks/cloudflare pip install -r hooks/cloudflare/requirements.txt This downloads deydrated and then downloads the CloudFlare hook that is needed. It installs the required libraries too. The last bit of configuration that is needed is setting up a config file in the dehydrated directory. nano `dehydrated/config` Add the following three lines export CF_EMAIL=YOUR_CLOUDFLARE_EMAILADDRESS export CF_KEY=YOUR_CLOUDFLARE_API export CF_DEBUG=true Substitute your CloudFlare login email and API key as appropriate. The CF_DEBUG line can be set to false if you don't wish debugging information to be printed to logs/ . Register with Let's Encrypt and accept their terms of service: ./dehydrated --register --accept-terms Finally, you're ready to generate/install the SSL certificates needed. One note: I needed to adjust the shebang line in hooks/cloudflare/hook.py to be python3 . Run the following commands to generate the certificates. These will end up in dehydrated/certs with the full URL of each certificate. ./dehydrated -c -d nas.example.com -t dns-01 -k 'hooks/cloudflare/hook.py' ./dehydrated -c -d gitlab.example.com -t dns-01 -k 'hooks/cloudflare/hook.py' The path to these files are what will go in your Apache Virtual Host files: SSLCertificateFile /path/to/dehydrated/certs/nas.example.com/cert.pem SSLCertificateKeyFile /path/to/dehydrated/certs/nas.example.com/privkey.pem SSLCertificateChainFile /path/to/dehydrated/certs/nas.example.com/chain.pem I set up a crontab entry for each of my subdomains to attempt to renew the certificate once a week. Dehydrated will not attempt to renew a certificate if it's not going to expire in less than 30 days, so we aren't making unneeded calls to Let's Encrypt. 0 1 6 * * /path/to/dehydrated/dehydrated -c -d nas.example.com -t dns-01 -k '/path/to/dehydrated/hooks/cloudflare/hook.py' 10 1 6 * * /path/to/dehydrated/dehydrated -c -d gitlab.example.com -t dns-01 -k '/path/to/dehydrated/hooks/cloudflare/hook.py' Conclusion With this final step, I have a home server that I can access from anywhere. It allows me to backup pictures automatically, holds my private repositories and is protected by SSL. The SSL certificates renew automatically."},{"tags":"Technical Solutions","url":"installing-gitlab.html","title":"Setting up GitLab on the new server","text":"Introduction Back when I ran Vipers, my fellow admins and I hosted a small set of code repositories - SVN, Mercurial and Git - to host some of our custom code. We ran RhodeCode and the fork, Kallithea , when RhodeCode close sourced some of it's code and couldn't figure out if the license it used actually allowed themselves to do that. A private repository was awesome for plugins, server configurations and personal projects. When the community was shuttered, some of the plugin code was migrated to GitHub and it's sat there untouched since. My personal projects were either migrated to GitHub or simply stored outside of version control if it couldn't go in a public repository. That was less than ideal, but it worked. With the new home server set up, I wanted to get source control set back up for my non-public personal projects. I rejected RhodeCode right away due to the experiences I had when they changed licenses. Turns out, they had done it again in the meantime. I didn't want to deal with that. I attempted to install Kallithea using their instructions , but I kept running into Python syntax errors. It wasn't worth the time and effort to figure out the problem. So, I turned to GitLab . It'd definitely overkill for what I really need, but it works and if I ever truly decide to get fancy, I have a lot of other tools I can use. The core functionality is what I'll be using and is free. The three other versions cost some money and contain features that would be useful for large team, not a single developer or very small team. Installation Dependencies Installing GitLab is pretty simple. There are a couple dependencies needed, but I already had both OpenSSH and Postfix installed, so I was able to skip the first step in the official installation guide . I installed the Ubuntu Omnibus package. sudo apt-get install -y curl openssh-server ca-certificates postfix Getting the package The GitLab repository needs to added and then installed. To add the repository, issue this command: curl https://packages.gitlab.com/install/repositories/gitlab/gitlab-ee/script.deb.sh | sudo bash To install the GitLab package, you need to provide an environment variable when you issue your apt-get install command. This will be the URL where you want to access your GitLab installation. sudo EXTERNAL_URL=\"http://gitlab.example.com\" apt-get install gitlab-ee Complete the installation Once the install, above is complete, you need to log in to complete the process. In your browser, navigate to the URL you provided above. Set/reset the password as prompted and then login. Post-install Tweaks Using Apache instead of Nginx The omnibus package comes with Nginx bundled. Unfortunately, I don't have any experience managing an Nginx instance but do have experience with Apache. I want to use something that I know to make my life easier. Fortunately, GitLab can handle this with a few minor changes to the configuration . In the /etc/gitlab/gitlab.rb file you'll need to make several settings changes. You also need Apache already installed and the www-data user (on Ubuntu) added to the gitlab-www group. Find nginx['enable'] and set it to false In web_server['external_users'], add www-data` to the array. Note that this is an array and not a single string. In `gitlab_rails['trusted_proxies'], add the IP address of the Apache web server. Change the gitlab workhorse settings to the following (default) values. These may already be in the configuration file. If so, you probably don't need to modify them. gitlab_workhorse['listen_network'] = \"tcp\" gitlab_workhorse['listen_addr'] = \"127.0.0.1:8181\" Finally, run sudo gitlab-ctl reconfigure for the settings to take effect. Now, you need to configure Apache's virtual host. GitLab provides example virtual hosts . Since I installed the omnibus package and am using Apache 2.4, I selected the gitlab-omnibus-apache24.conf file. Adjust all instances of YOUR_SERVER_FQDN to the fully qualified domain name of your server. This will go in /etc/apache2/sites-available/ and a symlink in /etc/apache2/sites-enabled/ will point to this file. sudo touch /etc/apache2/sites-available/gitlab.conf sudo ln -s /etc/apache2/sites-available/gitlab.conf /etc/apache2/sites-enabled/gitlab.conf Use SSL to access GitLab The example virtual host provided by GitLab uses HTTP only. I want to set up my instance to use HTTPS. I'll be doing this with Let's Encrypt , like I did when I set up NextCloud in the previous post. I cover the exact steps for Let's Encrypt in another post. The keys referenced in the virtual host configuration file below created by that process. The first change to make is to redirect the HTTP version of your domain to HTTPS. The goal is that all traffic to GitLab will go over SSL. Adjust the ServerName variable as appropriate. <VirtualHost *:80 > ServerName gitlab.example.com ServerSignature Off RewriteEngine on RewriteCond %{HTTPS} !=on RewriteRule .* https://%{SERVER_NAME}%{REQUEST_URI} [NE,R,L] </VirtualHost> Then, everything in the sample virtual host file can be put in the <VirtualHost *:443> block. At the top of this block, we need to reference the Let's Encrypt keys: SSLProtocol all -SSLv2 SSLHonorCipherOrder on SSLCipherSuite \"ECDH+AESGCM:DH+AESGCM:ECDH+AES256:DH+AES256:ECDH+AES128:DH+AES:ECDH+3DES:DH+3DES:RSA+AESGCM:RSA+AES:RSA+3DES:!aNULL:!MD5:!DSS\" Header add Strict-Transport-Security : \"max-age=15768000;includeSubdomains\" SSLCompression Off SSLCertificateFile / path / to / dehydrated / certs / gitlab . example . com / cert . pem SSLCertificateKeyFile / path / to / dehydrated / certs / gitlab . example . com / privkey . pem SSLCertificateChainFile / path / to / dehydrated / certs / gitlab . example . com / chain . pem Save and restart Apache. You should be automatically redirected over HTTPS when you visit your GitLab URL. Allow spaces in repository names One of the only problems I ran into with GitLab is that, by default, repositories with spaces in them can't be viewed in the web browser. It throws a 400 Bad Request when trying to view the directory. There is a bug report regarding this problem. The developers are working on updating the samples in a way that is guaranteed to work through the whole system. For me, though, the first comment which suggests a minor RewireRule change works great. In the virtual host, fine the line RewriteRule .* http://127.0.0.1:8181%{REQUEST_URI} [P,QSA,NE] and remove the NE so that it reads RewriteRule .* http://127.0.0.1:8181%{REQUEST_URI} [P,QSA] Restart Apache and you can navigate to the directory with a space. Setting up SMTP GitLab can send out emails and requires the ability to do so when resetting a password, at minimum. I don't want this email to be marked as spam, so I used one of the free providers from here and set up an account. After editing the /etc/gitlab/gitlab.rb file to match the provider I selected, I ran gitlab-ctl reconfigure . Now any emails GitLab sends out goes through the trusted email provider instead of coming directly from my residential IP address. This means my mail provider trusts it. I also send out less than 5 emails a month currently, so I am well below the tier where I lose my \"free\" status. Conclusion At this point, GitLab is set up over SSL on my server. I can log in and start setting up repositories. Migrating and importing the code bases I didn't want to put on a public GitHub account was very satisfying. Maybe I'll look into some of the more advanced features GitLab offers in the near future, but for the time being I'm happy with what I have and the knowledge that I can expand what I do with GitLab."},{"tags":"Technical Solutions","url":"travisci-insecure-environment-variables.html","title":"Travis CI doesn't keep your environment variable secure","text":"Introduction On December 27, 2017 I reported a security issue directly to the security team as their CONTRIBUTING.md recommends. I received an automated response that a human would follow up with me soon. It was their end of year, two week vacation (which is awesome!). I sent the same email again on January 26, 2018 and received a response back from AJ Bowen, a Build Infrastructure Engineer at Travis CI on January 29, 2018. They'd created an internal issue to track the behavior and would follow up within two weeks. I followed up with AJ on February 28, 2018 and didn't receive a response. We're now over three months since my initial report. I believe it's time to make this more public so that others know to be careful with their Travis CI managed environment variables. The Issue Travis CI is an application that allows you to automatically test and deploy applications after a commit is pushed to GitHub. I've used this ability to run unit tests, automatically deploy updates to PyPI , and more recently when testing deployment to AWS using the Serverless framework. It's that last one that led me to this issue. Part of deploying to AWS requires that you have credentials to deploy. I didn't want to put my AWS deploy credentials in GitHub, even if they are encrypted . Instead, I decided to set my variables in the Travis CI Settings . I went forward with my testing, watched the deploys happen as expected and eventually needed to transfer my repository to a third party. I used GitHub to transfer the repository to the new owner. We tested a build and watched it deploy. The Travis CI console showed a successful deploy. The problem is, it deployed to my AWS account using my AWS credentials. These \"secure\" environment variables had been transfered to a third party and were no longer in my control. Reproduction - Short Version Reproducing the issue is trivial. The short version is this: On one GitHub account, create a repository with a .travis.yml file On the Travis CI account associated with step 1, set up an environment variable and elect not to show the value in the build log Transfer the GitHub repository to another account. At this point, the environment variables defined in step 2 are accessible by the new owner from step 3. Reproduction - Long Version The detailed steps taken to reproduce this issue show that Travis CI is simply looking for the environment variable values and scrubbing those from the build logs. Once transfered, an edit can be introduced to show these variables with minimal work. Create a repository Create a new repository and add something. For this test, I created a simple Python Hello World file, and named it hello.py . print(\"Hello World\") Commit this change to your new repository. Enable Travis CI integration Go to Travis CI and log in with the GitHub account associated with the above step. Sync your account. Then enable integration by changing the repository switch. Create a .travis.yml file With the integration now in place, set up a basic build script by adding a .travis.yml file to the repository. language : python python : - \"3.5\" script : - python hello . py This script will set up a build task and run your hello.py file, using Python 3.5. You will see that \"Hello World!\" is printed in the build console. Add Environment Variables Now that our build script is working, we can work on \"deployment\". Deployment to AWS (or other cloud services) requires that you provide credentials. I am not a fan of including credentials in my repository, even if they are encrypted. Opting for an environment variable should be more secure, as the credentials are never in your repository in the first place. It is important to note that you are still giving your credentials to Travis CI in this case. To set up environment variables, click on \"More Options\" and \"Settings\" within the Travis CI application. Now scroll down to \"Environment Variables\". Add the name of the variable and the value. Be sure to leave the default value of \"Off\" selected. You don't want to display this value in the build log. Finally click \"Add\". I've added a second variable for further testing. Notice that variable values are hidden from view after clicking \"Add\". Check values during build With the variables saved, restart your build. When the build has completed, check the build log. Even though we aren't using these values yet, we can see the environment variables exist and are \"Secure\". Accessing the values These values are not actually secure. Travis CI is filtering for the values of these environment variables and if the specific string is found, it is scrubbed from the log. We can see this with a small change to hello.py . import os print ( \"Hello World!\" ) aws_id = os . environ [ 'AWS_ACCESS_KEY_ID' ] aws_secret = os . environ [ 'AWS_SECRET_ACCESS_KEY' ] print ( \"AWS KEY ID: |{}{}|\" . format ( aws_id [: len ( aws_id ) // 2 ], aws_id [ len ( aws_id ) // 2 :])) print ( \"AWS SECRET KEY: |{} {}|\" . format ( aws_secret [: len ( aws_secret ) // 2 ], aws_secret [ len ( aws_secret ) // 2 :])) In this example, we are splitting the values of the environment variables in half. For the AWS_ACCESS_KEY_ID value, you smash these two together. This will match the environment variable value, and will not be shown because the pattern still matches the value: |{}{}| For AWS_SECRET_ACCESS_KEY , we split the two halves with a space and print it out. This will be shown, because the extra space no longer matches the exact value of the environment variable. |{} {}| Commit and push the change to GitHub. In Travis CI, we see the following in the build log: As expected, the first pattern is hidden because it matches the environment variable. The second pattern is shown, because the space in the middle means the pattern no longer matches. Transfer the repository Now that we've shown the variables are accessible, it's time to transfer the repository to a new owner. In GitHub, this can be accomplished by going to the repository settings and going down to the red \"danger area\". Once you've entered the name of the current repository and the name of the new owner, we wait for the new owner to accept the transfer. Build with the new owner Make a change and commit it to the new repository. I simply modified the \"Hello World\" line: print(\"Hello World from new owner!\") A new build will kick off. You can see that the repository has transfered to the new owner in the build log. You can also see the environment variables were transfered to the new owner. Other than the new owner being listed, the build log shows the same output as before Variables in the UI You can also see these variables have transfered by going back to \"More Options\", then \"Settings\" in Travis CI. The values are still hidden behind the input password field. Impact of bug The example above shows two problems. The bigger problem, in my opinion, is that environment variables are transfered to a new owner. The secondary problem is that \"secure\" variables are really just obfuscated. Accessing them is trivial. With this demonstration, we added in a step to show that the variables can be seen by the original owner. However, it is just as likely that the new owner could introduce such a change after the repository is transfered. This bug requires the owner of the repository to perform the \"dangerous\" GitHub action of transferring a repository. That means it's impact is limited. However, it's just as likely that the original owner has forgotten that environment variables were set up in Travis CI, an entirely separate system. When a GitHub repository is transfered to a new owner, the environment variables in Travis CI should not travel with. This is especially true for the \"secure\" variables. I'd rather that a build breaks after the transfer due to the lack of appropriate variables being set up than having my cloud credentials be sent to a third party. Mitigation Mitigation of this bug, until Travis CI stops transferring environment variables to new repository owners, requires the original owner to remove the variables prior to transferring the repository. One of the steps that the owner should take is to log into Travis CI and ensure all secure variables have been removed from the Travis CI environment. This will break the builds, but it will also ensure that private variables aren't leaked unintentionally to a third party. Repository The repository for testing is available on GitHub . This issue has been reported in the following places: Travis CI Github Issues Hacker News Charcoal HQ"},{"tags":"Technical Solutions","url":"installing-nextcloud.html","title":"Installing NextCloud","text":"Introduction In the last post, I described how I set up ZFS on the new server . With a newly configured operating system and tons of space, it's time to start using it. One of the goals I mentioned when I set up this server was the ability to: Back up data from all devices in the house automatically. As camera phones have gotten better, we've found that we carry our bulky digital camera less and less. The problem with the phone camera is that we need to get the pictures to the computer. I don't want to hunt down a data cable or email the pictures to myself. I'm also not a fan of posting everything to social media. I want my phone to send the pictures to a backup location automatically. I'm going to accomplish that by hosting an instance of NextCloud on this new server. Fortunately, the install process is pretty simple for this one. NextCloud provides installation instructions . When I installed it in mid-February 2018, it was on version 12.x. As of this post, in late March 2018, it's on version 13.x. I'll cover install and upgrade processes in this post. Installation Prerequisites For NextCloud you'll need either MySQL or MariaDB. I host it via Apache2, so we'll have that installed too. NextCloud is written in PHP, meaning we need that too. sudo apt-get install apache2 mariadb-server php7.0 libapache2-mod-php7.0 php7.0-mbstring php7.0-curl php7.0-zip php7.0-gd php7.0-mysql php7.0-mcrypt php7.0-bcmath php7.0-xml php7.0-json php7.0-tidy Enable the Apache2 rewrite module and restart the web server. sudo a2enmod rewrite sudo service apache2 restart Set up the database You'll need to create a database for NextCloud. Log into your database using credentials that can create new users and databases. root will work. mysql -uroot -p Next, execute a couple SQL statements to create a database and create a user that can access the database. Make sure you use a secure password. CREATE DATABASE nextcloud; GRANT ALL PRIVILEGES ON nextcloud.* TO 'nextclouduser'@'localhost' IDENTIFIED BY 'YOURSECUREPASSWORDHERE'; FLUSH PRIVILEGES; \\q Download NextCloud As I mentioned above, I initially installed version 12 of NextCloud. The latest version can be found on the NextCloud install page . The URL from that page should be used instead of the version 12 link in the following code block. The code block below will be putting NextCloud in the default location Ubuntu has Apache look. You can modify that as needed. If you do so, the virtual host will need to be modified slightly. sudo cd /tmp && wget wget https://download.nextcloud.com/server/releases/nextcloud-12.0.2.zip sudo unzip nextcloud-12.0.2.zip sudo mv nextcloud/ /var/www/html We need to adjust ownership of the files so that Apache can read them. The default user and group, in this case is www-data . If you have configured your server to use a different user or group, adjust this command accordingly. sudo chown www-data:www-data -R /var/www/html/nextcloud Create the Virtual Host I'll be exposing this to the internet and I'll be accessing it via the internet. That means I really don't want to send data unencrypted to or from NextCloud. I'll be setting up the standard port 80 web server traffic to redirect to the secure port of 443. I cover generating SSL certificates in another post. I use Let's Encrypt . The keys referenced in the virtual host configuration file below created by that process. Create a new virtual host. sudo touch /etc/apache2/sites-available/nextcloud.conf sudo ln -s /etc/apache2/sites-available/nextcloud.conf /etc/apache2/sites-enabled/nextcloud.conf Now you need to edit this newly created file sudo nano /etc/apache2/sites-available/nextcloud.conf Paste the following: <VirtualHost *:80 > ServerAdmin YOUR@EMAILADDRESS DocumentRoot /var/www/html/nextcloud/ ServerName nas.example.com Redirect permanent / https://nas.example.com/ <Directory /var/www/html/nextcloud /> Options FollowSymLinks AllowOverride All Order allow,deny allow from all </Directory> ErrorLog /var/log/apache2/nas.example.com-error_log CustomLog /var/log/apache2/nas.example.com-access_log common </VirtualHost> <VirtualHost *:443 > ServerName nas.example.com DocumentRoot /var/www/html/nextcloud/ RewriteCond %{THE_REQUEST} &#94;.*/index\\.php RewriteRule &#94;(.*)index.php$ /$1 [R=301,L] SSLEngine on SSLCertificateFile /path/to/dehydrated/certs/nas.example.com/cert.pem SSLCertificateKeyFile /path/to/dehydrated/certs/nas.example.com/privkey.pem SSLCertificateChainFile /path/to/dehydrated/certs/nas.example.com/chain.pem <IfModule mod_headers.c > Header always set Strict-Transport-Security \"max-age=15552000; includeSubDomains\" </IfModule> <Directory /var/www/html/nextcloud /> Options FollowSymLinks AllowOverride All Order allow,deny allow from all </Directory> ErrorLog /var/log/apache2/nas.example.com-error_log CustomLog /var/log/apache2/nas.example.com-access_log common </VirtualHost> There are two separate virtual host configurations being created here. The first one, on port 80, is setting up the permanent redirect to the HTTPS site. In the secure virtual host configuration, we're setting a small rewrite rule to provide nicer URLs and configuring the SSL certificates to use. The DocumentRoot variables should match the path you installed NextCloud into in the previous step. Application Configuration There are a few settings that you need to change in the NextCloud configuration. Do this by editing /var/www/html/nextcloud/config/config.php . If this file doesn't exist, you need to copy /var/www/html/nextcloud/config/config.sample.php to /var/www/html/nextcloud/config/config.php . The important settings to check are: - `datadirectory`: In my case, this was pointed at a dataset I created when I [set up my ZFS pool][1] - `overwrite.cli.url`: Changed to point to the HTTPS version of the URL I want to use Complete the installation Restart Apache and the navigate to the domain you've set up for your NextCloud installation. I am assuming that you know how to set up a DNS record for the server name you specified in your virtual host configuration. Once you've reached the domain in your web browser, follow the instructions on screen. You'll need the database username and password you created above. You'll also create an administration user. Upgrading After some time, NextCloud will update. You should apply these updates, as they'll include new features and security patches. Log into NextCloud using your administration user. Click on the Gear icon in the upper right and pick \"Settings\". On the left hand side, select \"Basic settings\". Half way down the page you'll see the version you are currently running and whether or not there is an update available. If there is, you can begin the update from here. NextCloud does not support skipping versions when updating. This means if you are on version 12, you can upgrade to version 13. You can not, however, upgrade directly from 12 to 14. Syncing data NextCloud provides client applications that allow you to automatically sync data to your install. There are clients for both computers and mobile devices. My use case only requires the mobile clients right now, but that may change in the future. From the install page , you can find the clients for Android, iOS and Windows devices. Select the appropriate installer on your device. Once the mobile client is installed, you need to provide the URL to your installation and a username and password that can access your information. I've enabled automatic uploads of new pictures from my devices only when I'm on a wireless connection (no sense wasting mobile data). This, however, is why I wanted the SSL certificates. The client doesn't let me whitelist uploading from specific networks. I'd prefer I don't send my pictures unencrypted. Results I've been using NextCloud for almost three months so far. I love it. Previously, I'd have to find a data cable and remember to manually backup my pictures once and a while. Now, it \"just happens\". If I take a picture at home, it's backed up within seconds. If I take a bunch of pictures while I'm out of the house, my pictures are backed up within minutes of me getting home."},{"tags":"Review","url":"django-course-basic-to-advance.html","title":"Review of Udemy's Django Course from Basics to Advance","text":"Introduction With the new server coming along nicely, I wanted to take a quick refresher on Django. Last summer I went through another Django course . That was a decent course. I didn't need anything that intensive or that focused on the non-Django portions though. I have a couple plans for web control panels that will help me manage the aspects of the new server I care most about. I decided to take a look at Django Course from Basics to Advance by Bucky Roberts. It is billed as a five hour course and seemed to hit major aspects of the framework. It was released in January 2018, so it was brand new at the time I signed up. Spoiler alert: Avoid this course . It's a waste of money. The bad grammar in the title should have been the red flag that made me avoid it. Unfortunately, that was only the start of my issues with the course. Thoughts on the course Literally ten seconds into the first lecture: \"If you guys were wondering what the F Django is, it's not a movie, well it is a movie.\" It only goes down hill from there. A few complaints: A majority of the lectures start with \"Alright Hauses\" \"Think of [Django] like PHP, if you guys are fuzzy on it, but cooler.\" Lecture 3 includes a rant about what an \"app\" is and how everyone is developing \"apps\" now. In lecture 10, the instructor starts with a rant about an overweight airline passenger sitting next to them on a plane Insults and mocking of front end developers One of the lectures ends with \"To be honest, my mom keeps texting me and it's kind of annoying.\" Another lecture is interrupted with this train of thought: \"There is a dog barking outside. It's annoying me.\" Finally, one lecture ends with \"Thank you guys for watching. Don't forget to subscribe!\" It's a YouTube channel. The instructor uploaded their YouTube channel to Udemy. Not only that, they uploaded two year old tutorial videos. It's not even current! There are technical issues with the course too: The instructor is using Django 1.9. There is three years out of date at this point. Installation instructions tell users to use administration or sudo rights for all Python libraries Installation instructions are done using easy_install not pip In the urls.py files that define URL routes, there are lines that look like this: url(r'&#94;admin/', admin.site.urls) The instructor attempts to describe the r in front of '&#94;admin/' by saying: Whenever I type \"r\", it means 'regular expression'. This is just flat wrong. r indicates a raw string . From the documentation: Both string and bytes literals may optionally be prefixed with a letter 'r' or 'R'; such strings are called raw strings and treat backslashes as literal characters. Final Thoughts Don't waste your time with this. Don't look at it. Don't think about it. This course is a YouTube channel. If you really want to watch a two year old series running a three year old version of Django watch it on YouTube . It's not worth it. The information is outdated and wrong in many spots. The instructor uploaded this course for a quick dollar. They didn't build this course for Udemy. They built it for YouTube and you can tell. The quality of content here is not what I've come to expect from Udemy course. Fortunately for me, Udemy Support was very helpful when I requested a refund. I listed most of the complaints I did above and mentioned it course was just a YouTube channel that had been uploaded to Udemy. I was issued a refund in under an hour. I also left my first public review of a course on Udemy so that other students know they can get the same content elsewhere. I still want to find a quick refresher. I'll have to go hunting for that later."},{"tags":"Technical Solutions","url":"zfs-pool-on-ubuntu.html","title":"Setting up a ZFS pool on Ubuntu 16.04","text":"Introduction Previously in this series, the new NAS was assembled. Ubuntu 16.04 has been installed and updated. It's time to do something with all those hard drives! I'll be setting the seven 4TB drives in a single ZFS pool. I'm using ZFS for protection against data corruption. It offers several other features too. I'll be using dual parity, which means I could lose two drives and be able to recover. The goal is never to test this, but I'd rather not go through a data loss scare again. Before we begin, it's a good idea to ensure Ubuntu has been updated. sudo apt-get update && sudo apt-get upgrade With the update complete, let's get started. Installing ZFS Installing the ZFS file system is simple on Ubuntu. sudo apt-get install zfs parted Ta-da! Your system is now capable of setting up ZFS pools. The parted package will be used to set up a ZFS pool shortly. Setting up our pool Pools are the basic building block of ZFS. A pool is made up of the underlying devices that will store the data. Setting up our ZFS pool requires a little bit of prep work for our new drives. First, ensure that the zfs package installed correctly by running: sudo zpool status At this point in the process, you should get the message no pools available . Adding the GPT label I'm setting up this pool with brand new drives. We need to add a GPT label to each disk so that ZFS doesn't complain about disks having an invalid vdev specification when we create the pool. To do this, we'll find the names of our drives first ls -l /dev/sd* On my system, I get a result similar to this: brw-rw---- 1 root disk 8, 0 Feb 13 09:23 /dev/sda brw-rw---- 1 root disk 8, 1 Feb 13 09:23 /dev/sda1 brw-rw---- 1 root disk 8, 2 Feb 13 09:23 /dev/sda2 brw-rw---- 1 root disk 8, 5 Feb 13 09:23 /dev/sda5 brw-rw---- 1 root disk 8, 16 Feb 13 09:23 /dev/sdb brw-rw---- 1 root disk 8, 32 Feb 13 09:23 /dev/sdc brw-rw---- 1 root disk 8, 48 Feb 13 09:23 /dev/sdd brw-rw---- 1 root disk 8, 64 Feb 13 09:23 /dev/sde brw-rw---- 1 root disk 8, 80 Feb 13 09:23 /dev/sdf brw-rw---- 1 root disk 8, 96 Feb 13 09:23 /dev/sdg brw-rw---- 1 root disk 8, 112 Feb 13 09:23 /dev/sdh We'll be adding the GPT labels to each of the unformatted drives. The unformatted ones are the listed drives that don't have a numeral as well. For me, that means we'll be working with sdb , sdc , sdd , sde , sdf , sdg and sdh . The sda drive has been formatted and contains partitions already. Those are sda1 , sda2 and sda5 . For each drive, except sda in my case, we need to run the parted command. sudo parted /dev/sdb This will give you a short dialog. All you will need to do is issue the mklabel GPT command and then quit (using q ) GNU Parted 3.2 Using /dev/sdb Welcome to GNU Parted! Type 'help' to view a list of commands. (parted) mklabel GPT (parted) q Information: You may need to update /etc/fstab. Getting device IDs Once the GPT labels are added, we can create our pool. However, we're not going to use the device paths returned above. Theoretically, those can change (especially if you replace a drive). That would be bad and mess with the entire ZFS pool. Instead we're going to create the pool by using the device id. ls -l /dev/disk/by-id/* This returns output similar to this: ... lrwxrwxrwx 1 root root 9 Feb 13 09:23 /dev/disk/by-id/wwn-0x50014ee20f1d3114 -> ../../sdc lrwxrwxrwx 1 root root 9 Feb 13 09:23 /dev/disk/by-id/wwn-0x50014ee20f3ba2b9 -> ../../sdg lrwxrwxrwx 1 root root 9 Feb 13 09:23 /dev/disk/by-id/wwn-0x50014ee2647227b7 -> ../../sdb lrwxrwxrwx 1 root root 9 Feb 13 09:23 /dev/disk/by-id/wwn-0x50014ee26490a21e -> ../../sdd lrwxrwxrwx 1 root root 9 Feb 13 09:23 /dev/disk/by-id/wwn-0x50014ee2b9c81501 -> ../../sdh lrwxrwxrwx 1 root root 9 Feb 13 09:23 /dev/disk/by-id/wwn-0x50014ee2b9e6ab61 -> ../../sdf lrwxrwxrwx 1 root root 9 Feb 13 09:23 /dev/disk/by-id/wwn-0x50014ee2b9e6b857 -> ../../sde lrwxrwxrwx 1 root root 9 Feb 13 09:23 /dev/disk/by-id/wwn-0x5001b444a9525c87 -> ../../sda lrwxrwxrwx 1 root root 10 Feb 13 09:23 /dev/disk/by-id/wwn-0x5001b444a9525c87-part1 -> ../../sda1 lrwxrwxrwx 1 root root 10 Feb 13 09:23 /dev/disk/by-id/wwn-0x5001b444a9525c87-part2 -> ../../sda2 lrwxrwxrwx 1 root root 10 Feb 13 09:23 /dev/disk/by-id/wwn-0x5001b444a9525c87-part5 -> ../../sda5 ... lrwxrwxrwx 1 root root 9 Feb 13 09:23 /dev/disk/by-id/ata-WDC_WD40EFRX-68N32N0_WD-WCC7K0HLKUXT -> ../../sdb lrwxrwxrwx 1 root root 9 Feb 13 09:23 /dev/disk/by-id/ata-WDC_WD40EFRX-68N32N0_WD-WCC7K0HLKXS0 -> ../../sdc lrwxrwxrwx 1 root root 9 Feb 13 09:23 /dev/disk/by-id/ata-WDC_WD40EFRX-68N32N0_WD-WCC7K4YJ6T0U -> ../../sdg lrwxrwxrwx 1 root root 9 Feb 13 09:23 /dev/disk/by-id/ata-WDC_WD40EFRX-68N32N0_WD-WCC7K5LCEYN4 -> ../../sdf lrwxrwxrwx 1 root root 9 Feb 13 09:23 /dev/disk/by-id/ata-WDC_WD40EFRX-68N32N0_WD-WCC7K6VNN6TA -> ../../sde lrwxrwxrwx 1 root root 9 Feb 13 09:23 /dev/disk/by-id/ata-WDC_WD40EFRX-68N32N0_WD-WCC7K6VNNTXY -> ../../sdd lrwxrwxrwx 1 root root 9 Feb 13 09:23 /dev/disk/by-id/ata-WDC_WD40EFRX-68N32N0_WD-WCC7K7TSA4VS -> ../../sdh lrwxrwxrwx 1 root root 9 Feb 13 09:23 /dev/disk/by-id/ata-WDC_WDS100T1B0A-00H9H0_174256421671 -> ../../sda lrwxrwxrwx 1 root root 10 Feb 13 09:23 /dev/disk/by-id/ata-WDC_WDS100T1B0A-00H9H0_174256421671-part1 -> ../../sda1 lrwxrwxrwx 1 root root 10 Feb 13 09:23 /dev/disk/by-id/ata-WDC_WDS100T1B0A-00H9H0_174256421671-part2 -> ../../sda2 lrwxrwxrwx 1 root root 10 Feb 13 09:23 /dev/disk/by-id/ata-WDC_WDS100T1B0A-00H9H0_174256421671-part5 -> ../../sda5 Notice that both formats symlink to the same location. This means you can pick which ever format you like better. However, I recommend the second one that contains the device serial number. It'll make it easier to determine problem disks in the future. Create the pool Now that we've determined the device ides for each of our hard drives, it's time to actually create the pool. As I mentioned above, we'll be creating using dual parity ( raidz2 ). We'll be naming our pool data . Once this command is complete, /data will be where this pool resides. sudo zpool create data raidz2 /dev/disk/by-id/ata-WDC_WD40EFRX-68N32N0_WD-WCC7K0HLKUXT /dev/disk/by-id/ata-WDC_WD40EFRX-68N32N0_WD-WCC7K0HLKXS0 /dev/disk/by-id/ata-WDC_WD40EFRX-68N32N0_WD-WCC7K6VNNTXY /dev/disk/by-id/ata-WDC_WD40EFRX-68N32N0_WD-WCC7K6VNN6TA /dev/disk/by-id/ata-WDC_WD40EFRX-68N32N0_WD-WCC7K5LCEYN4 /dev/disk/by-id/ata-WDC_WD40EFRX-68N32N0_WD-WCC7K4YJ6T0U /dev/disk/by-id/ata-WDC_WD40EFRX-68N32N0_WD-WCC7K7TSA4VS This will take a little while, but a surprisingly smaller amount of time than I initially expected. Once the creation is complete, take a look at the status of your new pool: sudo zpool status pool: data state: ONLINE scan: none requested config: NAME STATE READ WRITE CKSUM data ONLINE 0 0 0 raidz2-0 ONLINE 0 0 0 ata-WDC_WD40EFRX-68N32N0_WD-WCC7K0HLKUXT ONLINE 0 0 0 ata-WDC_WD40EFRX-68N32N0_WD-WCC7K0HLKXS0 ONLINE 0 0 0 ata-WDC_WD40EFRX-68N32N0_WD-WCC7K6VNNTXY ONLINE 0 0 0 ata-WDC_WD40EFRX-68N32N0_WD-WCC7K6VNN6TA ONLINE 0 0 0 ata-WDC_WD40EFRX-68N32N0_WD-WCC7K5LCEYN4 ONLINE 0 0 0 ata-WDC_WD40EFRX-68N32N0_WD-WCC7K4YJ6T0U ONLINE 0 0 0 ata-WDC_WD40EFRX-68N32N0_WD-WCC7K7TSA4VS ONLINE 0 0 0 errors: No known data errors That last line is important. No known data errors is good. Create datasets Where pools are the basic building blocks of ZFS, datasets is a term for a ZFS file system, volume, snapshot or clone. Each dataset can be managed and configured differently. This means that you can compress one dataset, but leave the others alone. You can put a quota on one, but leave the others without a quota. Creating a dataset is pretty easy: cd / sudo zfs create data/storage This will create a dataset that exists at /data named storage . You can have child datasets that inherit attributes from parents (or even grandparents) by doing something like: sudo zfs create data/storage/music This will create the new dataset at /data/storage named music . Once you've set up your datasets, you can see they were all created and how much space they have available by issuing sudo zfs list . Set up complete With that, we've finished setting up ZFS on Ubuntu 16.04. I set up a few datasets for my purposes. I'm one step closer to getting this running and handling all of the digital data in the house."},{"tags":"Side Activities","url":"new-house-server.html","title":"...and then there was a backup server","text":"Introduction In my last post , I covered the events that lead to my data loss scare. Faulty, untested, backups will bite you every time. The question is just, \"when will it happen?\". By mid-to-late January (three months later), I'd gotten everything back from SERT Data Recovery and was happy that everything was recovered. It was time to finally build that huge NAS. Server Goals A NAS for a home backup solution could be something as simple as a prebuilt device with a couple hard drives. I'm a bit of a geek and have a lot of digital data, not to mention family pictures, years worth of programs I've written, and a digital music and movie collection. I'm a digital pack rat, but a well organized digital pack rat. I also wanted to get more out of this server than \"plug the device into the router\". I ran game servers for Vipers for five years. I did a little bit of server work at Caterpillar. I've toyed with server virtual machines off and on for testing various packages and software over the years. Through all of this, though, I've never had a server in the house that I could use to run some of my scripts on. Those always ran on my desktop because it was always on. This server was going to change that. I had several goals when building this thing: Have more storage space than I needed for several years. I didn't want to rebuild this in 18 months because I was bad at planning. Years ago when I built my computer for college, I stuck two 120 gigabyte hard drives in the machine and thought I'd never fill that. When I came home that first summer, I already had to upgrade hard drives because I was low on space. Run a server version of Linux. I don't want to buy a license for a Microsoft server product and my Microsoft Academic Licenses expired a while ago. During my time with Vipers, I used a Red Hat variant of Linux. At Caterpillar we used Ubuntu. Utilize ZFS for protection against data corruption. This combined with my more recent usage of Ubuntu lead me to decide on using Ubuntu Server for the operating system. At the time of this post, I'll be using the 16.04 LTS version. I'll continue to upgrade to future LTS versions. Back up data from all devices in the house automatically. As camera phones have gotten better, we've found that we carry our bulky digital camera less and less. The problem with the phone camera is that we need to get the pictures to the computer. I don't want to hunt down a data cable or email the pictures to myself. I'm also not a fan of posting everything to social media. I want my phone to send the pictures to a backup location automatically. Host my personal git repositories and personal projects. Be able to stream music and movies to other devices on the network. Hardware Now that I've decided my goals, it was time to pick out hardware. The biggest decision was to determine how much storage space I'd be getting. The idea was that hard drives would be the majority of the cost of this machine. In the end, I went with the following hardware: Rosewell 4U server chasis. It's rack mountable for the future when I can convince myself that a server rack in the basement is a thing I want to spend money on and haul around. Supermicro MBD-X11SSM-F-O Micro AT server motherboard (LGA 1151) Intel Xeon E3-1230 V5 3.4 Ghz processor 2x Supermicro certified MEM-DR416L-SL01-EU21 16 GB DDR4-2133 ECC server memory. Take careful note of that model. I originally ordered MEM-DR416L-SL01-ER21 (notice the single \"R\" to \"U\" character difference). The motherboard did not like the ER21 at all. EVGA 650 power supply (I've been really happy with EVGA power supplies on my last 4 machine builds). 1x Western Digital Blue 1 terabyte SSD (for the operating system and other applications) 7x Western Digital Red 4 terabyte hard drives (for all the data) Enough SATA cables for all 8 drives I'll be running ZFS in RAIDZ2 (dual parity). This means with 7 drives, two will be effectively parity drives. I'll have a total of 20 terabytes, minus formatting, for data. After formatting this comes down to a little over 16 terabytes of usable space. Considering that the rest of the household has a combined 5 terabytes, if I use up every available bit, I'm hoping that 16 will last me a while. That RAM... I went with the Supermicro board based on a recommendation from a friend. Supermicro's site is really good. It has tested compatible hardware lists and, it turns out, a knowledgeable person behind their online store's chat feature. The problem that I ran into when building this machine is that the compatible RAM was really hard to find. I didn't realize that and ordered the mother board in my first batch of components. When I finally went to look for RAM, I failed to notice a single character difference between the EU21 version that I needed and the ER21 version that I ordered first. I assembled the machine, plugged everything in, and turned on the new server. Then it beeped at me. A lot. After some troubleshooting, re-seating the RAM and finally realizing that I ordered the wrong stuff, I exchanged what I ordered with what I needed. The EU21 RAM worked perfectly. What's next The hardware is assembled. Ubuntu 16.04 Server has been installed. The next step is configuring the server to be the backup solution for the entire house and meeting my other goals. I'll have a few more posts in this series on how I accomplished those goals. Stay tuned!"},{"tags":"Side Activities","url":"backup-your-data.html","title":"Well, there goes my data...","text":"Introduction October 2017 started nicely. The weather was still good. Fall hadn't really arrived yet, though I was still raking some leaves. Halloween was approaching and soccer season had just ended so I had time of weekends to do a few more projects. Then in mid-October one of my hard drives died. It was sudden. There were no SMART warnings. There wasn't any weird sounds. In fact, the only indication that there was a problem initially was that one of my running applications couldn't access a log file. I didn't think any of it. I took a few minutes to install a Windows update that had been nagging me for a day or so and rebooted the machine. A reboot on my desktop normally takes twenty-ish seconds. As the reboot stretched into the fifth minute, I realized there was a problem. However, I was initially getting ready to blame that Windows update. A Data Disaster Finally, the reboot finished and I logged into the machine. Immediately, I received alerts that files couldn't be accessed and programs couldn't start. I tried to launch Firefox and was told it couldn't be found. \"Well, this is bad,\" I thought. Turns out that was an understatement. After a short investigation, I found that one of my hard drives wasn't being detected by Windows. On my desktop, I have three hard drives. The first is, a decently sided solid state machine that hosts the operating system. The second is a large solid state drive that holds my game installs. The third is a large spinning hard drive (Seagate), which is where I install software and hold my data. The missing drive was the large one that holds my data. Panic set in. However, I had just backed stuff up to my trusty external hard drive. \"At most I'll lose a week's worth of stuff. And, I still have the pictures on phones and camera SD cards,\" I said, trying to comfort myself. I also new I had at least two brand new and unused drives identical to the one that had just had a problem. Those were the result of cheap hard drives when I build my desktop and the promise that one day I'd build a true backup solution for the house. Yeah, hadn't done that yet. After messing with the missing drive by unplugging it and rebooting (bringing back normal boot times), swapping it to another SATA port (\"hey, maybe it's the motherboard\"), trying to access it in a Linux LiveCD and plugging it into an external drive, I concluded that it was dead. I pulled it from the desktop, resisted smashing it to pieces in frustration and plugged one of it's unused siblings in. Windows booted right up. I had an empty hard drive. I'd be spending a day reinstalling software, but I could handle that. I plugged in my backup external drive and started copying data back into place. I breathed a sign of relief when all the family pictures were restored. I lost a weekend, but everything looked good. I went to bed Sunday night happy and planning on finally building that backup solution. \"That was almost a disaster!\" The Problem Appears On Monday morning, I saw the family off to school and work and then settled in for my day. I went to pull up a personal project that I'd work on for an hour or so before the work day began. Well, it turns out that one of the things I stored on that drive that hosts all my data was my personal projects. It also turns out that I never actually backed up my personal projects. More than decade worth of personal programming projects, horrible attempts at graphic work (seriously...graphics aren't my thing), notes for games that I'd love to design if I ever get the time were gone. I dug through my most immediate backup. I found old drives - because \"you never know when you'll need it\" - and dug through those. Nothing. I have no idea how I missed this rather important directory in all my various manual and semi-automated backup routines over the years. I'd either have to suck it up and lose that history or get serious about data recovery. Data Recovery Attempt Back in college, I was manager of the university's help desk. One of the tasks we performed was data recovery for students. Usually this was on flash drives that had \"suddenly deleted a paper\", but occasionally we'd have to work on a hard drive too. Our recovery efforts were limited to a quick run of a handful of applications that attempted to recovery deleted files. In the world of stressed students, this was all that was usually needed. If you could recover that term paper, thesis paper or dissertation, you were a hero. That's what I tried. I hoped it'd work. Windows still couldn't see the back drive. A Linux LiveCD could see that a drive existed, but couldn't find any data. That was still progress. I ran software for almost two days as it tried to find anything for me to recovery. Nothing. Nothing at all. It looked like some how erased everything from the drive. I was back to my choices - lose the history or find a professional. I went with a professional. I couldn't lose that much of my history. I still those projects for both personal and professional work. SERT Data Recovery I went with SERT Data Recovery . The technician over the phone explained their process. They asked questions that made me confident in their ability to at least diagnose the problem, and most importantly were thousands of dollars cheaper than a competitor I spoke with. They explained their pricing structure (they provide a quote and it won't be a giant range), and pricing for replacement parts and allowed me to either send in an empty drive or pay for one that my data would be returned to me on. They answered my questions. They also didn't scold me for doing the single most damaging thing to a drive. I had done it repeatedly. I had turned the bad drive back on. I had turned it on and let it spin for days as I attempted to recover data. I shipped them the drive. After the initial diagnosis came back - one of the platter heads had stopped working - I paid for the replacement part and waited. Eventually the data was recovered with a 100% recovery rate. I was ecstatic. My data was shipped back to me and I quickly transfered my projects to the new drive. My only complaint in the entire process was that the recovery process took almost seven weeks. I shipped the drive at the end of October and received the data back the week before Christmas. However, all of my data was recovered, so it's a pretty minor complaint. What's next? It's been a little over a month since I've transfered my data to a the new drive in my desktop. I've manually run my back up process two or three times. I'm pretty much where I was at the beginning of October. Crossing my fingers that when I get on the computer in the morning, all my data will be there. That's not acceptable any more. I'm finally building the backup solution. It'll be a huge (for a family) NAS. The goal is to be able to back up from Windows, Ubuntu and a couple Android phones automatically. The next entry will cover the device itself and my choices for certain hardware. I'll follow that up with an article on the software set up to get the entire thing working."},{"tags":"Jobs","url":"how-i-found-an-awesome-remote-only-job.html","title":"How I found an awesome remote only job","text":"Introduction It's been over three months since I left my position at Caterpillar , but leaving that job wasn't as simple as finding a position and changing jobs. Since my initial post that I was changing positions, I've gotten questions from several professional contacts that were also looking to move, but weren't entirely sure of the process I used to find my new job. I hope to answer some of those questions with this post. New Job Criteria One of the advantages I had during this search was that I had a job already. This allowed me to hunt for a job at my own pace. I didn't need a new position immediately. The slower pace also let me set my own criteria for what I wanted in a new job. Full time work from home At Caterpillar, I worked from home one to two days a week. I did this for several years and enjoyed it. I found that I was much more productive. I was able to focus on the work that needed to be done that day because the distractions of working in a cube farm weren't present. I didn't hear the side conversations that I wasn't involved in. I didn't get the \"hey can you help me\" questions that could be solved with a few seconds of trying some new code. I was able to concentrate on the task and not context switch frequently. Additionally, the drive to and from the office was consuming almost two hours a day. In the winter, this was brutal. I was leaving as the sun rose and getting home well after it had set. I wasn't seeing the family. My most important criteria was born from these two. I wanted to be able to work from home, full time. I was not opposed to the occasional yearly get together, but I didn't want to go into an office on a regular basis. Pay and benefits must improve Caterpillar had great benefits. I don't remember ever worrying about health coverage, or prescription drug coverage. Pay was \"industry average\", which always seemed a bit lower than what sites like glassdoor.com said I should make. Part of this was due to the yearly bonus I was eligible for. This was dependent on the business performance and not guaranteed. It also fluctuated a lot. The fact that Caterpillar went through nearly five years of poor market performance didn't help with those bonuses either. A new position would need to meet or exceed the health benefits I had. The pay would need to improve too. Working from home would have the advantage of not paying for gas for the car as frequently. It'd also reduce the maintenance costs of putting so many miles on the car. Something challenging A new position is going to bring new challenges. It's new. You don't know the job. But, I wanted it to be challenging after I made it over that initial period of adjustment. I started looking for a job that would utilize my programming skills, leadership skills from the projects I've led, and maybe some of the community management skills I had from Team Vipers and various StackExchange communities I participate in. The search I had my initial, if broad, criteria defined. The next step was to start the hunt. I utilized several sites to help with the initial search and to help narrow down the list of companies I'd really like to talk with. remote|ok - A job board that updates throughout the day, yet doesn't have thousands of postings. It is specific to the \"IT-ish\" world. This was helpful in finding companies that are open to remote jobs, because I'd see those companies listings repeatedly over the course of my hunt. We Work Remotely - Another \"IT-ish\" job board that updates one or two times a day. It has multiple categories of jobs and usually is pulling in positions from other job boards. I preferred using this over the source sites because this had a much smaller set of jobs and was restricted to my area of interest. Stack Overflow Jobs - I have a love/hate relationship with Stack Overflow Jobs. On the one hand, it had so many UI/UX issues . On the other hand, it makes it really easy to find remote jobs and narrow it down to only jobs I'm interested in. This is where I eventually found the posting for the job I took. If I'm being honest, my search for a job took over eighteen months. I found several jobs that were interesting and that I applied for, but the process was slow. There were several false starts, several job applications were ignored, and several interviews that didn't move forward at either my choice or the company's choice. Keeping it all straight I put out a lot of job applications. This meant that I needed to adjust my resume and cover letter to each company. I needed to keep track of when I applied, who I interacted with, when interviews were scheduled, and when to give up on a job. Enter: Trello Prep: This is a reminder to update resume and cover letter for each position, and a list of the labels I utilize. The colored labels made it easy to identify the current status, and whether this was a local or remote position. After searching for about nine months, I started to consider office positions in a few specific cities. Interesting Positions: All new jobs I was considering would go here. It'd consist of a link to the job posting. This was made really easy by utilizing a bookmarklet provided by Trello that would send any link to this list on this board. Companies to watch: As my hunt continued, I found several companies that seemed interesting or that had expressed interest in my skill set. I set up links to the job pages for each of these companies so that I could see if anything new and interesting was posted. Submitted applications: After updating my resume and cover letter, filling out the application forms, and pressing submit, a job posting would be moved from \"Interesting Positions\" to \"Submitted Applications\". Interviews: When a company liked an application and scheduled an interview, I'd move the card to this step. Additionally, I'd add a comment with the name of the people I'd be talking with - Human Resources representative, technical leads, managers - so that I knew who I'd be talking with. Recheck opening availability: If a card sat in \"Interesting Positions\" for a period of time, I needed to check if the position was still available before finally submitting an application. There were many times that I'd mark a job as interesting and then never apply. This list helped keep the viable list of positions down to what was still available. Cold Opportunities / Rejections: Just as I ignored positions, my applications were ignored many times. Other times, an interview process would just fizzle out. These seemed like the company didn't want to send a rejection notice and hoped that ignoring me would provide a hint. It was unprofessional, and I wouldn't look at those companies again. Other times, official rejection responses would be sent back. I appreciated these, even though they were rejections. Butler for Trello The last list on the board was Butler for Trello . This is the system that automated a lot of the moving of jobs through the process and applying proper labels. When a new job was added to the \"Interesting Positions\" list, a due date was added for one month from now. Triggering off this due date, the butler would move cards to \"Recheck\" if the due date was exceeded. When a card was moved to \"Submitted Applications\" the due date was changed to be three weeks from now. The thought here was that if I haded heard from the company in three weeks, I was probably being ignored and thus rejected. This automation helped keep the board clean and usable. I was always presented with a list of openings I was interested in that were current. Moving through the interview process, the cards would be moved if a period of time was exceeded. I didn't need to remember to move stuff based on when I submitted an application, or when the last interview was. It just happened. It was amazing. Getting the job As I said above, this was a long process. That list of cold opportunities and rejections is pretty long. I talked to a lot of different companies and went through a lot of different interviews. Eventually, though, I found a job advertising a Senior Integration Tester at a new start up. The interview process consisted of 4 interviews - one with the Senior Vice President of Development, and three individual interviews with team leads (my peers). I also needed to submit a project demonstrating how I'd test the API. The interviews felt like conversations. We discussed my project. I had to defend a few decisions, but there were no wrong answers. Everyone was professional, yet personable. After all of this, an offer was presented and after some last minute \"do I really want to make a big change in my life\" doubts were squashed, I accepted. I love the job."},{"tags":"Review","url":"aws-serverless-apis-apps-review.html","title":"Review of Udemy's AWS Serverless APIs & Apps - A Complete Introduction course","text":"Introduction I've been involved with the SmokeDetector spam hunting project for several years. In that time It's grown from a small Python script to a large combination of Python, Ruby and user scripts. The infrastructure hasn't really changed in that time, though, and it's starting to show. SmokeDetector, the Python script, is run by multiple users so that random network problems that kill the instance don't take the entire system off line. Arguably, the project should work on fixing this instability, but the alternative that we came up with works too. At least it did. Now this distributed way of running is starting to cause issues. Keeping each instance in sync with one another is a very difficult problem. We've built a decent solution to this for handling spam patterns, but we've failed to replicate this for other things like permission sets or notifications. Adding this involves integration with GitHub and a series of new commands. It's painful. One of the recent discussions to solve this problem was a centralized database. That brings me to this course. The AWS Serverless APIs & Apps - A Complete Introduction by Maximilian Schwarzmüller on Udemy looked like a good overview for AWS lambda and DynamoDB, two solutions the SmokeDetector project was looking into using. Thoughts on the course This course provides a good overview of Amazon's API Gateway, AWS Lambda and several other Amazon services. There is a little bit of coding involved - using NodeJS - but a large majority is dedicated to working in the various Amazon consoles. Except for one optional lecture, the entire course can be completed using Amazon's free tier. This is great for a course and one of the factors I used to determine if I really wanted to take the course. The instructor is very clear, throughout the course, which aspects cost additional money so that no unexpected charges show up from Amazon. The course covers the API Gateway, AWS permissions, AWS Lambda, body mapping templates, DynamoDB, authentication with Cognito and hosting a serverless SPA. At first glance this seems like a lot to cover in an 8 hour course. The instructor has broken these into small digestable lectures that do a good job of introducing each new service, explaining the theory behind each and then using the service to implement functionality into the course long project. A course long project is a good way to show how each of these Amazon services are useful to a larger project. Instead of focusing on a small example project where it is difficult to expand the course work into the real world, a real world project with various requirements is used. This project uses each of the services I mentioned above in a natural way. Implementing new functionality doesn't feel forced, instead it is something that users would expect from a modern application. I am very happy with this course. Everything was very well presented. The project was useful because I can easily understand how each of the Amazon services function. I also have a good idea how the console works and how to troubleshoot problems because of the work that was done in this course. My only complaint with regard to this course is that the video lectures are interrupted with text based lectures consisting of more details and links to documentation. I appreciate these links and used several of them, but the interruption of the videos to present these broke the nice flow of the lectures. Presenting these links at the end of the section, I think would be less of an interruption. This was a minor thing, though, and doesn't take away from how much I enjoyed the course. Final Thoughts I feel that I have a decent understanding of Amazon's services at this point. I am looking forward to applying this new knowledge and, hopefully, improving the SmokeDetector infrastructure at the same time. I highly recommend this course if you are thinking of migrating anything to Amazon. I feel much more comfortable about moving aspects of a project to this new solution."},{"tags":"Side Activities","url":"collecting-diamonds-on-stack-exchange.html","title":"Collecting Diamonds on Stack Exchange","text":"Introduction It's been over two years since I first ran for moderator on Stack Overflow. I've ran for moderator three times , previously on Stack Overflow. In each election I've done better, coming in fifth in the third election. Well, it's been a little over 8 months since the last one and new moderators are needed again. I decided to run once more with the knowledge that if I lost, I probably wouldn't run again in the next election. Nomination Phase The nomination phase started off as usual, with a handful of users posting their nomination. This time there were 12 candidates, meaning there would be a primary to narrow it down to 10 before the final election. My nomination was the following: I'm Andy . I've answered the questions posted by the community here I encourage you to take a look. Why should you vote for me? I've been a moderator on Community Building for several years. I was appointed to a moderator position on Hardware Recommendations . I know the moderator tools and have worked with the current moderators. I'm active in the review queues (I am the top reviewer in the Low Quality Post reviewers, with over 26,500 reviews in this queue). I also enjoy the other moderation aspects of Stack Exchange. I believe that moderation can be tool assisted. I've helped to flag a sizable percentage of comments on Stack Overflow. I've helped build the community spam detection bot . These types of tools help eliminate the obvious bad stuff so that moderation time can be spent on the less obvious stuff. I have a history of good community moderation, am here consistently, and believe I can help the current team. Nomination Reflections An astute reader may have noticed this is pretty similar to previous nomination posts. There are a couple major differences though. The first thing is that I am the the number one Low Quality Posts reviewer on the site. I am pretty proud of this particular statistic. It shows just how much work I've done during my tenure at Stack Overflow to improve the quality of the site. The unfortunate thing is that I'd probably lose this position as a moderator because they don't sit in the review queues. The other major change was that I had picked up a moderator position on Hardware Recommendations. That happened at the end of June. Hardware Recommendations is about ten times the size of Community Building (a site I've moderated for several years). It's also a couple orders of magnitude smaller than Stack Overflow. Primary Phase The most exciting part of the election season is the primary phase. The community can see the scores of users over time and have built tools to watch those numbers change over time. It turns out that this time, my numbers were really high. There were plenty of good people in the election this time. One interesting thing that I found was that a lot of candidates, like me, were supportive of automation. Several users utilized bots that posted low quality content to various chat rooms. This is a big change from previous elections. It was a welcome change. I think that automated quality content checking can help a lot. Election The election ended on August 1st. ( A busy day for me, apparently ). It was a close election. Most surprisingly, no one won in the first round with everyone else picking up carry over votes to get second. I think that speaks to the quality of the candidates. After 8 rounds in OpaVote , both Cody Gray and I were elected the two newest moderators on Stack Overflow! Post Election The election ended a few weeks ago. I handled more moderator flags in my first hour as a Stack Overflow moderator than I had at both Community Building and Hardware Recommendations combined. What I'm saying...Stack Overflow has a ton of flags that need to be handled. In good news, since the election, we have gotten the moderator queue down from about 1,100 flags to about 75 at any given time. I doubt it will stay that low, but it's still nice to see that I was immediately helpful. Finally, since the election I turned off the comment flagging bot. It had been used for just over 3 years. The community is currently debating whether or not it should run under a moderator account. The thing that I am finding more interesting about this discussion is that the community seems to agree it's helpful, respects the 99+% accuracy, would love for Stack Exchange themselves to run this tool, but doesn't want the bot to run with moderator privileges. There is, however, a very sizable portion of the community that does want this done under my account. We'll see how this plays out, but I'm hoping to be able to use the bot again soon."},{"tags":"Jobs","url":"a-decade-at-caterpillar.html","title":"A reflection on my decade at Caterpillar","text":"Right out of college Ten and a half years ago, I was the manager at the university helpdesk and less than six months from graduation. I hadn't heard back from the company I interned with. I was getting concerned. I went to the university's yearly career fair. I had companies I wanted to talk with, a few that looked interesting and a handful that I knew handed out decent freebees, so I'd stop by those too. After all, what college student is going to turn down a freebee? I don't remember what company I was speaking with. I remember it wasn't going well. The interviewer was making unfunny jokes about the location of my home town and how far it was from their place of operations. I was slowly trying to extract myself from the conversation and really wanted to take my resume back too. Finally, I was free and turned to walk away and almost knocked over a recruiter from the next booth over. I apologized and she beamed at me. She also apologized but said it was because she'd been listening to the conversation I'd just had. I had sounded like someone she was looking to hire. I stepped over to the booth and was introduced to the Logistics arm of Caterpillar. The interview on campus and follow up all day interviews about two weeks later went very well. I was offered a job at a salary I was happy with. It turned out that the company I'd had my internship with would eventually offer me a job and not even come close to the starting salary Caterpillar offered, which made me even more satisfied. I was to start in June following graduation. The first day My degree is not in Logistics or transportation management, yet that would be the team I was joining. I was given my boss's name. I showed up, bright eyed and ready to begin my career at a company known the world over, and was told by the receptionist that my boss no longer worked at this facility. She'd moved to a new area of the company less than two weeks ago. Eventually, I was introduced to my new boss and the team. I was handed off to my team lead and we promptly started going over a gigantic system. I remember being overwhelmed and confused. My college degree didn't use terms like \"bill of lading\" or \"route optimization\". I tried to keep up. Eventually, lunch rolled around. I was handed a set of instructions on how to get myself added to the system and told to follow those after lunch. I was young and eager. I followed the directions. In doing so, I learned how to add myself to the system. The company also learned a lesson that day: their instructions were wrong. I followed exactly what I was given. In doing so, I locked out the system administrator account for the transportation management system. This was a 'mistake' that is still brought up a decade later. On day one, it's a horrifying experience. A bit later, it's a harmless first day mistake that took a few minutes for an experienced technician to resolve. Fortunately, day two went much better. A revolving door of bosses The first four years at Caterpillar, I met many great transportation consultants. They were very smart, had worked with many other companies and had been all over the world. I was a bit jealous of their ability to travel. These consultants taught my about managing transportation, about setting up a system for a new company, and helped me design and build tools to improve Caterpillar's ability to do the same. They also taught me something else about Caterpillar: if you aren't a Caterpillar employee, your opinion matters less. They are paid a ton more than me and my more senior coworkers, but if an idea doesn't come from an employee, it's not given fair consideration by management. The problem with this should be obvious. Those with the most experience setting up and managing systems to get parts and supplies on time to our facilities weren't able to provide suggestions for how to improve the system even more. System growth stagnated and my first boss was replaced. The second boss came from overseas and had experience we were trying to move to the US. Their assignment in the US lasted a year or so, and I was given a new boss again. A fourth boss was assigned about nine months before I eventually moved to another division. Four bosses in four years. Through all of this, though, I built and managed some amazing applications. These included a brand new order processing system, a container management system, and a dashboard to unify all of the different systems into one quick at a glance system. During one of the boss transitions, I was moved to sit with my users and learn their pain points. This was one of the best decisions that came out of these first four years. By sitting with the users and talking with them daily, I learned exactly what they had problems with. I designed simple work arounds, automated external tools and ways to improve their pain points. My yearly review reflected how much the customers appreciated my help and me sitting with them. Eventually, I was moved back with the rest of the IT team. At that point, I felt a bit like an outsider. I'd been sitting with my users for over a year. None of my coworkers had been given the opportunity to do so. I was moving from the trenches back to the ivory tower. I lost the interaction with my users and I think my team suffered because of this. We were slower to respond to issues, because now the users had to go through the official channels of calling the helpdesk and waiting until a member of my team was assigned the problem to work with. Simple questions couldn't be answered with a two minute conversation. During this move, a third party logistics service was brought in to 'improve' our processes. It turns out that a start up company without a large customer doesn't know how to manage transportation for a company that spends hundreds of millions of dollars a year to move stuff around. The relationship didn't improve over time. Instead, each side attempted blame the other for the smallest of issues. After being assigned the task of ensuring our data was transfered every day by an early time, I started looking for another position in the company. I was tried of getting up early and missing time with the family. The long drive I found a new position fairly quickly. It was recommended to me by a friend I knew outside of work. It was a Python development position for a tool used by the entire division. I interviewed, was offered the job, and then told by my boss I couldn't move yet. Despite asking ahead of time, the answer had changed once I was actually offered the job. I spent months waiting to be allowed to move. Eventually, I was allowed to move. The one down side of this new job was its distance from my house. My first job was about 15-20 minutes away. This new one was 45 minutes away, on a good traffic day. It was worth it though. I liked the job better. I worked much more closely with my users and over the next six years the job evolved from supporting a desktop application to building an entire analytics environment. We built a system to track failure reasons and determine who was responsible for paying a warranty (saved millions of dollars every year since implementation), a predictive failure dashboard allowing service engineers to reach out to customers before something happened, expand from desktop to web development and deploy a large an expensive enviroment for hosting tons of sensor data from machines around the world. The down turn Unfortunately, the economic situation for Caterpillar, especially after the down turn, was always \"it's going to get worse before it gets better\". For years, I watched rounds of layoffs. I watched employees with decades of experience be escorted out while underperforming employees remained in place. Decades and decades of knowledge walked out the door, never to return and were replaced, most often, with nothing. My team avoided the brunt of these layoffs, but they were never fair from our doorstep. The looming threat of \"maybe next time\", was always there. With a large number of layoffs, it's not surprising that someone would walk out with confidential information. It happened. IT and HR cracked down on the rest of us. First came the removal of little things - USB access - then came the increased security on the laptops. Security software that took 4GB of memory on a fresh install. On day one of a new machine, half of your system resources went to \"security\". Restarting your computer was a 15 minute process due to this software. Then HR stated that you must be in the building 8 hours a day. Working from home in the evenings didn't count. This was the straw that started my job search. With a 45-60 minute drive each way, I'd come to an agreement with my boss that I'd spend a majority of the day in the office but do releases in the evenings from home. This allowed me to be with my family at nights and finish the day after they'd gone to bed. HR's new rules eliminated this. So, I worked out the ability to work from home two days a week. This still wasn't ideal for those three nights that I was home later, but it improved home life a bit, for a while. The kids were growing and starting to be involved in more after school activities. Activities that I had to pick and choose whether or not I could attend due to this decree to keep my chair warm in the office. I hated missing these things. The kids hated me missing them. Then HR \"reminded\" us that it was still required that we are in the office for 8 hours a day. This didn't feel like a professional work environment any more. It felt like we were being treated like little kids by HR. The new job Eventually I found a new job. My first day is, officially, tomorrow. Today is my last day at Caterpillar. It is unfortune that I have to leave. I really enjoyed the work and the people. My managers have worked with me (and the rest of my team) to help us be as flexible as possible. But, I can't ignore that I'm missing out on family things more and more. Caterpillar says things like \"work-life balance\", but I'm not sure they actually believe that any more. If they do, it's not a \"work life balance\" for families where both parents are employeed - especially if they aren't both employeed at Caterpillar. A week before I found the new job, a new flexible work schedule was announced where employees could work 80 hours over the course of 9 days instead of the usual 10, giving you every other Friday off. At first glance, that sounds appealing. But, after minute or so of figuring of how it works there is a fatal flaw: I'd be home even later over the course of 9 days and then be home on a Friday - when the rest of the family is at work or school. That doesn't solve anything for me. The new job is a full time remote position. I'll be home every day. I can step out for an hour to go to a class room party or take a morning off to go on a field trip. The flexibility I've been told about is amazing. The official vaction policy is: Employees have the authority to use their judgment and discretion and take temporary periods of time away from work as vacation, without loss of pay, as their work permits. Sold! Goodbye Yellow! Caterpillar has been my home for a decade. Despite my complaints about work life balance, above, I'm not leaving the company with malace or illintent. Caterpillar is a large company and will continue long after I've left. The people that are there, especially my coworkers, are intelligent people who build amazing things. Unfortunately, in my case, Caterpillar seems to have forgotten that those intelligent people like to spend time doing things other than keeping their chairs warm. I am thankful for the decade of experiences, the professional relationships, and chances to work on interesting projects. Caterpillar, I wish you well in the future. If you are going to listen to one piece of advice from a former employee, I recommend you take a look at those employees in your work force where the household is a dual career house. Your work life balance initiatives do not appear to account for those employees and it is for that reason that you lost me. Goodbye Yellow. I'll always remember my time with you."},{"tags":"Review","url":"django-fullstack-bootcamp-course-review.html","title":"Review of Udemy's Python and Django Full Stack Web Developer Bootcamp course","text":"Introduction After ploughing through two machine learning prerequisite courses , I wanted to have a change in content for the next course I took. I've used Django in the past to build IRVING - a dashboard that allows users to run queries against many database types and display the results in one location. The majority of this was done almost 6 years ago though. It was built to assist in a job I no longer hold. Since then, I've ignored it and haven't used Django for a major project. I have used it to test a small snippet of code here or there, but never a complete application. I remember liking Django. To that end, I decided to take Jose Portilla's Python and Django Full Stack Web Developer Bootcamp course on Udemy. Thoughts on the course I am conflicted on how to rate this course. On the one hand, it covers a lot in the 30ish hours that the lectures take. Starting at the very basics and moving through more advanced Django topics, it covers HTML and CSS then moves to Bootstrap, Javascript, the DOM, jQuery, and Python before finally getting to Django. On the other hand, it moves through those basic lectures very slowly. Django isn't even reached until two thirds of the way through the course. The lectures were a combination of hands on coding and an explanation of what the code is doing. The instructor types slightly faster than me. This was a problem only at transitions between files, for example moving between an HTML file and a CSS file. I found myself rewinding the video lectures many times around these transitions. One thing that I liked very early on in the lecture was the introduction of GitHub's ATOM text editor . I've used this once before, for a job interview that went very poorly (another story, for another time). I'd heard good things about it though. I used it throughout the course as a way to force myself to use something other than PyCharm. Now that the course is complete, I have decided that I like ATOM, but not enough to switch from PyCharm. It does have some nice features, especially with plugins, that I may utilize when building boiler plate code and templates though. Foundation vs Django content The foundational material - HTML, CSS, Bootstrap, Javascript, jQuery and even the Python - dragged on a little long for my taste. As I mentioned above, this foundation took up the first two thirds of the course. I'd have prefered it if more time could be spent covering other aspects of Django, especially some of the concepts that are thrown at the student in the last clone project. The Django content, itself, was great. It started at basics like setting up a project and single application within the project and moved through URL routing, templates, views and briefly touched on the admin side. There were advanced topics on class based views and the debug toolbar, both of which are important when developing. I can't help feeling that important things were missed though. For example, the admin side is barely touched. It is incredibly capable, yet the most that is done in this course with the admin backend is registering a model to appear. Groups and permissions aren't touched at all. Customizing views in the backend aren't mentioned, either. Another thing that I was hoping would be covered as part of the \"and much,much more!\" bullet in the course description was both signals and channels , but they are not mentioned. Clone projects are useful in giving students a \"real world\" example that they are familiar with to bring everything they've learned together. There are two such projects in this course, a blog and a message board/social media site. The blog one was a logical extension of the 5 parts that were covered by Django. The message board, however, wasn't as useful. In an effort to be more \"real world\", the instructor missed a lot of steps in the preceeding foundational lectures. This project introduces multiple applications in the same project, but never introduced that concept previously and doesn't expand on it much here. Instead, the project becomes a combination of speed coding to keep up with what's happening in the lecture and copy and pasting code from the notes when the instructor does the same thing. I appreciate the more realistic project, but for all the time that was spent building the foundation of Django knowledge, there is a big gap between the last lecture and this clone. Final Thoughts This course was worthwhile, once I made it through the foundational courses. It was a great refresher for what I'd done 6 years ago. Either I did things very inefficiently then, or Django has had a ton of improvements (or both), but looking back at IRVING, I see many things that could be improved. I'm not sure if I actually will though, just due to not using the application any longer. The long foundational courses either should have been condensed. I would have rather had more focus on Django and covered more topics there. This is especially true after completing the second clone project. New concepts were used in this project, but glossed over as \"there is excellent documentation on this\", instead of the in depth explanations that were provided earlier in the course. I'd recommend this course for the Django topics. However, if you are coming into the course with any type of web development background (even the relatively basic one I have), be prepared to be bored during the first half of the course."},{"tags":"Side Activities","url":"stack-overflows-problem-feedback-from-an-experienced-user.html","title":"Stack Overflow's Problem - Feedback from an experienced user","text":"Introduction Stack Overflow launched in 2008. As it nears it's 9th year of operation, I am afraid the resource that I depend on is losing it's way. Stack Overflow launched after I graduated college. I can't imagine how helpful it would have been during that time period, but it's been invaluable in my professional career. I joined the site about a year after it's public launch, in October 2009. In that time, I've gone from lurker to participant to moderator candidate (several times). I know Stack Overflow and Meta Stack Overflow. I am a moderator on another Stack Exchange site and have a good understanding of how the network operates. I also am one of the most prolific reviewers in the Stack Overflow Low Quality Posts review queue and have built several applications that work with the Stack Exchange API. I am a power user and know the network and the community. With those credentials out of the way, I want you to understand that I am active on the network. I am in good standing on Stack Overflow and am not a disgruntled user. I am a concerned user. I am getting more and more concerned that Stack Overflow - the company - is losing it's way. This post isn't another \"Stack Overflow sucks\" post (Google if you're curious). I'm going to present a few areas that I'm concerned about and hopefully provide either my suggestions for improvement or acknowledge that I don't know the solution but want the team to be aware of in the future. I still believe Stack Overflow is an incredible resource. I'd just like it to fix some of the perceived missteps that have occurred over the past two years. What's going wrong? In the past two years, Stack Overflow has made several changes that the established community hasn't liked. Some of these changes still are not liked. These changes include the Teams feature, the new top bar, the Stack Overflow (versus existing Stack Exchange) mobile app, and Documentation. There have also been minor missteps that have caused a rift between portions of the community and the company. These areas include multiple political stances, and a number of post quality improvements that haven't been made. Each of these, separately, is a minor problem that could be worked through and moved on from. The problem I'm seeing is that taken together, all of these are causing a rift between users, power users and the community. Let's work through each of these items. Teams Teams was announced in October 2015 and clarified a week later. It was then shut down after nine months. The page it used to go to now has the following blurb (emphasis is mine): Teams was in private beta for almost a year with 295 teams created and while we believe in its potential value, after a lot of consideration we've decided to un-ship the idea for the time being. We've realized that making a successful version of the Team page, as we originally proposed would ultimately take more time and resources than we want to devote to it. Our resources are currently allocated on projects to enhance and improve quality on Q&A, Documentation, and Jobs on Stack Overflow, as a result we don't have the dedicated developers to get Teams to its fullest potential. The intention was to add more features to Teams, but we never expanded it to anything beyond a team description. The emphasized section sounds good, except that the one section that is taking up a majority of time (Documentation) has it's own major issues. The area that many power users want developers to focus on is Q&A. The problem with Teams, and many of the projects mentioned in this post, is that this was a feature that removed focus on areas the community wanted improved. Meta Stack Overflow has been asking for improvements to reduce the number of low quality posts for years. Moderators have been asking for better tooling. The review queues are overflowing with tasks and the number of users performing reviews isn't high enough to keep up. Teams was built without a true end goal and users weren't entirely sure what to do with it. This was the first in a series of mis-steps that continue to plague community interactions when new features are announced. Top Bar The new top bar was announced in November 2016. It went through a handful of iterations before being released in mid-February 2017. During the iterations users provided feedback. When initially released, though, much of this feedback felt ignored. Things like notification overload , stickiness of the top bar , and hidden review counts were all mentioned during the three months of testing but not implemented until the change was live to millions of users. After three months of usage, a larger problem was noticed. One of the review queues was constantly full . One of the changes that was made with this top bar was that the \"Review\" button no longer linked directly to the \"Suggested Edits\" review queue. Now it went to the page showing all review queues. Users that used to click once to get to a review queue were now presented with a list of queues to work in. Some of these queues are much more time consuming that others. It turns out the number of reviews being done has decreased significantly since the top bar was implemented. The spike in reviews in February 2017 is when the new top by was released. Since that release, the number of reviewers has plummeted. This has been attributed to notification fatigue and not linking users directly to the Suggested Edits queue. Three months after implementation, it took the community asking for results ( disclaimer: I asked the question ), to find out how the top bar has been performing. It turns out that the top bar is performing decently well compared to what the developers were expecting, with the exception of fewer review tasks being performed. The problem with this project, is that it's felt unneeded and has materially impacted one of the quality control features of the site. There is still a vocal group of users that don't like it because it doesn't match the rest of the network. Several are concerned about the review queue problem. Experienced users felt that they were ignored during the beta tests. Users provided feedback and examples of problems and it was only after implementation when millions of other users experienced the same thing that these changes were made. Mobile App A recent announcement (as in last week, at the time of this post) announced a new Stack Overflow mobile application . The community response was not positive. Users asked why a new application was being built when one already existed (the response was \"branding\"). Users asked why the new app was less functional than the existing one (it's limited to Stack Overflow verus the entire Stack Exchange network). Users asked why it took a year to develop and why the existing application hasn't received bug fixes in that year. I think one of the most disappointing things about this is a response I received in the comments from the VP of Engineering: @Andy You're right, it wasn't worth a year. There's a long, sad story here, but it was originally expected to only take a few months and... well, here we are a year later. We decided to go ahead and launch and see what we can learn, and we'll reassess from here. – David Fullerton? May 17 at 16:26 Another user expressed the dissatisfaction in a very pointed way . They provided a list of features that the community has asked for over the years that many feel have been ignored. The VP's response to this wasn't encouraging either: I appreciate that there are a lot of issues on Stack Overflow that need to be addressed, and maybe we haven't been responding to them as quickly as we should. But Stack Overflow Q&A is a big, established product, most of the problems left are hard, and we can't let maintenance become the only thing we work on or we'll just slowly run out of money and go out of business. We are trying to both maintain Q&A and solve new problems for developers and reach new audiences. The latter is hard, and maybe we'll fail on a lot of our ideas, but we're not going to stop trying. – David Fullerton? May 17 at 21:10 This sounds like work on the Q&A side is feature frozen at this point. They are done innovating in this area and instead are focused on drawing in users via other features - like Jobs or Documentation. Multiple times in the comments the new app was promoted as being able to use the Dev Story or Jobs features in the future. Perhaps it's just me, but I don't apply for jobs via my phone. That doesn't seem like a good way to really put the effort needed into a cover letter or application. Documentation Now we've reached Documentation. This is the project that's sucked up development time over the past two years. This is the project that Stack Overflow developers are defending tooth and nail and the community has all but given up on. Documentation was announced back in August 2015. It's had a ton of updates since then. It was met with initial enthusiasm but that quickly turned around. When the system launched for all users, one of the first complaints was that the reputation generated via documentation was doing back things to the main Q&A site. This resulted in a massive recalculation of reputation and resulted in many users losing a lot of their internet points. Another change that was announced with the introduction of a new review queue for documentation . Initially, developers didn't expect the low quality to begin immediately, it seems. Long time users weren't surprised. Now we've reached the point where the company is realizing that the users knew what they were talking about. Documentation is undergoing a massive change , to the point that much of it is being completely redone - not fixed - scraped and redone. This project has years worth of feedback from the community that has been ignored. It is the blacksheep of Stack Overflow and many community users feel that quality of the content is lacking so badly that they don't participate any longer. This feeling isn't helped that many users have been explaining why things aren't working for a while and it's only after two years the developers are starting to realize the private beta testers, public beta testers and experienced community users mentioned many of these problems. In this particular instance, the company took Jeff Atwood's advice (cofounder or Stack Overflow) to not let the community tell you what to do to heart. To the company's surprise, a community of developers that live in programming documentation had decent thoughts on what does and does not work in programming documentation. Politics For many users, the lack of true social features on Stack Overflow and across the Stack Exchange network has been a good thing. You can't easily follow a single user, you can't send private messages to a user, and you can't really do anything on the site that isn't public to everyone. The focus is on content, not opinions or social interactions. This breaks down once and a while though when a big political thing occurs. The two most frequently mentioned instances are the response to the Obergefell v. Hodges Supreme Court decision and the response to President Trump's initial immigration executive order . Both of these caused huge uproars within the community when the company took a stand. These stands caused problems due to users holding opposite political views, users not wanting politics on their programming site, users not wanting to deal with the drama caused by the vocal members of the other groups. This led to an apology . The community wasn't pleased with this apology. Users mentioned in multiple answers to this apology that they don't want the company to post such political agendas on the site. It's out of place for a programmer community. Both of these instances are still brought up on Meta when the community feels that the company is imposing on them. I don't really have advice or suggestions on this problem other than \"I don't want to see this on Stack Overflow, because these hot button issues cause so much drama that nothing gets accomplished\". These posts grind Meta and chatrooms to a halt while everyone expresses their opinion on the post, on the post's existance, on one another and on related issues. Quality Improvements Finally, the community has been asking for years about ways to improve the quality of posts on the site. Stack Exchange started a project to improve the quality back in October 2016. This generated 80 different suggestions on how the community sees \"quality improvement\" taking place. Since then there haven't been any updates on the status of this project or even subprojects. This was brought up during all of the projects listed above by long time users. The hope was that this quality project would help. Being ignored hasn't brought any good feelings. The lower quality has been measureable and seen less participation from experienced users. The Fix Above I've pointed out several issues that I've seen over the past two years. These issues are part of a bigger problem though. It seems that Stack Overflow doesn't know to how handle it's community size any longer. It's in the top 300 sites visited in the US and receives half a billion views globally per month . Couple this with the fact that they don't have a sustainable business model yet and have a sizable team with good benefits and they are getting concerned. Q&A is what built Stack Overflow, but it isn't enough to sustain them. Thus, the other projects are being created. Unfortunately, in this process, it seems the company is forgetting it's existing user base at the expense of expanding to new users. Existing users are getting frustrated with the lack of quality improvements, being ignored and not having changes that benefit their use cases. Documentation has taken up a giant chunk of time and developer effort and it's all been wasted. The announcement that it is being redone has been met with \"thanks\" from the community, along with warnings to consider that \"quality\" problem. We'll see how it plays out, or if that quality issue is ignored like their own Quality Project. Which brings us to the final point I want to make. I think the feeling of Q&A being \"done\" is the biggest problem I've had with Stack Overflow over the last year. New features aren't being built in that space. Instead of focusing on some of the \"hard\" problems, the company is throwing stuff at the wall and hoping something will stick. Unfortunately, the four biggest projects in the last year have either failed completely (Teams, Documentation, Mobile App...perhaps) or have significant unintended consequences that isn't helping the quality issue users have been reporting for years. Power users, the underlying community that has put time and effort into growing Stack Overflow to that it is today, is feeling ignored. It is only after months or years long experiments fail that community opinions are finally validated or considered. Users have expressed concerns in each of the above projects repeatedly. Yet, those opinions were not addressed. The silos that the developers have built around themselves are causing the company to lose touch with it's community. This is being done at the expense of alienating the users that care and the cost of developer time. Users want a high quality site with answers to their questions. Even new or potentially new users want this. Stack Overflow continues to avoid dealing with that problem because \"it's hard\". The unfortunate thing is, this is costing the site users that return to provide more than one answer . This chart is showing the number of answers provided per month by different types of users. Users that have provided more than 100 answers, between 11 and 100 answers, between 2 and 10 answers and only a single answer. The furthest data point on the right is an artifact of being an incomplete month. From this chart, we can see that the only group that has continued to rise are users that provide a single answer over time. The other groups took a steep drop in April 2014 and haven't recovered since then. The number of experienced users that are participating has dropped. What happened in April 2014? That's been answered by a Stack Overflow community manager. The theory is that users aren't getting answers to their questions and due to being ignored they never return to participate further in the site. Another community manager also provided an answer : Starting around 2013 and peaking around March, 2014, people began asking fewer interesting questions. That lead to a decrease in voting on questions and fewer answers being given. Since the feedback on these uninteresting questions was discouraging, people began asking fewer questions on the whole. Meanwhile, truly poor questions continued being asked with little regard to negative feedback. Stack Overflow users began noticing increasing numbers of truly awful questions and decided, rightly, that downvoting and refusing to answer them is the best remedy. These questions fit broad categories of awful and users began withholding votes from questions that were not themselves awful, but bore some of the markers of awful. Fewer of these questions got answered and askers of mediocre questions did not see any point in trying to improve. Thus began a slow spiral downward. Not all is lost though, because there are the upticks. I hope it's enough to break the cycle, but I really fear that something needs to be done about this quality issue. This is the issue that is brought up by the experienced community. Where to from here? I continue to invest my time and effort into the community, but even as an active user who really wants the company and community to succeed, it's getting harder and harder to ignore that those of us that have been around for years are not being listened to any more. We're being treated as the grumpy old person that grumbles about the way things used to be. Our experiences on the site are brushed aside as being unhelpful to new users. That completely ignores that fact that we are still trying to reach the goal on which Stack Overflow was created: \"With your help, we're working together to build a library of detailed answers to every question about programming.\" To do this, we need high quality questions and answers so that we can actually provide help to all users. I think this is the biggest challenge that Stack Overflow is going to face in the next 18 months. I want Stack Overflow to continue to grow. I also want Stack Overflow to have high quality content. I think my experience and the experience of others can help build the features to accomplish this. We just need Stack Overflow to refocus on the Q&A portion of their network again."},{"tags":"Review","url":"deep-learning-prereq-linear-logistic-regression.html","title":"Review of Udemy's Deep Learning Prerequisites - Linear and Logistic Regression courses","text":"Course My last review covered Lazy Programmer's Numpy prerequisites course on Udemy. In my goal to learn more about NLP, I'm continuing through the courses this instructor has designed. To this end, that requires two additional prerequisites: Linear Regression and Logistic Regression . This review covers those two seperate courses. Thoughts on the lessons These two courses are each short three hour courses giving a crash course in how to perform linear regression analysis and logistic regression analysis. The courses cover both the theorectical math needed and the more practical Python code needed. The instructor states, several times, that simply copying the code isn't good enough in the later courses. The mathmatical background is needed to succeed later. With that in mind, I spent most of these lectures relearning much of the math. The instructor provides several detailed \"Theory\" lectures and I found those helpful. After the theory is introduced, the next step is applying this in code. The instructor's code is short and to the point. It shows the what was just taught and provides efficient numpy code to convert the theory into something workable. Once again, I'm happy with the instructor and their teaching style. I think this will go well with the future courses I'm planning on taking that are by the same person. I'm continuing to brush up on those math skills that haven't been used in a while, but at this point, I am happy with how everything is heading for this series of courses. I've posted my IPython Notebooks on GitHub for all three of the courses related to the prerequisites. Final Thoughts I am still looking forward to taking the courses designed by the Lazy Programmer . The series of prerequisites were very helpful in helping me to catch up on the skills I'll need, but didn't know (or remember much of) ahead of time. With the actual content coming up, I am hoping the lecture style and hands on coding sections continue be the same quality. The Linear and Logistic Regression courses didn't require much in terms of software - Python, numpy, pandas and matplotlib - but I still need to decide how I'm going to proceed with the courses. I'm not sure whether or not I want to set up a Linux machine or a virtual machine. Windows doesn't seem to play well with all the libraries I'll be needing. It may make my life easier too. So far, everyone seems to be using a Mac or Linux machine to teach these courses and offer only basic \"It may work on Windows\" advice."},{"tags":"Technical Solutions","url":"choosing-orm-library.html","title":"Choosing an ORM library for a new project","text":"Project History The SmokeDetector project is over three years old at this point. It's grown from a small python script to a decently sized application that integrates with another project. In that time, it's expanded what types of spam and patterns it looks for, what chat rooms it posts to, what external services it integrates with and how permissions to use the system are determined. A lot has changed under the hood. I was hoping to put a cool chart here showing code change over time, but some early decisions with the project really throw off the chart. Using a Ship of Theseus analogy for code, you can see how much has changed. The basic idea is, if a ship leaves port and replaces every plank along it's journey, is it still the same ship when it returns? With code, the idea is to apply this to lines of code in an application. What happened in 2014?! In late 2014, the project attempted their first machine learning method of detecting spam. In this time period, a commit was added that added about 200,000 lines of code to the project. This was almost all training data for a Bayesian algorithm. It wasn't needed and probably shouldn't have been added to the main repository. Unfortunately, it stayed in the repository for over a year and was finally removed in late 2015. This is the cause of the weird graph above, and why almost everything added in 2014 looks like it's missing in later years. What has really changed? After eliminating that bayesian directory from git history, you can get a much better idea of how much has changed. Very little of the original code, written in 2014, remains untouched. The explosion in code after that is due to new detection patterns, chat commands (and a rewrite), integration with MetaSmoke and the introduction of blacklists. Even more dramatically, you can see how long a line of code is expected to survive in the code base. Within one year, the team is removing over 40% what's been committed to the repository. Looking at these commits, it was determined that a vast majority aren't even code . They are new items to blacklist or new patterns to detect. Enter the database All of this type of data can be stored in a database and managed outside of code. In early 2017, those discussions started taking place. Several team members come from a Ruby background and were familiar with it's ORM method of accessing databases. They wanted something similar when a database was brought into SmokeDetector. A bit of research was done and it was narrowed down to peewee and SQLAlchemy . How to choose? Fortunately for the SmokeDetector team, there weren't any strong opinions either way. The biggest reason for choosing one over the other came down to a comment made by the peewee author on reddit. They state: [...] SQLAlchemy is the gold standard for ORM in the Python world. It has a very active community and a maintainer who is committed to excellence. If you're a glass-half-empty guy, to put it another way, you can't go wrong if you choose SQLAlchemy. The weaknesses they list for using their own package is the smaller ecosystem, support and number of developers. Technical differences That's a boring story though. Not to be deterred from such a glowing review from a competitor, I wanted to see what the technical differences were between the two solutions. To that end, I put together a small Python notebook showing the differences between peewee and SQLAlchemy in a handful of tests. These tests included inserting two settings in an SQLite database, retrieving one, inserting a large list of users and then retrieving a subset of those users. The results were...unremarkable. The two libraries each took two tests (out of four) for being faster than the other. In both cases where SQLAlchemy was faster, it was between two and six times faster. Where peewee was faster it was between a fraction faster and twice as fast. The time scales are so small though, and SmokeDetector doesn't need to have thousands, hundreds or even tens of hits to the database a second. A hundred extra milliseconds isn't going to cripple anything it handles. Thus, the choice was made based on the recommendation of the author of the peewee library. SQLAlchemy has a larger community and better support."},{"tags":"Review","url":"deep-learning-prereq-numpy.html","title":"Review of Udemy's 'Deep Learning Prerequisites: The Numpy Stack in Python' course","text":"Course I found Lazy Programmer's Deep Learning courses on Udemy. A couple of those seem very interesting, specifically the ones about Natural Language Processing. I'd love to be able to enhance either my comment flagging , Zephyr or SmokeDetector with language processing capabilities. I also have tried, multiple times, to come up with a way to detect duplicate questions on Stack Overflow, but my limited knowledge of true NLP concepts has proved to be unhelpful in that task. I have the goal of getting to the NLP courses and completing a few other data science and machine learning courses this year. I've picked out a few that look interesting and useful in either personal or professional projects. That will be my main focus in the next several months. When I need a break from these topics, I may find another soft skills course to look at. Or, I have a play through of Witcher 3 waiting for me. To start this journey, I need to cover some prerequisites. Data Science and Machine learning utilize several important libraries and I need to know them or brush up my skills when using them. I took the Data Analysis with Pandas last winter and found that helpful. Lazy Programmer offers one covering the numpy stack. That's what I just finished. Thoughts on the lessons Deep Learning Prerequisites: The Numpy Stack in Python is a brief two hour course that gives a crash course in how to use numpy, pandas, matplotlib and scipy. It touches each of these very briefly. The goal of the course isn't to get you intimately familiar with the libraries. The goal is to get you to know the libraries exist and a few of their most common tasks. In this regard, it was very useful. The course also served as a nice introduction to the instructor and their teaching style. It is all hands on though, which I like. I prefer to do the coding (and add my own comments along the way) to watching someone present a huge chunk of code and walk through that line by line. I've posted my IPython Notebooks on GitHub. One other thing this course does is introduce users to the assumptions the instructor makes about the students. Machine learning and data science, in general, have a lot of math behind them. The instructor assumes that students have the theorectical background for this and teaches how to implement this math in Numpy. Note to self: Brush up on those math skills. I haven't used some of this in a long time. Final Thoughts This is an introduction course to the bigger \"Deep Learning\" courses that Lazy Programmer offers. It does it's job well and has increased my confidence in the instructor. My last couple courses haven't been great and I'm happy that this quick two hour course was available. I'm looking forward to taking some more of these courses. One thing I will need to consider as I go through this course: whether or not to either set up a Linux machine or a virtual machine. Windows doesn't seem to play well with all the libraries I'll be needing. It may make my life easier too. So far, everyone seems to be using a Mac or Linux machine to teach these courses and offer only basic \"It may work on Windows\" advice."},{"tags":"Review","url":"become-a-product-manager-review.html","title":"Review of Udemy's 'Become a Product Manager  | Learn the Skills & Get the job' course","text":"Course I recently finished the \"Become a Product Manager | Learn the Skills & Get the job\" course on Udemy. I had picked this up in December during one of Udemy's frequent $15 dollars. I was looking to take something non-technical that covers more of the \"soft skills\" I utilize in my job. I do a lot of software \"product manager\" work, without the title, at my job and was hoping to brush up on those skills. Thoughts on the lessons This course is billed as a 13 hour course, with activities and interviews with product managers. The course is taught by Cole Mercer (a Senior product manager at SoundCloud ) and Evan Kimbrell (founder of Sprintkick ). The course is packed with useful content, resources and interviews. I learned a lot and reinforced a lot of what I already knew. That information will be incredibly useful at work. That glowing review said, I had a hard time getting through the lessons. Despite the amount of useful information, I found the lessons to be...boring, at best. At worst, the instructors attempted to throw jokes in that just didn't work for this format. That type of distraction made it hard to stick with the lessons. Additionally, the quizzes that cropped up occassionally, were treated as a joke by the course creators. There were joke answers, there were joke explanations for answers and their were questions unrelated to the content but instead about the instructors. Finally, there are several points in the lessons that ask students to go the message board and provide feedback, do a quick activity, and interact with one another. I did a couple of these. I stopped doing them when I didn't receive any responses from either other students or the instructors. A closer look at the message board shows this is very common. I have to scroll back through a couple \"load more\" clicks on the message board to find any posts that even have a response. The interaction I was hoping for from the instructors just doesn't exist with this course. Final Thoughts The content in this course is amazingly helpful. There are links to articles that I'll be able to use for years. The information in the lectures is also helpful, if you can get through the bland presentation of the material. The frequent, distracting, jokes and lack of interaction makes this course less stellar. Overall, I'd consider this a middle of the road course. The presentation is what drags down this information packed course."},{"tags":"Programming Projects","url":"can-a-machine-be-taught-to-flag-spam-automatically.html","title":"Can a machine be taught to flag spam automatically","text":"Introduction This post was originally published on Meta Stack Exchange on February 20, 2017. I've republished it here so that I can easily update information related to recent developments. If you have questions or comments, I highly encourage you to visit the question on Meta Stack Exchange and post there. The post was featured across the entire Stack Exchange network for a week, too. This drove a huge amount of traffic to the question and resulted in some valuable feedback: TL;DR: We did it, so... yes. What is this? Charcoal is the organization behind the SmokeDetector bot and other nice things . This bot scans new posts across the entire network for spam posts and reports them to various chatrooms where people can act on them. If a post has been created or edited, anywhere on the network, we've probably seen it. The bot utilizes our knowledge of how spammers work and what they have previously posted to come up with common patterns and rules to detect spam in the new and updated posts. You've likely seen the SmokeDetector bot if you visit chatrooms such as Tavern on the Meta , Charcoal HQ , SO Close Vote Reviewers and others across the network. Over time, the bot has become very accurate. Now we are leveraging the years of data and accuracy to automatically cast spam flags. With approximately 58,000 posts to draw from and over 46,000 true positives, we have a vast trove of data to utilize. What problem does this address? To put it simply, spam . Stack Exchange is one of the most popular networks of websites on the Internet, and all of it gets spammed at some point. Our statistics show that we see about 100 spam posts per day, on average over the last three months. A decent chunk of this isn't the type you'd want to see at work (or at all). The faster we can get this off the home page, the better for all involved. Unfortunately, it's not unheard of for spam to last several hours, even on the larger sites such as Graphic Design. Over the past three years, efforts with Smokey have significantly cut the time it takes for spam to be deleted. This project is an extension of that, and it's now well within reach to delete spam within seconds of it being posted. What are we doing? For over 3 years, SmokeDetector has reported potential spam across the Stack Exchange network so that users can flag the posts as appropriate. Users have provided feedback to inform the bot on whether the detection was correct or not (referred to as \"feedback\"). This feedback is stored in our web dashboard, metasmoke ( code ). Over time, we've used this feedback to evaluate our patterns (\"reasons\") and improve our accuracy. Several of our reasons are over 99.9% accurate . Early last year, and after getting a baseline accuracy from jmac (thank you!), we realized we could use the system to automatically cast spam flags. On Stack Overflow the current accuracy of users flagging spam posts is 85.7%. Across the rest of the network users are 95.4% accurate. We determined we can beat those numbers and eliminate spam from Stack Overflow and the rest of the network even faster. Without going into too much detail (if you really want it, it's available on our website ), we leverage the accuracy of each existing reason to come up with a weight indicating how certain the system is that a post is spam. If this value exceeds a specific threshold, the system will cast up to three spam flags on the post. We cast multiple flags utilizing a number of different users' accounts and the Stack Exchange API. Via metasmoke, users are given the opportunity to enable their accounts to be used to flag spam (You can too, if you've made it this far). When a post is eligible for flagging because it exceeded the threshold set by each individual user, accounts are randomly selected from the pool of enabled users to cast a single flag each, up to a maximum of three per post so that we never unilaterally nuke something. What are our safety checks? We designed the entire system with accuracy and sanity checks in mind. Our design collaborations are available for your browsing pleasure ( RFC 1 , RFC 2 , RFC 3 ). The major things that make this system safe and sane are: We give users a choice as to how accurate they want to be with their automatic flags. Before casting any flags, we check that the preferences the user has set result in a spam detection accuracy of over 99.5% over a sample of at least 1000 posts. Remember, the current accuracy of humans is 85.7% on SO and network wide it is 95.4%. We do not unilaterally spam nuke a post, regardless of how sure we are it is spam. This means that a human must be involved to finish off a post, even on the few sites with lower spam thresholds. We've designed the system to be tolerant of faults - if there's a malfunction anywhere in the system, any user with access to SmokeDetector can immediately halt all automatic flagging - this includes all network moderators. If this happens, it needs a system administrator to step in to re-enable flags. We've discussed this with a community manager and have their blessing on the project. Results We have been casting an average of 60-70 automatic flags per day for over two months, for a total of just over 4000 flags network wide. These flags were cast by 22 different users. In that time, we've had four false positives. We would like to be able to automatically cancel these particular cases. This isn't possible though, so we've created a feature request to retract flags via the API . In the mean time, the flags are either manually retracted by the user or declined by a moderator. The above graph plots the weight of the reasons against its overall volume of reports and accuracy. As minimum weight increases, accuracy (yellow line and rightmost Y-axis) and total reports (blue line) on the left-hand scale increase. The green line represents the number of true positives, which are verified by SmokeDetector user feedback. This shows the number of posts we've automatically flagged per day over the last month. The jump on February 15th, is due to increasing the number of automatic flags from 1 per post to 3 per post. You can see a live version of this graph on metasmoke's autoflagging page . Spam arrives on Stack Exchange in waves. It is easy to see the time of day that many spam reports come in. The hours, above, are UTC time. The busiest spam times of day are the 8 hour block between 4am and Noon. We have affectionately named this \"spam hour\" in the chat room. Our goal is to delete spam quickly and accurately. The graph shows the time it takes for a reported spam post to be removed from the network. This section has three trend lines that show these averages. The first, red section is when we were simply reporting the posts to chatrooms and all flags had to come from users. You can see we are pretty constant in the time it takes to remove spam during this period. It took, on average, just over five minutes to get a post removed. The green trend line is when we were issuing a single automatic flag. At implementation, we eliminated a full minute from time to deletion and after a month we'd eliminated two full minutes compared to no automatic flags. The last section, the orange, is when we implemented three automatic flags to most sites. This was rolled out last week, but it's already had a dramatic improvement on the time to deletion. We are seeing between 1 and 2 minutes to time to deletion. As mentioned above, spam arrives in waves. The dashed and dotted lines on the graph show the average deletion time during these two different time periods. The dashed lines show deletion time during 4am and Noon UTC, the dotted lines show the rest of the 24 hour period. An interesting thing this graph shows is that time to deletion during spam hour was higher when we didn't cast any automatic flags. It was removed faster outside of spam hour. That reversed when we started issuing a single auto-flag. The spam hour time to deletion is slightly lower than the average. Comparing the two time periods though, time to deletion during non-spam hour at the end of the non-flagging time period and the end of the single flag period are roughly the same. We'll update these in a few weeks too, to better show the trend we are seeing with three automatic flags. Discussion We are confident in SmokeDetector and the three years of history it has. We've had many talented developers assist us over the years and many more users have provided feedback to improve our detection rules. Let us know what you want us to elaborate on, features you're wondering about or would like to see added, or things we might have missed in the process or the tooling. Take a look at the feature we'd really like Stack Exchange to consider so that we can further improve this system (and some of the other community built systems). We'll have Charcoal members hanging around and answering your questions. Alternatively, feel free to drop into Charcoal HQ and have a chat."},{"tags":"Review","url":"loony-corn-machine-learning-review.html","title":"Review of Udemy's 'From 0 to 1: Machine Learning, NLP and Python - Cut to the Chase' course","text":"Course I decided to squeeze one more course in before 2016 ended. This time I chose \"From 0 to 1: Machine Learning, NLP and Python - Cut to the Chase\" by the LoonyCorn team on Udemy. This course was a 20 hour overview of machine learning techniques. I've been interested in taking some more formal training on various machine learning algorithms and this course sounded like it would be useful. Thoughts on the lessons I was very disappointed in this course. The lessons had large portions of time that overlapped one another, the background music was distracting, the instructor's speaking volume isn't consistent, the \"coding\" portions weren't done in a way that allowed me to follow along, code was presented with bugs and the instructor glossed over these with a simple \"we'll fix those later\". The theories were presented fairly well but really should have been condensed into a handful of lessons instead of spread out so broadly. It took me a few lessons to realize it, but the course notes - effectively power point slides that are hand written during the lecture - and the instructor's voice were recorded at different times. Multiple times through the lessons, the exact same portion is used repeatedly. Lecture notes are \"rewritten\" or explained again. This repetition is used ineffectively, as the instructor's voice isn't consistent within a single lecture. The repeated parts are either much louder or much softer than the surrounding lecture material. My biggest disappointment, though, was with the coding exercises. I was hoping for some hands on activities. Instead, the instructor spoke while the code - with all comments - was coded \"live\". The code was done quickly and offered no time for me to both code and listen to the explanation. Like the lecture notes, the code and the vocal explanation was recorded at different times. This means when a bug is encountered in the code, it is simply glossed over in the explanation as an area they will return to later to fix. This was incredibly frustrating. Final Thoughts This course wasn't worth my time. The theory portions were just good enough that I stuck with it hoping to pick out something useful, but it never seemed to materialize. I did complete the course and don't feel I've learned anything that I can apply elsewhere. In the new year, I'll be looking for another machine learning course that, hopefully, is much better than this one."},{"tags":"Review","url":"data-analysis-with-pandas-review.html","title":"Review of Udemy's 'Data Analysis with Pandas' course","text":"It's been a few years since I finished my formal education. I've been getting the itch to take a few more structured training courses, but don't have the desire to commit to another full university degree. Fortunately, there are a lot of places online that now offer training and college courses (both free and paid). I'll be picking out a few and going through them as my free time permits. I'll be sharing my thoughts on the courses in a series of posts with a review of the course and, if available, a link to show what I produced during the course. Data Analysis with Pandas I recently completed Data Analysis with Pandas by Boris Paskhaver on Udemy. I use pandas both at work and in personal projects and constantly find new things the library can do. This course is billed as a 19 hour course spanning 173 lectures. The curriculum looked like it covered both basics and topics I was less familiar with. It also helped that the instructor was offering a coupon code on reddit to take the course for free. I decided to take the course. Spoiler : Even if this wasn't free, this class was worth the price. It covered a large part of pandas and did so in a way that didn't make me tune out the instructor, even after 19 hours. Thoughts on the lessons The course starts a bit slow with your basic \"How to install\" tutorials. The entire course is done in a series of Jupyter Notebooks and, obviously, requires that you have Python (the instructor uses 3.5), pandas and a few other modules installed before we get to the good stuff. The instructor uses OS X during all lectures, but that has no effect on how things are communicated to the student. Everything is done in the Jupyter Notebooks or at a conda prompt. This works the same across all major operating systems. The only reason I felt this was slow was because I already had all of the requirements installed due to using the library before. I understand the need for this lesson, though, and can't hold it against the instructor for needing to include this. One thing that I liked about the introduction was that the instructor provided a series of CSV files we'd use throughout the rest of the lessons. These CSVs were varied in size and composition. I thought this was a great way to keep everyone on the same page. I've seen a lot of tutorials on pandas around the internet and most of them depend on generating random data. By providing everyone with a set of CSVs it is much easier to focus on specific aspects of the data and how certain functions work. Series and DataFrames The course really begins in Section 2 which is on the pandas Series object. It covers what a series is, various methods of creating one, and then goes over the various methods and attributes you can use on a series object. The module covers 21 lessons and lays the foundation for the entire pandas library. Section 3, 4, and 5 cover DataFrames . These sections cover 41 lessons. DataFrames are the heart of the pandas library. This is the object you'll use most of the time, which explains the number of lessons that focus on DataFrames. These lessons cover everything from selecting a series (column) in a data frame, adding a new column, dropping null values and sorting values. More advanced topics include various ways of filtering a dataframe to only the data you are interested in, applying a function to all values and working with either index labels or index positions. These modules are the heart of the entire course. Text data and DateTime data Sections 6 and 10 deal with text data and datetime data. One complaint I had about these two sections was that they were separated so much. Both types of data need to be operated on the same way in pandas. Namely, you have to add either .str or .dt when calling a string or datetime function. I also think that datetimes are important enough and used frequently enough that getting a lesson in on how to properly use them early would make more sense. That complaint aside, both sections cover their content well. There isn't anything ground breaking here, especially if you've done any sort of work with either strings or dates in Python. The functions introduced all work as you'd expect based on that experience. The DateTime section also provides information on how to work with date offsets and time deltas. Adding and subtracting days/weeks/months is always important and the lessons cover how to do so pretty well. Advanced Features Sections 7, 8 and 9 cover aspects of pandas that many will use as well. These cover ways to join and group multiple data sets into a single data frame and the benefits of each method. MultiIndexes are not something I've used a lot, but after these 14 lessons I have already thought of ways I can improve my code at work to utilize these. Panels Section 11 covers a topic that was brand new to me: Panels. I have never used them or heard of them. A series is a 1D dataset, a dataframe is a 2D dataset and a panel is a 3D dataset. It is a group of dataframes. The instructor explained this concept very clearly and worked through multiple examples of how to build and use a panel. In this lesson, the panel we utilized was created by calling Google Finance for multiple companies. However, the panel that was returned was not formatted the way I expected and it bothered me for several of the lessons. At first, this was mentioned by the instructor in passing, and then seemed to be ignored. However, toward the end of the section, the course covered ways of reforming the panel to be in a different format. Input/Output and Visualization Sections 12 and 13 covered the portions of pandas that my users see at work. The end result of data manipulation. I produce visuals or excel documents and they are happy. The lessons cover how to export to CSVs and Excel (you need an ExcelWriter ). It also covers the four most common visualizations I produce at my job - line charts, bar charts, pie charts and histograms. Outside of the scope of this course, but something I'll look into, is what else matplotlib can do. We did very little to customize our plots and I know that the library can do so much more. Options The final section covers a few of the miscellaneous options pandas has. There isn't anything exciting in this section and these 4 lessons are short. Placing them at the end is a decent spot for them, as it reminds you to go look at the documentation to see what other options are available (there are a ton). Final Thoughts While I used a coupon code to take this course for free, I feel it was worth the listed price. The course covers 19 hours and 172 lessons. The may seem overwhelming at first, but the lessons are short, with most falling in the 4-7 minute range. There are a couple that reach the 10+ minute point, but they still held my attention the entire lesson. The instructor has the lessons structured in a way that allows you to pause and return at the start of any lesson without having to reexecute a long series of code. Most of the lessons are self contained, where a dataset is either reimported at the start of each lesson or a new one is introduced. The only Section that didn't seem to follow this pattern was the visualization module. That module kept using code from previous lessons. The section is less than an hour, but if you take a break you may need to rerun some of your code in this section. There are 4 \"quizzes\" in this course. If you've worked through the lessons with the instructor, the questions are very easy. I've now spent 19 hours (re)learning pandas and I feel that I've still just scratched the surface. I've learned a lot that I'll be taking back to my code, but throughout the course I still got the impression there is much more to pandas. Notebooks The notebooks that I created during this course are all available on GitHub"},{"tags":"Side Activities","url":"third-times-the-charm.html","title":"Third time's the charm?","text":"Introduction Last year, I ran for moderator ( twice ) on Stack Overflow and didn't make it through the primaries. I came close on that second run. Now, a year later, and a year more experienced, I'm going to try again. This post will document my progress through the election cycle. Spoiler Alert : I didn't win. The rest of this post details my thoughts as the election occured though. Nomination Phase The election this year took a slightly different route than last time. In previous years, the election was announced at the same time as the call for nominations began. Users had a week to nominate themselves, then we answered a series of community provided questions during the primaries, then the final election. This year, the election was announced a week in advance of nominations. During the week, a call was put out for Community questions . When a nomination was posted, the answers would be posted as well. This change was made due to how much the community needed to read during the primaries. The primaries were only a few days long and the Q&As were usually ten questions for each user. When a primary has 20-30 nominees, that is a lot of reading that was expected in a short period of time. By bringing this phase forward, now the community has the entire election cycle to read and interact with the nominees. I provided one question that was used in the final selection of questions. I mentioned last time that I thought it was a great question, so I suggested it again: Do you have any Meta posts that you're particularly proud of, or that you feel best demonstrate your moderation style? My nomination My platform isn't all the different than the last two times. Hi Everyone, I'm Andy and I'd like to be a moderator for you and Stack Overflow. I've answered the questions posted by the community here . I encourage you to take a look. Why should you vote for me? I've been a moderator on Community Building for over two years. I know the moderator tools and have worked with many of the current moderators. This interaction will continue as a new moderator here. I have a lot of helpful flags. A decent percentage of these are on comments , but not all. I'd like to help keep the site clean without adding to the current moderators' work load. I'm active in the review queues (currently holding 5th in Low Quality Post reviewers of all time), provide edits to posts, answers and enjoy the moderation aspect of Stack Exchange. I have a history on Meta.SO that shows I'm involved in the meta aspect of the site as well. I enjoy the moderation aspect on Stack Overflow (and Stack Exchange in general). I have a history of good community moderation, am here all the time and believe I can help the current team. During the first full day, I'm gotten positive responses to this post. My two favorite, so far, are: Andy's work around comment flags has been very impressive. I'm definitely curious to see what his thoughts on the mod queue are and if we could incorporate some of his work permanently on the site. Better identification of flags is something that would be very nice to have permanently. - bluefeet Stack Overflow Community Manager and There are always some nominees for this position who are very active, some who have good judgment and cool heads, and some who innovate with their approach to community moderation. Andy is the rare candidate who very clearly checks all three boxes. As a user on SO for 3.5 years, a moderator pro tempore on Engineering SE for 1.7 years and an early participant in the Community Building SE beta, I strongly support this nomination. - Air Moderator on Engineering.SE My candidate score this time is an impressive 39/40. This is up six from a year ago, and up ten from my first run. The one missing point is due to missing the Refiner badge. I believe the reason for this is because of my workflow. I, generally, don't edit and answer questions at the same time. If I'm answering, I'm not in \"edit\" mode. If I'm editting, I'm usually in \"moderation\" mode. It's something I'll work on. I'm 38 out of 50 questions there, so I'll get it soon enough. Candidate questions None of the questions were that surprising. With the added benefit of a week to prepare answers prior to nominating, I am very pleased with my answers . Two answers have generated a bit of discussion though. A 10k+ user regularly has their comments flagged as \"rude or offensive\" or \"not constructive\", to the tune of 4-5 flags a day. No comment by itself is particularly offensive, but their general tone causes them to be flagged by multiple users. You've contacted them privately about this, but they believe that they aren't doing anything wrong and that people are being too sensitive. The flags keep coming in on their comments. What, if anything, do you do next? My response is: No one has an exemption from the Be Nice policy. I think the first step is to understand why nothing has already been done about the user. 4-5 a day seems like the user has moved beyond the \"nuisance\" stage. I think a temporary ban is appropriate, with another explanation as to what is expected when interacting with others. While some users are more sensitive than others, a stream of this many flags across an extended period of time doesn't lead me to believe the problem is with the community users. The point raised in the comments was that I was rushing into banning the user without commuicating first. I disagree with that, and explained that they've already been contacted privately and ignored those warnings. A ban is the next step in getting the user's attention. I was told this would be \"humiliating\" for a high rep user. Again, I disagree and believe it's not humiliating, but educating the user. The second question that generated some discussion was: You impose a temporary ban (say 1 week) on a user for what you judged as reasonable and valid reasons (the user gets notified by email of your action and the reason). The user replies to your email acknowledging the transgression, says they won't do it again and asks for the ban to be lifted. The user sounds genuine. Do you remove the ban? Do you even reply at all? Explain your reasoning. The context of this question applies to longer bans too. If it helps get the juices flowing, consider the situation of a second offence for the same behaviour, which has a default ban period of 1 month. My response: I have two answers for this question, based on the user's history. If this is a first offense, up to this point the user hasn't been pushing limits and attempting to disrupt others, and the ban isn't related to voting fraud, then I'd be willing to remove the ban. Sometimes a ban is put in place to get the user's attention . Once the situation has been resolved, the ban is no longer appropriate and should be removed. On the other hand, if the user has a history of crossing the line and looking for a reaction, or if the ban is related to vote fraud, I'd simply not reply and the user will return in a week. Stack Overflow has enough \"voting irregularity\" bans that I imagine the responses to such bans are all similar (and invalid). I see no reason to change that policy. The push back I recieved on this was that I was letting a user off the hook by unbanning them. I argued that unbanning has been done in the past. Sometimes the ban is needed simply to get the user's attention and start the conversation and explain that why they are doing is wrong. If the user abuses the trust at that point and repeats the behavior, then the longer ban is completely justified. A bit of compassion isn't a bad thing. Primary Phase There are 12 nominees, so a primary will occur. Once again, the primary phase will reduce the number of candidates in the final phase to 10. With so few being eliminated this time around, it feels a little unneeded. The primary will last for a few days and during that time users can vote candidates up or down depending whether they believe the nominee should be a moderator. I'll return in a few days... Primary Results The Primary phase has ended and the final election has begun. I ended the primary in 5th place, securing a position in the final election. I have a sizable margin between my position and sixth place as well. One other stat that I'm rather proud of: I received the fewest number of down votes of any candidate. On to the election! Election Phase The election lasts for several days and covers a weekend. We'll see how it turns out in a few days. Election Results Well, the election has concluded. I didn't secure on of the three positions for moderator. I finished in 5th place , with my elimination propelling second and third place to a victory. I was eliminated in the 10th round of the Meek STV process. Good luck to the new moderators! Post Election thoughts This election started differently than the previous two I've run in. This election was announced a week in advance and solicited community input for questions for the candidates. I think this was a good change. The element of surprise in the previous two made it much more stressful. Additionally, by having the questions available at the start of the election - instead of at the start of the primary phase - I was able to better answer the questions. Previously, the questions would be available at the start of the primary phase. With the amount of reading needed to get through one candidate's answers, let alone all of them, I imagine that many people didn't read all of the responses. The other nice thing about this lead time, is that I had time to get my answers read for when I posted my nomination. By posting the questions and answers at the same time, I was able to have my responses available the entire time. Score-wise, on the questionnaire, I did much better than my opponents. I think a big reason for this is that I have my responses posted as soon as my nomination was posted. One question this time, though, seemed to split the candidates. I mentioned it previously, but it was regarding potentially removing a temporary ban. You impose a temporary ban (say 1 week) on a user for what you judged as reasonable and valid reasons (the user gets notified by email of your action and the reason). The user replies to your email acknowledging the transgression, says they won't do it again and asks for the ban to be lifted. The user sounds genuine. Do you remove the ban? Do you even reply at all? Explain your reasoning. The context of this question applies to longer bans too. If it helps get the juices flowing, consider the situation of a second offence for the same behaviour, which has a default ban period of 1 month. I was one of two candidates that explicitly stated we'd consider removing the ban. A third user didn't state it explicitly, but did say they'd consider it. I was surprised by the harsh tone the others took, especially since there is a lot of previous discussions on Meta where the outcome is the moderators or community managers removing the ban. I was happy to see that the other candidate who said they'd consider removing the ban get elected though. I still believe that removing the ban is a valid option. Especially because their next ban would be much longer if they broke my trust. We'll see when the next election on Stack Overflow is, but with three new moderators and no resignations, I suspect it'll be a while. I'll consider running again then."},{"tags":"Technical Solutions","url":"my-experiences-releasing-a-package-to-pypi.html","title":"My experiences releasing a package to PyPI","text":"Motivation In some of my other projects , I've needed to make extensive use of the Stack Exchange API. I built a small library - StackAPI - to assist in this task and released it on Python's PyPI repository . This post is going to cover some of the technical decisions and issues I ran into while going through this process. This was my first project being released to PyPI. My goals when releasing this were: Clean up my own code so that it is usable by others Improve the documentation and host the documentation on Read the Docs Automatically release it to PyPI, if it passes basic tests Those goals sound simple. In the future, they probably will be, but for this first release it wasn't as simple as I was hoping. Project Layout Before this project, I'd written modules and libraries that were used by myself (for personal projects) or as part of a larger application (for work). In both cases, though, I had a directory structure that looked something like this: /project_root /mymodule __init__.py mymodule.py __init__.py This was close to the end goal, but lacked some files in the project_root that were needed for a proper install via pip . The important file that was missing was setup.py . I needed this file to ensure that everything would install with a simple pip install stackapi setup.py My setup.py is pretty basic and available on GitHub. There are a couple important things though. version : This is needed, but I didn't want to have to constantly remember to update this when pushing a version to PyPI. This was one of my first criteria when starting the project. I wanted to automate as much as I could, and versioning was at the top of that list. It's small, but easy to forget and keep syncronized across all the files in the project. I decided to utilize bumpversion and Fabric to manage this specific field (both here and elsewhere in the project). install_requires : StackAPI is built in the fantasitc Requests library. To ensure this was installed when StackAPI was install, it was needed in this field. tests_require : The testsuite I build utilizes the mock library. I don't want that to be installed if the developer isn't running the tests, so it is added to this field. test_suite : I wanted developers to be able to run python setup.py test to execute the test suite. To do so, I had to point to the where the tests were being executed from. The rest of setup.py seemed to be fairly standard when compared to other Python libraries. Bumpversion and Fabric As I mentioned, I wanted to automate any versioning that was required. To do so, I used the bumpversion library and wrote a small Fabric script to handle it automatically. bumpversion uses a config file , to determine what it is going to do. I configured it to automatically create a commit and a new git tag for each version. I then pointed to a couple files where the current version is listed. When bumpversion is executed, it will change the version in each of those files to the new version. It will then create a single commit to the git repository with a commit message similar to Bump version: 0.1.6 → 0.1.7 That is nice and clean. It tags the commit for me, which is useful later, when I want to push the change to PyPI. To make running bumpversion a bit easier, I utilized a Fabric routine I found online and adjusted it for my purposes. When I run fab release , all of the bumpversion 'stuff' occurs. Then I just have to push the commit (and new tag) to GitHub. Final Project Layout The final project layout I settled on was this: /stackapi /docs ... /stackapi __init__.py stackapi.py /tests .gitignore .travis.yml fabfile.py setup.cfg setup.py Read the Docs Configure the project In the final project layout, you can see there is a docs directory. One of my goals was to make this library usable and understandable by other developers. A good part of that means having decent documentation. I spent way more time than I expected cleaning the documentation in the code and creating documentation with examples. Most of that time was spent learning the sphinx documentation style and ReStructuredText, which Read the Docs utilizes. The first step in this process was installing and setting up the initial configuration for the documentation: pip install sphinx sphinx-autobuild Then, I created the docs directory and switched to it and ran: sphinx-quickstart This starts a short, interactive, wizard. Fill out the questions. At the end of this, it creates a conf.py file in the docs directory. The rest of the documentation is ReStructuredText files. To see how the documentation looks, from the docs directory, run: make html This creates a _build directory. If you open _build/html/index.html , the documentation can be browsed locally. I do not commit this directory to git, though. It is ignored in .gitignore , as a user can regenerate it at will. Configure Read the Docs Once I was satisfied with how the documentation looked, I had to configure Read the Docs to read my GitHub repository. To repeat those steps: Sign up (or log in) at Read the Docs (part of this will be associating the account to a GitHub account) Visit your dashboard and click \"Import a project\" Fill out the form, but in my case the defaults were all appropriate. Do note that URLs are case sensitive. Click \"Create\". This is the first version of your documentation. To keep the code updating as you update GitHub, log into GitHub and go to the repository's \"Settings\" page. Click \"Webhooks & Services\" Click \"Add Service\" Select \"ReadTheDocs\" and add the service At this point, each time you push a change to the repository, a new set of documents will be built. I then added the Read the Docs badge to my README.rst for a simple link to the detailed documentation. .. image :: https://readthedocs.org/projects/stackapi/badge/?version=latest :target: http://stackapi.readthedocs.org/en/latest/?badge=latest :alt: Documentation Status Force rebuild of docs Toward the very end of this project, Read the Docs had a minor hiccup and failed on building my documentation. I didn't want to force a build by making a fake commit. Instead, Read the Docs provides the information needed to force a rebuild. It requires a very simple POST to the Post Commit Hook they provide. In my case, this was as simple as running this command (provided from the Dashboard): curl -X POST https://readthedocs.org/build/stackapi PyPI Nearing the end of the journey, it was time to see what exactly PyPI required. The first step was setting up an account on both the Test and Production instances of PyPI. PiPY Test: http://testpypi.python.org/pypi?%3Aaction=register_form PyPI Live: https://pypi.python.org/pypi?%3Aaction=register_form Having one on both was important while testing. It meant that I didn't have to send broken versions to the live PyPI server, and I could adjust ReStructuredText formatting issues without requiring another release to PyPI. Each time a version is pushed to PyPI it must have a new version number. By using the test instance, I could use as many of these fake versions as needed to fix things. Hooray for test environments! Before we perform this step automatically, we need to test that the PyPI accounts work. By following portions of a \"First Time with PyPI\" tutorial, I focused by steps down to these: Create a .pypirc file in your home directory - not your project directory. This won't be required once Travis CI is set up and configured, so having the passwords in this, temporarily, wasn't an issue because I eventually deleted the file. The file looks like this: [distutils] index-servers = pypi pypitest [pypi] repository = https://pypi.python.org/pypi username = your_username password = your_password [pypitest] repository = https://testpypi.python.org/pypi username = your_username password = your_password Register the package on PyPI Test: python setup.py register -r pypitest Register the package on PyPI Live: python setup.py register -r pypi Upload the project to Test: python setup.py register -r pypitest Upload the project to Live, if you're ready for your first release. Remember, once a version is released to PyPI, it can't be used again (or overwritten): python setup.py register -r pypi If all of the above passed to your satisfaction, you can remove the .pypirc file and move on to configuring Travis CI. Travis The last step in this process will be using Travis CI to perform some basic tests and, if this was a new release, push the changes to PyPI. The Travis config file is available on GitHub . My goal is to support 'modern' Python with this library. I've configured Travis to test against multiple versions of Python, ranging from 2.7 to 3.5. StackAPI is installed using python setup.py -q install . Then the test suite is run. The important bits are in the deploy section. on : tags : true branch : master If there is a new git tag pushed to GitHub, and the tests pass, Travis CI will push the code to PyPI. Since bumpversion makes a new git tag with each new version, this works perfectly. This does require that my password be included in the yml file. To keep this secure, I utilized the Travis Command Line Client ( gem install travis ). In my local directory, I then ran: travis encrypt --add deploy.password This added the password to the YML file. Conclusion This was the first time I've released something to PyPI. It took a lot more set up than I expected it would take. However, now that I've gone through the process, gotten used to the ReStructuredText format that Sphinx and Read the Docs require, and set up PyPI for one project, I think it'll be fairly simple to do in the future. Most of the work is getting the other services to talk with GitHub and practicing good developer habits (documentation...). All StackAPI Links All of these links are to the various places that StackAPI lives on the internet GitHub : https://github.com/AWegnerGitHub/stackapi Read the Docs : http://stackapi.readthedocs.org/en/latest/ TravisCI : https://travis-ci.org/AWegnerGitHub/stackapi PyPI : https://pypi.python.org/pypi/StackAPI"},{"tags":"Technical Solutions","url":"how-i-set-up-openshift-travisci-and-flask.html","title":"How I built a Flask application that integrates with Travis CI and OpenShift","text":"Motivation Since I shut down Vipers early this year, I've been itching to do something web related. Web technologies aren't my best technical skill, but I like trying out new things and learning something in the process. I use Python at work. I like Python a lot. With Christmas and New Years coming up, I want to have a project during my down time. My goal is to get a Flask application built and then deployed to OpenShift . Part of this deployment is to utilize TravisCI . I'm planning on using pytest and hypothesis for my test suite. Finally, I want to use my own (sub)domain, instead of the provided rhcloud one. Of these three technologies, I've used only Flask before. The comment flagging bot I built has a dashboard built in Flask. I've never used OpenShift or TravisCI. I selected OpenShift because it has a couple features I want that Heroku doesn't. The biggest one, according to the previous link, was that OpenShift has support for MySQL and Heroku doesn't (surprisingly). I want to use TravisCI and automated testing, because one of my goals for next year at work is to introduce automated tested to our development. (I work with Engineers, not coders...that's my excuse and it's a bad excuse, so I'm going to try and fix it.) To get ready for that goal, I want to test out a system that does continuous integration/automated testing. Both OpenShift and Travis CI provide me with free services. Hypothesis and py.test provide me with a way to generate comprehensive test conditions. OpenShift set up Signing up for OpenShift was easy. Fill out the form, provide an email address - though they don't like email addresses with + signs, which is disappointing - and then click the link they email back to you. Next, the rhc OpenShift client tools are needed. This is a Ruby package. I have no experience with Ruby, so I needed to install Ruby as well. I ran into a problem almost immediately. The page for installing these tools says: OpenShift rhc can be run on any operating system with Ruby 1.8.7 or higher assuming you have the requisite user permissions to install programs. Instructions for specific operating systems are provided below. Based on that, I figured I'd install the latest version of Ruby . At the time I tested this, that was 2.2.3. Unfortunately, when I ran the command to install the rhc tools, I received an error. After a bit of Googling, I found that it doesn't like 2.2x. So, I installed 2.1.7 instead. Next, I ran: gem install rhc This installs several gems and took a few minutes to complete. Next, rhc setup This started the OpenShift setup wizard. It consisted of filling out the few prompts and letting it generate an SSH key and then connecting to my account. Remember the Namespace you select. Again, this took a few minutes. Flask setup The next step was to set up my first \"Gear\". This would be the Flask application. I'll work on the database next. First, I just want Python and Flask to function properly. Fortunately, this is very easy, as OpenShift has a Flask template. rhc app create testapp python-2.7 --from-code=https://github.com/openshift-quickstart/flask-base.git A few notes: I am utilizing Python 2.7, because that is the recommendation from the Flask team. testapp can be any alphanumeric string. This is the name that will appear in the Web Console. A specific note, _ is not alphanumeric. I'm getting the feeling that OpenShift doesn't like \"special\" characters. The --from-code parameter will download that repository and use it as the base of your application. At this point, Flask can be run locally using: python app.py The application can be pushed back to OpenShift at this point and there should be a functional page on your OpenShift domain. In your command line, from the directory of your project: git add --all git commit -m \"Adding Flask application\" git push This will take a moment. At the end, you should see these lines in your command prompt: remote : Git Post - Receive Result : success remote : Activation status : success remote : Deployment completed with status : success If all three are a success, then you should be able to visit your URL. Your URL is a combination of your selected Namespace and the application name you created. http://<namespace>-<testapp>.rhcloud.com/ This should show a \"Welcome to Flask on OpenShift\" page. If you append /test to your URL, you'll get a message that says \"It's Alive!\" If it doesn't, you can check your error logs by running: rhc tail -a <testapp> MySQL setup Next, I set up my application to utilize MySQL. It's the database I have the most experience with, so I decided to keep that aspect of this project simple for myself. The first step was to add a MySQL 5.5 cartridge to my test gear (OpenShift terminology). I did this in the OpenShift web console. The UI provided me with the option to install various databases and one of those was MySQL. Clicking the link caused a few second delay as it was set up, and then I was presented with login credentials to my database. Step one...done. The next step is installing the correct Python modules to utilize MySQL. I selected PyMySQL (again, experience) and SQLAlchemy . I added these to both requirements.txt and setup.py . The idea behind doing it in both places is to make life easy for myself in the future. Additionally, the quick tutorials I've looked at for TravisCI encourage the usage of requirements.txt , while it seems OpenShift uses the setup.py . I'll fix that eventually, but getting it set up initially, this will be fastest. Add these to requirements.txt sqlalchemy==1.0.9 pymysql==0.6.7 Flask-SQLAlchemy==2.1 Add this to the install_requires list in setup.py 'sqlalchemy==1.0.9','pymysql==0.6.7','Flask-SQLAlchemy==2.1' The nice thing about OpenShift is that the credentials to the database are placed in environment variables , so I don't need to embed the passwords, connections strings, or anything potentially sensitive in my code. For MySQL these are available as: Variable Name | Purpose ------------------------------------------------ OPENSHIFT_MYSQL_DB_HOST | The host name or IP address used to connect to the database. OPENSHIFT_MYSQL_DB_PORT | The port the database server is listening on. OPENSHIFT_MYSQL_DB_USERNAME | The database administrative user name. OPENSHIFT_MYSQL_DB_PASSWORD | The database administrative user's password. OPENSHIFT_MYSQL_DB_SOCKET | An AF socket for connecting to the database (for non-scaled apps only). OPENSHIFT_MYSQL_DB_URL | Database connection URL. Utilizing the database Setting up SQLAlchemy and MySQL is fairly easy. I tested this with a simple table and ensured that it appeared in the database as expected. class User ( db . Model ) : __tablename__ = 'users' id = db . Column ( 'user_id' , db . Integer , primary_key = True ) name = db . Column ( db . String ( 60 )) A few adjustments were made to the import statements of the Flask application: from flask_sqlalchemy import SQLAlchemy A couple variables were created and loaded: app.config.from_pyfile('flaskapp.cfg') db = SQLAlchemy(app) Finally, the flaskapp.cfg file was modified to include these two lines: SQLALCHEMY_DATABASE_URI = os.environ['OPENSHIFT_MYSQL_DB_URL'] + os.environ['OPENSHIFT_APP_NAME] SQLALCHEMY_ECHO = False Remote MySQL Access I like to use MySQL Workbench while building and testing to watch what's happening in the database. To use that with OpenShift, I had to jump through a few small hoops. Open MySQL Workbench and create a new connection Give the connection a name In \"Connection Method\", select \"Standard TCP/IP over SSH\" The SSH Hostname is the full name of your OpenShift gear where MySQL is installed. It should look like namespace-appname.rhcloud.com The SSH Username is the gear's Unique Identifier. This can be found by looking at the OPENSHIFT_GEAR_UUID environment variable. It can also be found in the web console, but looking at the \"remote access\" section. It shows a connection string. You need the username portion. This is the part that appears before the @ in the ssh longuniquestring@namespace-appname.rhcloud.com command. Set the SSH key file. On Windows this is in \\Users\\<username>\\.ssh\\id_rsa by default Set MySQL Hostname equal to the value in OPENSHIFT_MYSQL_DB_HOST Set Username equal to the value in OPENSHIFT_MYSQL_DB_USERNAME (this was also provided to you when you installed MySQL) See Password equal to the value in OPENSHIFT_MYSQL_DB_PASSWORD (again, this was provided to you when MySQL was installed) TravisCI setup I want to play with automated testing. The idea behind this is to get a jump start on a goal for next year at work and to learn something new. I'd like to utilize Travis CI to perform the tests and if they pass, deploy to OpenShift. If the tests fail, I don't want to push a broken build to OpenShift. That's the goal...we'll see how it turns out. But, the first step is getting Travis CI and OpenShift talking to one another. Travis CI integrates with GitHub, so what I'm going to do in reality is push to GitHub and let Travis CI pick up the changes. From there, it will perform it's tests. If the tests pass, it will push the commit to OpenShift. On GitHub, create a new repository for your source. This is where you will be pushing your code for Travis CI to pick up. From your OpenShift directory (which is already a git repository): git remote rename origin openshift git remote add origin https://github.com/<USER>/<repositoryname>.git git push -u origin master This resets the origin to point to your new GitHub repository and sets up a new remote. Then it pushes the changes to GitHub. Log into your Travis CI profile page . Make sure you are logged into GitHub first, as this will create the profile automatically. Press the \"Sync now\" button at the top of the page to pull a list of all of your repositories. Once that is done, find the repository you just set up, and enable integration with that repository. Next you need to set up Travis CI and the .travis.yml file. gem install travis This will install a Ruby script that assists in this process. If you get an error when running this, you need to create an empty .travis.yml file first and then run the command again. travis setup openshift Fill out the prompts. Defaults should be fine in most cases, but do pay attention to \"OpenShift application name\". If your GitHub repository is named differently than your OpenShift application name, the default for this prompt will be incorrect. One last note, for a quick test you can change the script section to script : true This forces the tests to pass. Once you've written tests, you can do something like: script: - py.test This will run your test scripts, utilizing py.test Deploy these changes to GitHub: git add .travis.yml git commit -m \"Deploying Travis\" git push origin master This will commit and push the changes to GitHub. A few seconds later, if you are watching Travis CI, you'll see it notices the new commit and starts running tests. If you tests complete with a status code of 0 (successful), it will deploy the changes to OpenShift. If the tests fail (any other status code), it will not deploy to OpenShift. py.test setup Setting up the testing frame work involves a few Python modules. These need to be added to both requirements.txt and setup.py . pytest>=2.8.0 hypothesis==1.16.0 pytest-runner==2.6.2 The next step is setting up some quick integration with setup.py , so that users can run python setup.py test and execute your tests. In setup.py , add (or edit) the setup_requires list to include pytest-runner . Add (or edit) the tests_require list to include pytest . I also added the following to my setup.cfg [aliases] test = pytest I modified my setup_requires list a bit, so that it's conditional. Since this would install the pytest-runner on every call to setup.py , even when the module wouldn't be called, I wanted the runner to only be required when pytest is utilized. import sys needs_pytest = { 'pytest' , 'test' , 'ptr' } . intersection ( sys . argv ) pytest_runner = [ 'pytest-runner' ] if needs_pytest else [] setup ( setup_requires = [ #... Other requirements here ] + pytest_runner , ) I wanted to test that my tests were working correctly. I created a tests directory, which is where I plan on storing all of my test cases. pytest will find any files that start or end with test and execute them. I created a very simple test_tests.py file with the following simple test (taken from the Hypothesis Quickstart ) @given(st.integers(), st.integers()) def test_ints_are_commutative(x, y): assert x + y == y + x Finally, Travis CI needs to be told what to do. Modify the script key to include py.test script: - py.test After a successful run through Travis, you'll see something like this: tests/test_tests.py . =========================== 1 passed in 0.26 seconds =========================== The command \"py.test\" exited with 0. Custom Domain I have my own domain name. I want to utilize OpenShift with one of those domains, instead of the default one provided. Since I've using the free tier, that will rule out using the SSL certificate that is wildcarded to the whole rhcloud.com domain. I can live with this. If I need SSL on my domain, I'll upgrade. To set up OpenShift to use your domain, log into the web console. Go to the gear you are configuring. At the top, where the full domain is displayed, is the option to \"Change\". Select that option. Input the full domain (and subdomain) you want to utilize and click \"Save\". After a few seconds, you'll get a notification that the alias was created. The next step is to configure the DNS records. I utilize CloudFlare for my domains, so the instructs will be specific to that, but should apply to any DNS system. Login to your management system and go to the area where you can specify DNS records. For my test, I set up a subdomain of one of my domains as the alias I wanted to use. In your DNS system, set up a CNAME that points to the original hostname on OpenShift. The CNAME should be the subdomain you told OpenShift about. Save the record. CloudFlare recognized this immediately and redirected me to my Flask application. Hooray! Conclusion With this, the set up is complete. You have a Flask application, connected to MySQL, that is integrated with a CI system which automatically deploys to OpenShift when all tests pass and uses CloudFlare (because I already was doing so), to provide a CDN. On to building something!"},{"tags":"Side Activities","url":"i'm-running-for-moderator-on-stack-overflow-again.html","title":"I'm running for moderator on Stack Overflow again","text":"Introduction In April, I ran for moderator on Stack Overflow and didn't make it through the primaries. That's ok though, there were several very good users that did get elected . In a surprise announcement, though, Stack Overflow is running a second election this year. This is the first time this has happened since 2011. I'm still interested in a position and I'm still active in the community, so I'm going to run again. This post will follow the process. Nomination Phase Like last time, the nomination phase began with users throwing their hat into the ring. Nominations were slower and fewer this time. Only 19 nominees, so no one was eliminated due to low reputation. Several users from the last election are rerunning too. My Platform My platform hasn't changed much since the previous run. Below is my nomination post. This time, I tried to pull emphasis off the automated script by putting it lower on the list of things I've done and instead focused on the moderation tasks I do on Stack Overflow and the work I've done on Community Building . We'll see if it works. Hi Everyone, I'm Andy and I'd make a great moderator for Stack Overflow. Why vote for me? I'm active in the review queues (currently holding 10th in Low Quality Post reviewers of all time), provide edits to posts, answers and enjoy the moderation aspect of Stack Exchange I've been a moderator on CommunityBuilding for nearly a year and a half. I know the moderator tools and have worked with several of the current moderators. This interaction will continue as a new moderator here. I've built an automated script that continues to handle noisy comments very accurately . I have a history on Meta.SO that shows I'm involved in the meta aspect of the site as well. I have a history of good community moderation already. I enjoy the moderation aspect on Stack Overflow (and Stack Exchange in general). I deal with users with respect, even if our opinions on an issue differ. With this, I received my \"candidate score\". It was 33/40. Not the highest, but better than last time. The score wasn't mentioned in April. I am not expecting it to be an issue this time either. Primary Phase Updated November 21, 2015 The primary phase is in the third day. In day 1, I was hovering around 9th/10th place. Overnight, between days 1 and 2 though, I dropped down to 11th. I've been sitting here consistantly for a full day now and, while still gaining votes, I'm not gaining as fast as 10th position. It appears I may not make the cutoff by Friday's deadline. While disappointing, there are a few things that I came away with that I'm very happy about. In the last primary, I revieved 1,492 positive votes. I've surpassed that already. I've over 2,100 currently. I'm pleased with that upswing. I was also more prepared for the questionnaire portion of the primaries this time. I've gotten the second highest number of upvotes on my responses . Several of the questions were similar to last time, but there are a few that I think should be included in the future elections. Questionnaire This first question is a great post for candidates. It allows them to show off their involvement in Meta and show their best work. For users, it gives them a sense of how a candidate interacts with the community. I am very surprised that several candidates list only one or two posts. This seems to be doing a disservice to themselves. Do you have any Meta posts that you're particularly proud of, or that you feel best demonstrate your moderation style? My response to this question: I'm proud of several of my posts both here on Meta.SO and on other network sites I participate in. Here on MSO, I have two questions that I am proud of: Can a machine be taught to flag comments automatically? I estimate 10% of the links posted here are dead. How do we deal with them? In both of these, you can see that I care about quality on Stack Overflow. I've spent time analyzing the problem, as I see it, and present my findings to the community. I participated in the discussions that both posts generated and continue to run the bot to this day. Elsewhere on the network, my participation in meta has helped to shape communities. For example, on Hardware Recommendations , my meta post about \"What type of hardware is allowed\" helped to set the scope of what the community accepts as on topic hardware. I've also helped to set up the high quality guidelines for questions and argued against certain types of tags and hardware . With all of these, I've presented my arguments and logic and strived to remain professional. I believe the community on HardwareRecs has seen that as well. As a moderator on Community Building , I've been involved in many discussions . I was involved in the discussions to rename the community from Moderators.SE to CommunityBuilding.SE. I've been involved in discussions about slow growth of the community . I've also presented arguments that go against other moderators, and walked away still feeling like a moderation team. (Go communication!) Finally, on OpenSource, I made a post about how moderators had implemented a policy to watch the reviewers. It was similar to the long removed \"flag weight\" option that used to exist. I believe the post was presented in a way that questioned the decisions of the moderators, yet remained professional. With all of these meta posts, across the network, I think you can pick up on my moderation style and personality. I like data and I try to present my thoughts in a way that is understandable to all. I'm also willing to speak my mind. This second question I struggled with for a bit. I've had ideas on how Stack Overflow/Stack Exchange could improve, but what did I want to present in this response. If you could add/revise one Stack Overflow policy/guideline, what would you change? Why would you change it, and what would it mean for the community? My response to this question: At the risk of talking myself out of a position, I think more community moderation would help the problem that Stack Overflow has with scaling moderators. There are a couple areas that I think would work well in opening this to the higher reputation users Comment flagging: Comments can be removed if enough users flag a comment. If not, a moderator needs to handle the flag. Instead, opening this as a review queue can remove a lot of this burden from the moderators. Users could handle all but the \"Other...\" flag. There may be guidance needed on the \"Obsolete\" one due to the difference between \"obsolete comment\" and \"obsolete code block\" differences. Audit Review reviews: On Stack Overflow, we get a decent number of disputed audit review posts on meta. There may be a way to get users with a history of passing both audits and good reviews involved in dealing with these disputed audits. The idea would be to say whether an audit is good or not. These changes, and other areas where the community could be leveraged for moderation tasks, helps to remove the burden on moderators. Handling 2,000 (and growing) flags a day means that something needs to change. Moderators are exception handlers. They should be handling the cases that are exceptional - not comments that are no longer relevant. For the community, this would be more involvement with the moderation aspect. Users would be able to more quickly clean up a comment thread. Flag it and it appears in the review queue. From here, the moderators don't need to be involved. The downside of this is that it adds another queue for users to be involved with. Primary Results With the primaries over, I ended with 2483 positive votes. This put me in 11th place. Sadly, this was not enough to get into the election. I was 185 votes shy of over taking 10th. Good luck to the candidates that made it. One of the tools that came out of this election was a way to visualize various data points to compare candidates. I provided a couple notes about outliers various candidates show regarding aspects on the site. I found it interesting to see what each user had \"specialized\" in. Election Updated December 8, 2015 The election is over and the new moderators have settled in. We've had our first bout of public drama over one of these moderators actually moderating a chat room too. gasp Final thoughts I was closer to the top 10 this time, but still missed it. Even more surprising was that the user that ended up in 3rd in the primaries didn't even come close to getting elected. He was eliminated in the 5th round of final STV votes. I still think I'd make a great moderator for Stack Overflow, but I need to figure out the best way to promote myself in the next election."},{"tags":"Jobs","url":"how-not-to-recruit-me.html","title":"How not to recruit me","text":"Introduction A few days ago, on Meta.StackOverflow, a product manager asked for feedback on how they could make first contact between employers and potential employees more valuable to the potential employee. I provided my feedback . I've reposted it below, with some minor changes to further explain my ideals. The summary of this entire post is: If I have to spend time to make my resume jump out and catch your attention, I expect you to spend just a little bit of time telling me about your company and the position you are recruiting for. I don't think a few sentences with these details is that much ask. What I want to see I want to know what the company does and I want to know who you are. I also want to know what positions you are looking to fill and a few details about the position are also important. Messages I've recieved Message 1 This is the start of a good message: It has the following good elements: Who the recruiter is Position they are recruiting for A company name It does not offer the following: What the company does (other than \"build great software\"). Explanation which of my skills or experiences they are interested in. Are you interested in my Python answers on Stack Overflow? My projects on Github? My PHP experience from years ago? This is important to know. If the company is looking to recruit me for knowledge I shared years ago (ie. PHP in my case), I'd probably be less useful right now because I haven't used PHP in years. I have not kept up with recent changes to the language or various frameworks. Explanation of what the position will do, projects I'll be involved in, or challenges they are facing now that I'll help solve. I did not respond to this position. Message 2 This has the following useful elements: I know your name, position and company. Due to who the company was in this particular message, I am even vaguely aware of what the company does. Name recognition helps, but is not very high on my list of \"important things\" What this does not have: A description of the position. \"Looking for top talent...to make a significant impact on our systems\". Ok. How? What will I be doing? How did you find me? I actually talked to this recruiter and I have to say that I'm both disappointed in the company and myself for not noticing some red flags in this message. This wasn't an interview for a specific position. This was a general recruiting email and I suspect it was sent to multiple users. Flags that I missed in the message: Lack of any details at all and use of buzzwords (\"top technical talent\", \"fast paced\", \"start-up like\", \"eCommerce industry\"). Discussion about personal career goals and technical background. Some of my back ground should be obvious from my Careers profile, SO profile and links on both. More importantly, I missed the \"personal career goals\" flag. The details they wanted were to see if my goals aligned with any open positions they had at the time. When the discussion took place, the recruiter knew little more than my name. There were no questions about my careers profile (at the very least) or other projects I mentioned on my profile. Shame on me. Message 3 This message isn't useful to me at all. Who is this company and what do they do? I have no experience in mobile apps. How did they find me? What is Chad's relationship to the company? I suspect this message showed up because I work for one of their largest clients, live 10 miles from their office and work, literally, right across the street from them. I didn't know that at the time though. I had to ask a co-worker if they'd ever heard of the company. They pointed out their office at lunch. I did not respond this this message. Message 4 This message is worthless. You are seeking employees who are willing to relocate. Great. Good for you. Are you interested in my skills or just a warm body to keep your chairs warm? No details about the company, other than a link to a web site No name at all. Who am I talking to? No details about the job. I hope they found someone to hold their seats to the floor 8 hours a day. If not, I recommend large rocks. Summary of the messages I need details and I need details more than what the salary is going to be. Obviously, I need to do some research about your company, if I've never heard of you. But, throw me a bone. Tell me a bit about your self. I had to make a fancy resume to get you to reach out to me. I did something that caught your eye. Now, do something that will catch mine. I don't need to work at a company that is the next Google or Apple or start up flush with cash. But, I do want to know about the company before I talk to you. If you can't spend a few seconds explaining a bit about the company, I don't want to talk with you. I don't think a few sentences is that much to offer: \"Here at COMPANY NAME, we are looking for a POSITION with skills in LANGUAGE or experience in INDUSTRY. I see you have both and think you'd be able to help TEAM NAME with their on going project of MAKING THE WORLD BETTER. Your PROJECT ON GITHUB looks like you've dealt with aspects of this problem before. We've gotten some press recently about our innovations in this area (check them out HERE).\" How to get my attention I think the post above makes it clear what doesn't work. The companies that I've responded to (other than message 2, above) have all done the following: Indicate they've at least looked at my resume by commenting on some aspect of it. I've had recruiters mention projects I've done, jobs I've held, or individual bullet points that caught their attention. This is great. It means you aren't reaching out to me because I happened to hit all of your keywords when the resume passed through your Human Resources department. Provide a short description of the job they want to fill. I don't need a full job posting. A link to such a posting is sufficient. However, if you could summarize it in a sentence or two, that'd make both of our lives easier. Reading full job descriptions isn't the most fun thing in the world and often have industry (or even company) specific acronyms. That isn't helpful to either of us, because I may not know them. Just tell me what your team does within the company. That's good enough to get my attention. Talk to me like a person, not some number that a database search returned. As a hint, if you can figure out that I like to go by \"Andy\", instead of \"Andrew\", you've already done a far better job than most recruiters I've spoken with."},{"tags":"Side Activities","url":"link-analysis---technical-explanation.html","title":"Link Analysis - Technical Explanation","text":"Introduction In my last two posts, I've discussed the number of rotten links on Stack Overflow and a proposal to fix said links . In this post, I'm going to discuss how I performed this analysis. Set up The database The process began by downloading the March 2013 data dump . I loaded the posts into a [MariaDB] instance on my local machine. This was accomplished with a very simple script and patience, as the script took a while to run. load xml local infile '/path/to/posts.xml' into table posts rows identified by '<row>'; The data Once this was done, the next step was selecting my random sample of data. I did this by randomly selecting 25% of the days in a year and then pulling all posts for those days across all years of Stack Overflow's existence. The Python script I used to do this was fairly simple: from datetime import timedelta , datetime from random import randint from math import ceil def random_date ( start , end ): return start + timedelta ( seconds = randint ( 0 , int (( end - start ) . total_seconds ()))) percentage = 0.25 days = 366 dayslist = [] for d in xrange ( int ( ceil ( days * percentage ))): dayslist . append ( random_date ( datetime ( 2008 , 1 , 1 ), datetime ( 2008 , 12 , 31 ))) At the end of this run, the days that I cared about are in the dayslist variable. I used that to pull questions and answers from the database that were created on that month/day combination. In the end, this resulted in just over 25% of the total posts being selected. To ensure that I could replicate the results, I also saved the dates that were selected. Parsing the data The next step was to parse out links from the data. I used the following script to extract anchor text and links from a post. def links_in_post(post): \"\"\" Returns a list of all links found :param posts: A list of dictionaries with a 'body' key containing HTML strings [ { 'body': \" <b> This is HTML </b> \" }, ] :return: A list of tuples containing anchor text and URL [ ('Display Text', 'http://example.com') ] \"\"\" logging.debug(\"Extracting links...\") links = [] images = [] regexp = \" &.+?; \" list_of_html = re.findall(regexp, post) for e in list_of_html: if e in invalid_entities: h = HTMLParser.HTMLParser() unescaped = h.unescape(e) post = post.replace(e, unescaped) doc = html.fromstring(post) for link in doc.xpath('//a'): links.append(Link(text=link.text_content(), link=link.get('href'))) for image in doc.xpath('//img'): images.append(Link(text=image.get('alt'), link=image.get('src'))) all_items = links + images seen = set() unique_items = [item for item in all_items if item[1] not in seen and not seen.add(item[1])] return unique_items The regular expression being utilized, is to strip out HTML entities. This was needed due to weird parsing issues with non-ASCII characters. Fortunately, I wasn't the first to encounter oddities like this . The list comprehension at the end of the function is returning only unique tuples of anchor text/link. I was surprised how often I'd end up with tuples such as ('this', 'http://google.com') in the same post. This uniqueness saved a lot of processing time later. After all links in a post had been extracted, this information and information about the post itself, was saved to the database. If a post had no links, it was not saved. The database consisted of three tables. Links - This table contains the base URLs seen in all posts. URLs are distinct. It also contains an ID that will be utilized for linking to the other tables. Post Links - This table contains information about links in a post. This includes the specific anchor text/link combinations Link Results - This table contains the results of link status checks Processing the posts was fairly time consuming, but was able to be parallelized easily. That significantly cut down on processing time. Checking the links The most time consuming portion of this entire project was actually checking link status. Each link that appeared in the Links table was checked. As I mentioned in my first post, the original idea was to simply send a HEAD request to each URL. The idea was to save myself and the end point a tiny amount of bandwidth. I had over a million links to process. I figured a little saved bandwidth wouldn't hurt. I turns out this isn't a good idea. When I started seeing larger sites as not being accessible, I go suspicious that something was wrong. These sites were returning status 405 errors. This indicates that the method is not allowed. So, I switched to GET for every link. The next problem I ran into was that many sites didn't like the default user agent of the spider. They rejected requests with 404 and 401 errors. In the end, I got around this by mimicking Firefox on every request. With those kinks worked out, every link was sent a GET request that looked to be from a Firefox browser. The process would allow 20 seconds per link. If the link didn't respond in that time limit, it was declared inaccessible. A week later, I repeated the process with anything that hadn't returned a status code less than 400. Once more, on the third week, I repeated this with the failed links. At the end of three weeks, I had a list of sites that were inaccessible to me - on a residential connection - three times over a period of three weeks. Results The SVG image that I created for the write up was generated with Pygal. The tables were the result of various break downs of the data via queries to the status results. Wrap up I am rather proud of how the results turned out for this project. I went into it expecting about 15% of links to be broken, but I didn't really realize what the meant. Fifteen percent of 21 million total posts is over 3 million. That's a large number. BUT, it also ignored that a large percentage of posts don't contain links. I failed to consider that in my original hypothesis. Less than half of my sample had links (2.3M out of 5.6M). Of the 2.3M with links, only 1.5M were unique links. The final result of 10% failed links makes much more sense in this context. Ten percent of 1.5M links means that there are 150K links that are bad."},{"tags":"Side Activities","url":"a-proposal-to-fix-broken-links-on-stack-overflow.html","title":"A proposal to fix broken links on Stack Overflow","text":"This post was published by me on Meta Stack Overflow on August 7th, 2015. I've republished it here so that I can easily update information related to recent developments. If you have questions or comments, I highly encourage you to visit the question on Meta Stack Overflow and post there. This is a follow up to yesterday's post about how many links on Stack Overflow are starting to rot. The proposal I propose another hybrid of the previous broken link queue (as was mentioned above in comments and other answers ) and an automated process to fix broken links with an archived version (which has also been suggested ). The broken link queue should focus on editing and fixing the links in a post (as opposed to closing it). It'd be similar to the suggested edits queue, but with the focus intended to correct links not spelling and grammar. This could be done by only allowing a user to edit the links. One possibility, I envision is presenting the user with the links in the post and a status on whether or not the link is available. If it's not available, give the user a way to change that specific link. Utilizing this post, I have a quick mock up of what I propose such a review task looks like: The Queue All the links that appear in the post are on the right hand side of the screen. The links that are accessible have a green check mark. The ones that are broken (and the reason for being in this queue) have a red X. When a user elects to fix a post, they are presented with a modal showing only the broken URLs. The Automation With this queue, though, I think an automated process would be helpful as well. The idea is that this would operate similarly to the Low Quality queue, where the system can automatically add a post to the queue if certain criteria are met or a user can flag a post as having broken links. I've based my idea on what Tim Post outlined in the comments to a previous post . Automated process performs a \"Today in History\" type check. This keeps the fixes limited to a small subset of posts per day. It also focuses on older posts, which were more likely to have a broken link than something posted recently. Example: On July 31, 2015, the only posts being checked for bad links would be anything posted on July 31 in any year 2008 through current year - 1. Utilizing the Wayback Machine API , or similar service, the system attempts to change broken links into an archived version of the URL. This archived version should probably be from \"close\" to the time the post was originally made. If the automated process isn't able to find an archived version of the link, the post should be tossed into the Broken Link queue When the Community edits a post to fix a link, a new Post History event is utilized to show that a link was changed. This would allow anyone looking at revision history to easily see that a specific change was only to fix links. Actions performed in the previous bullets are exposed to 10K users in the moderator tools. Much like recent close/delete posts show up, these do as well. This allows higher rep users to spot check (if they so desire). I think this portion is important when the automated process fixes a link. For community edits in the queue, the history tab in /review seems sufficient. If a post consists of a large percentage of a link (or links) and these links were changed by Community, the post should have further action taken on it in some queue. Example: - A post where X+% of the text is hyperlinks is very dependant on the links being active. If one or more of the links are broken, the post may no longer be relevant (or may be a link only post). One example I found while doing this was this answer. I don't think that this type of edit from the Community user should bump a post to the front page. Edits done in the broken link queue, though, should bump the post just like a suggested edit does today. By preventing the automated Community posts from being bumped, we prevent the the front page from being flooded, daily, with old posts and these edits. I think that the exposure in the 10K tools and the broken link queue will provide the visibility needed to check the process is working correctly. Process flows Queue Flow: Automated process flow: Potential pitfalls The automated link checking will likely run into several of the problems I did. Mainly: Sites modify the HEAD request to send a 404 instead of a 405. My solution to this was to issue GET requests for everything. Sites don't like certain user agents. My solution to this was to mimic the Firefox user agent. To be a good internet citizen, Stack Exchange probably shouldn't go that far, but providing a unique user agent that is easily identifiable as \"StackExchangeBot\" (think \"GoogleBot\"), should be helpful in identifying where traffic is coming from. Sites that are down one week and up another. I solved this by spreading my tests over a period of 3 weeks. With the queue and automatic linking to an archived version of the site, this may not be necessary. However, immediately converting a link to an archived copy should be discussed by the community. Do we convert the broken link immediately? Or do we try again in X days. If it's still down then convert it? It was suggested in another answer that we first offer the poster the chance to make changes before an automatic process takes place. The need to throttle requests so that you don't flood a site with requests. I solved this by only querying unique URLs. This still issues a lot of requests to certain, popular, domains. This could be solved by staggering the checks over a period of minutes/hours versus spewing 100s - 1000s of GET requests at midnight daily. With the broken link queue, I feel the first two would be acceptable. Much like posts in the Low Quality queue appear because of a heuristic, despite not being low quality, links will be the same way. The system will flag them as broken and the queue will determine if that is true (if an archived version of the site can't be found by the automated process). The bullet about throttling requests is an implementation detail that I'm sure the developers would be able to figure out."},{"tags":"Side Activities","url":"analysis-of-links-posted-to-stack-overflow.html","title":"Analysis of links posted to Stack Overflow","text":"This post was published by me on Meta Stack Overflow on August 6th, 2015. I've republished it here so that I can easily update information related to recent developments. If you have questions or comments, I highly encourage you to visit the question on Meta Stack Overflow and post there. TL;DR: Approximately 10% of 1.5M randomly selected unique links in the March 2015 data dump are unavailable. To be more precise, that is approximately 150K dead links. Motivation I've been running into more and more links that are dead on Stack Overflow and it's bothering me. In some cases, I've spent the time hunting down a replacement, in others I've notified the owner of the post that a link is dead, and (more shamefully), in others I've simply ignored it and left just a down vote . Obviously that's not good. Before making sweeping generalizations that there are dead links everywhere, though, I wanted to make sure I wasn't just finding bad posts because I was wandering through the review queues. Utilizing the March 2015 data dump, I randomly selected about 25% of the posts (both questions and answers) and then parsed out the links. This works out to 5.6M posts out of 21.7M total. Of these 5.6M posts, 2.3M contained links and 1.5M of these were unique links. I sent each unique URL a HEAD request, with a user agent mimicking Firefox 1 . I then retested everything that didn't return a successful response a week later. Finally, anything that failed from that batch, I resent a final test a week later. If a site was down in all three tests, I considered it down for this test. Results 2 By status code Good news/Bad News: A majority of the links returned a valid response, but there are still roughly 10% that failed. (This image is showing the top status codes returned) The three largest slices of the pie are the status 200s (site working!), status 404 (page not found, but server responded saying the page isn't found) and Connection Errors. Connection errors are sites that had no proper server response. The request to access the page timed out. I was generous in the time out and allowed a request to live for 20 seconds before failing a link with this status. The 4xx and 5xx errors are status codes that fall in the 400 and 500 range of HTTP responses. These are client and server error ranges, thus counted as a failure. 2xx errors (of which was are in the low triple) are pages that responded with a success message in the 200 range, but it wasn't a 200 code. Finally, there were just over a hundred sites that hit a redirect loop that didn't seem to end. These are the 3xx errors. I failed a site with this range if it redirected more than 30 times. There are a negligible number of sites that returned status codes in the 600 and 700 range 4 By most common There are, expectedly, many URLs that failed that appeared frequently in the sample set. Below is a list of the top 50 3 URLs that are in posts most often, but failed three times over the course of three weeks. http://docs.jquery.com/Plugins/validation http://www.eclipse.org/eclipselink/moxy.php http://jackson.codehaus.org/ http://xstream.codehaus.org/ http://opencv.willowgarage.com/wiki/ http://developer.android.com/resources/articles/painless-threading.html http://valums.com/ajax-upload/ http://sqlite.phxsoftware.com/ http://qt.nokia.com/ http://www.oracle.com/technetwork/java/codeconv-138413.html http://download.java.net/jdk8/docs/api/java/time/package-summary.html http://docs.oracle.com/javase/1.4.2/docs/api/java/text/SimpleDateFormat.html http://watin.sourceforge.net/ http://leandrovieira.com/projects/jquery/lightbox/ https://graph.facebook.com/ https://ccrma.stanford.edu/courses/422/projects/WaveFormat/ http://www.postsharp.org/ http://www.erichynds.com/jquery/jquery-ui-multiselect-widget/ http://ha.ckers.org/xss.html http://jetty.codehaus.org/jetty/ http://cpp-next.com/archive/2009/08/want-speed-pass-by-value/ http://codespeak.net/lxml/ http://www.hpl.hp.com/personal/Hans_Boehm/gc/ http://jquery.com/demo/thickbox/ http://book.git-scm.com/5_submodules.html http://monotouch.net/ http://developer.android.com/resources/articles/timed-ui-updates.html http://jquery.bassistance.de/validate/demo/ http://codeigniter.com/user_guide/database/active_record.html http://www.phantomjs.org/ http://watin.org/ http://www.db4o.com/ http://qt.nokia.com/products/ http://referencesource.microsoft.com/netframework.aspx https://github.com/facebook/php-sdk/ http://java.decompiler.free.fr/ http://pivotal.github.com/jasmine/ http://api.jquery.com/category/plugins/templates/ http://code.google.com/closure/library http://www.w3schools.com/tags/ref_entities.asp http://xstream.codehaus.org/tutorial.html https://github.com/facebook/php-sdk http://download.java.net/maven/1/jstl/jars/jstl-1.2.jar https://developers.facebook.com/docs/offline-access-deprecation/ http://www.parashift.com/c++-faq-lite/pointers-to-members.html https://developers.facebook.com/docs/mobile/ios/build/ http://downloads.php.net/pierre/ http://fluentnhibernate.org/ http://net.tutsplus.com/tutorials/javascript-ajax/5-ways-to-make-ajax-calls-with-jquery/ http://dev.iceburg.net/jquery/jqModal/ By post score Count of posts by score (top 10) (Covers 94% of all broken links): | Score | Percentage of Total Broken | |-------|----------------------------| | 0 | 36.4087% | | 1 | 25.1674% | | 2 | 13.4089% | | 3 | 7.2806% | | 4 | 4.2971% | | 5 | 2.7065% | | 6 | 1.8068% | | 7 | 1.2854% | | -1 | 1.1935% | | 8 | 0.9415% | By number of views Note, this is number of views at the time the data dump was created, not as of today Count of posts by number of views (top 10): | Views | Total Views | |--------------|-------------| | (0, 200] | 24.4709% | | (200, 400] | 14.2186% | | (400, 600] | 9.5045% | | (600, 800] | 6.9793% | | (800, 1000] | 5.2574% | | (1000, 1200] | 4.1864% | | (1200, 1400] | 3.3699% | | (1400, 1600] | 2.7766% | | (1600, 1800] | 2.3477% | | (1800, 2000] | 1.9550% | By days since post created Note: This is number of days since creation at the time the data dump was created, not from today Count of posts by days since creation (top 10) (Covers 64% of broken links): | Days since Creation | Percentage of Total Broken | |---------------------|----------------------------| | (1110, 1140] | 7.2938% | | (1140, 1170] | 6.7648% | | (1470, 1500] | 6.6579% | | (1080, 1110] | 6.6535% | | (750, 780] | 6.5535% | | (720, 750] | 6.5516% | | (1500, 1530] | 6.3978% | | (390, 420] | 5.8508% | | (360, 390] | 5.8258% | | (780, 810] | 5.5175% | By Ratio of Views:Days Ratio Views:Days (top 20) (Covers 90% of broken links): | Views:Days Ratio | Percentage of Total Broken | |------------------|-------------| | (0, 0.25] | 27.2369% | | (0.25, 0.5] | 18.8496% | | (0.5, 0.75] | 11.4321% | | (0.75, 1] | 7.2481% | | (1, 1.25] | 5.1668% | | (1.25, 1.5] | 3.7907% | | (1.5, 1.75] | 2.9310% | | (1.75, 2] | 2.4033% | | (2, 2.25] | 1.9788% | | (2.25, 2.5] | 1.6850% | | (2.5, 2.75] | 1.4080% | | (2.75, 3] | 1.1879% | | (3, 3.25] | 1.0654% | | (3.25, 3.5] | 0.9391% | | (3.5, 3.75] | 0.8334% | | (3.75, 4] | 0.7165% | | (4, 4.25] | 0.6634% | | (4.25, 4.5] | 0.5789% | | (4.5, 4.75] | 0.5508% | | (4.75, 5] | 0.4833% | Discussion What can we do with all of this? How do we, as a community, solve the issue of 10% of our outbound links pointing to places on the internet that no longer exist? Assuming that my sample was indicative of the entire data dump, there are close to 600K (150K broken unique links x 4, because I took 1/4 of the data dump as a sample) broken links posted in questions and answers on Stack Overflow. I assume a large number of links posted in comments would be broken as well, but that's an activity for another month. We encourage posters to provide snippets from their links just in case a link dies. That definitely helps, but the resources behind the links and the (presumably) expanded explanation behind the links are still gone. How can we properly deal with this? It looks like there have been a few previous discussions: Utilize the Wayback API to automatically fix broken links. Development appeared to stall on this due to the large number of edits the Community user would be making. This would also hide posts that depended on said link from being surfaced for the community to fix it. Link review queue . It was in alpha , but disappeared in early 2014. Badge proposal for fixing broken links Footnotes This is how it ultimately played out. Originally I sent HEAD requests, in an effort to save bandwidth. This turned out to waste a whole bunch of time because there are a whole bunch of sites around the internet that return a 405 Method Not Allowed when sending a HEAD request. The next step was to sent GET requests, but utilize the default Python requests user-agent. A lot of sites were returning 401 or 404 responses to this user agent. Links to Stack Exchange sites were not counted in the above results. The failures seen are almost 100% due to a question/answer/comment being deleted. The process ran as an anonymous user, thus didn't have any reputation and was served a 404. A user with appropriate permissions can still visit the link. I verified a number of 404'd links to Stack Overflow posts and this was the case. The 4th most common failure was to localhost . The 16th and 17th most common were localhost on ports other than 80. I removed these from the result table with the knowledge that these shouldn't be accessible from the internet. There where 7 total URLs that returned status codes in the 600 and 700 range. One such site was code.org with a status code of 752. Sadly, this is not even defined the joke RFC. Follow up I posted a proposal on how I think this could be fixed."},{"tags":"Side Activities","url":"how-i-set-up-this-site-with-github-pages-and-cloudflare.html","title":"How I set up this site with GitHub Pages and CloudFlare","text":"Introduction In a previous post , I described why I moved from Wordpress to Pelican for my blog. This one goes a step further and describes how I eliminated the need for the dedicated server I'd been utilizing as a part of Team Vipers . By eliminating that server, I reduced my costs to zero but kept control over the DNS of my domain (thanks to CloudFlare ) and had an easier method of updating the site using GitHub Pages . GitHub Pages Setup To utilize GitHub Pages, I needed to create a new repository that followed the format GitHubUsername.github.io . This repository would house the content that is this site. I also set up a second repository which contains the source for the blog. This repository includes the templates, plugins and markdown version of the pages. The first repository was set up as submodule. git submodule add https://github.com/AWegnerGitHub/awegnergithub.github.io.git output I ignored the output directory in .gitignore on the source repository. Finally, I had to adjust publishconf.py slightly to DELETE_OUTPUT_DIRECTORY = False Without this, I was constantly destroying the output repository and had to reinitialize it. This prevents that from occuring. Now, a new post consists of writing up the Markdown page , generating the page with the command below (or the batch script ) and then committing and pushing the changes to the submodule to GitHub. # Generates HTML files without debugging information pelican content --output output --settings publishconf.py The new content is available immediately. Custom Domain You may notice that the URL for this site isn't awegnergithub.github.io , but instead andrewwegner.com . To accomplish this, I added a directory to the content named extra . In this directory is a single file named CNAME (no extension). In the file is my domain name. Next, I had to modify pelicanconf.py to add the extra/CNAME to the static path and then on generation move the CNAME file from this subdirectory to the root. I could have put it in the root of content by default, but Pelican provides a way to do this and it keeps content clean. One very important note , the EXTRA_PATH_METADATA is operating system sensitive. Since I am generating the content on a Windows machine, I had to use a backslash instead of the forward slash the documentation shows. I found this after posing a question on Stack Overflow on why it wasn't working as the documentation suggested. The two important fields to add or edit are: ... STATIC_PATHS = ['images', 'extra/CNAME'] ... EXTRA_PATH_METADATA = {'extra\\CNAME': {'path': 'CNAME'},} Cloudflare Setup The final thing I needed in order to get rid of my server was control over DNS. I could revert back to GoDaddy, but after a little research found that CloudFlare's additional CDN and security was a \"good thing\" (because, you know, I'm such a highly traffic'd blog these days). Step one was signing up to CloudFlare. This was a 3-5 minute thing. Once signed up and signed in, I went to set up DNS. This was as simple as adding my domain name and waiting for CloudFlare to import my existing DNS records. With this, I kept by Google Apps email intact (which is what I was most concerned with). Next, I went and removed the A records. I replaced these with CNAME records pointing to my GitHub Pages URL. I also added a www CNAME pointing to the same location. Since I have Pelican configured to strip it with the setting below, it doesn't matter other than people expect to enter www dot domain dot com in their URL bar. SITEURL = 'http://andrewwegner.com' Last, I had to point by name servers to CloudFlare instead of my dedicated server. They provide a list of registratars to choose from. Select your registrar and follow the instructions. My biggest issue here was remembering my GoDaddy password. After I made it into my account, the steps to change name servers were very simple. Once those are saved, you wait for the changes to propagate and enjoy your new GitHub Pages / CloudFlare web page for free."},{"tags":"Side Activities","url":"why-i-moved-from-wordpress-to-pelican.html","title":"Why I moved from Wordpress to Pelican","text":"Introduction For years, I maintained a Wordpress blog covering various things I've done or created. Most of these revolved around things I created to make administering Team Vipers easier for me and for the rest of the admin team. It was my way of documenting what I'd done (in case I ever needed to do it again) and providing a way to update the Team Vipers community about new plugins or applications that would be deployed to the community. My Issues with Wordpress The problem I had with Wordpress was that it was just to bulky for the simple posts I was making. I needed a database, a full web server (or a hosting provider), and either time to hunt for the \"perfect\" plugin(s) or PHP knowledge to do it myself. Early in my development career, I used PHP a lot. That was part of the reason I chose Wordpress. Oh! I know that language. If I ever need something, I can just whip it up myself. -Me, before the real world ambushed me and beat me with a stick Spam I spent time setting up Wordpress. I picked out a theme, plugins, and started saving future me with documentation. Then life happened. For whatever reason, I stopped updating Wordpress. My blog sat out there for weeks or months unvisited by anyone. Then, one morning, my phone vibrated and told me that I had a new comment on my site. Woo! Except it was spam. Boo! I marked it spam and moved on with my day. Later that morning, I glanced at my phone again. 32 emails. I am just not that popular. Something was wrong. Turns out, a spam bot found me. I sighed and then removed all the comments and checked the box indicating that users had to be registered to post. That solved my problem for a few months. Then the bots got smarter. They started registering. They started posting legitimate looking messages, except for that associated URL their name would link to in the comments. They pulled keywords out of the post and formulated a somewhat passable English question using those words. The spam prevention plugins I installed would slow the tide for a few weeks. The bots would adapt and then I'd be awash with spam posts again. Eventually, the solution was to completely disable comments. I'd spent way to much time dealing with spam on a blog that received very little legitimate traffic. Since I don't utilize the comments, Pelican provides a nice simple page that I can post my thoughts and not worry about getting hit by a spam bot. It also provides plugins so that I can include comments should I ever choose to do so in the future. For the time being, though, I have a nice simple page with no comments. That's exactly what I was looking for. Security If you watch any technology web sites, you'll notice that there are vulnerabilities found in Wordpress frequently. These require patches, which requires me to do something. It may be as simple as logging in and clicking a button to update, but it is still something I need to remember to do for a relatively minor site. When I'd log in to clear the spam backlog, I'd frequently also install updates for 10-20 plugins, themes or Wordpress itself. It was mostly painless, but I didn't like the idea of the site sitting there vulnerable for weeks at a time because I didn't visit and login. The dynamic nature of Wordpress and the underlying database exposed a fairly sizable target for a web page so small. Pelican generates static HTML pages. I don't have to worry about SQL injections, unauthorized logins, or anything else. I host a basic set of HTML, CSS and JavaScript files. That's it. PHP vs Python As I mentioned before, I used to use PHP frequently. It was my go to language. I picked Wordpress with the idea that I'd be able to hack together features I needed. The reality, it turns out, was that I wasn't actually interested in doing that. Instead, I picked out plugins that were close enough to the exact functionality I wanted. I transferred to a job where I used Python. Instead of having a language I used on the side (PHP) and a job where I was a glorified project manager, without the actual title of \"Project Manager\", I now had a job where I used a language (Python) for 8 hours a day. My usage of PHP plummeted. I found I could get what I wanted done in my side projects faster and easier with Python. At work I used Python to build tools for engineering problems. At home, I started using it for every day tasks. Soon, I realized I hadn't used PHP for several versions of the language. My knowledge of the language was outdated. The biggest reason I'd chosen Wordpress was no longer relevant, because I couldn't write anything complicated in PHP without glancing at documentation to do even simple things. It's sad that I lost the intimate knowledge of a language, but I feel that I've been more productive with Python anyway. Pelican is written in Python. Even more importantly though, it generates HTML files which are hosted. I don't need to run a Python environment on a server. I just need to host HTML files. Markdown Finally, I've fought with Wordpress's text editor countless times. This happened most often when attempting to add code blocks. It was a pain to do. It was a pain to fix when the blocks broke. Pelican supports Markdown. Markdown is supported by large organizations like GitHub, reddit and Stack Exchange. I use all three of those. I know how to utilize Markdown to create code blocks, headers, insert images, create bulleted lists. All without needing to fight how the text editor is going to actually save the data."},{"tags":"Side Activities","url":"i'm-running-to-be-a-moderator-of-stack-overflow.html","title":"I'm running to be a moderator of Stack Overflow","text":"Introduction Stack Exchange has over 130 sites in its network. Each of those sites has at least 3 moderators that are glorified janitors : A lot of the moderation work is extremely mundane, almost janitorial. It's deleting obvious spam, closing blatantly off-topic questions, and culling some of the worst rated posts in various dimensions. The ideal moderator does as little as possible. But those little actions may be powerful and highly concentrated. Judiciously limiting your use of moderator powers to selectively prune and guide the community — now that's the true art of moderation. This works so well in the Stack Exchange network because the users are able to handle most moderation tasks. As users gain reputation, they gain privileges. With these privileges, they are better able to maintain and cultivate the content on the site. Not all activity can be performed by users though. This is where moderators come in. I already have a diamond (♦) on the Community Building site. I was appointed one of the Pro Tempore moderators during the beta phase of the site. This is my first experience moderating on Stack Exchange, beyond what my reputation on Stack Overflow gets me. It is not, however, my first time moderating . Community Building is focused on various aspects of building communities - both online and off. It caters to users from all aspects of a site (owners, moderators, users, advertisers and any thing else). My experience here led me to pursuing a moderator position on Stack Overflow. This site is orders of magnitude larger than Community Building . It has millions of users and millions of questions. With all this traffic, there are still only 18 diamond moderators. The idea of community moderation shows that it scales well. Nomination Phase The election cycle begins with the nomination phase. Users nominate themselves for the position. In 1200 characters or less, you lay out your platform. In this phase, nearly anyone can nominate themselves. However, there are only 30 positions available. If more than 30 people nominate themselves, then the users with the lowest reputation get bumped. With only 11K reputation, I am at the lower end of the spectrum of other nominees, but I am well above the lowest. I have the 8th lowest reputation. I've looked at previous election cycles. It seems unlikely that there will be 30 or more nominees. I think that I am safe. My Platform I posted my platform shortly after the nomination phase opened. I stressed the work that I've already done on Stack Overflow and previous moderation experience on another Stack Exchange site (Community Building). I'm Andy and I want to be one of your moderators. Why vote for me? I've built an automated script to handle noisy comments . It runs daily, probably to the chagrin of the current moderators, but it helps keep the comments noise down and it is incredibly accurate. In addition to the automated flagging, I participate in the review queues, provide edits and post answers . As a moderator of \"Community Building\" , I know the tools used by the Stack Exchange sites. I've used this position to interact with the existing moderators for advise and questions. This interaction will not only continue but be helpful as a new moderator to SO. I have the flagging history and experience to make the appropriate judgement calls. I feel that is a positive attribute that will help me in helping you to have the best experience you can on Stack Overflow. I'll continue to help improve the site regardless of whether I am elected or not. I will be able to do a much better job at that, though, as one of your moderators. With this, I received my \"candidate score\". It was 29/40. Not the highest, but not the lowest either. I expect, if any one is concerned about this score, it will be mentioned it in the comments. Nomination Phase Comments Updated April 11, 2014 The comments on my nomination post have focused mostly on the pros and cons of the automated script I built to flag comments. I was expecting this discussion. I'd classify most of the comments about the script as \"cautious\". Users seem concerned about something completely automated used to moderate or that I focus on low priority content. My response to this concern was: [...] by automating the removal of this lower priority content it is easier to focus on the actively harmful stuff. I'd like to point out that I have done more than \"build a script\". I've actively participated in review queues on Stack Overflow for years. I've flagged appropriate content (with a high percentage marked as accepted). I have also been moderator on Community Building. All of this, in combination with the automation, makes up my \"platform\". I have the experience of moderation, the knowledge of how the system works and the drive to improve the community. It was also mentioned a few times that I'd be unlikely to utilize such an automated tool as a moderator. I promised to bring this concern up when I moved further into the election process. Finally, I received a few comments supporting either me or the work that I've done. The automation is very impressive - thanks for that, it's a real contribution to the quality of the site. That said - as a privileged user I suspect you won't be able to run these sort of scripts on your account for obvious reasons. – Benjamin Gruenbaum Apr 6 at 21:34 I can vouch for Andy's track record as a pro tem mod on Community Building. I want to note as well for the record that many of the main site questions and answers on Community Building are explicitly about moderation, and his contributions there also provide insight into his approach to the job. – Air Apr 7 at 16:54 Huh. I have to say I'm impressed. Putting you on my list of candidates to watch. – Qix Apr 7 at 19:23 I'd just like to point out that your response to my concerns is fair and reasonable, as well as polite, even if it isn't quite exactly how I see things. It certainly alleviates my concerns of misusing moderator power. I will be seriously considering voting for you. – jpmc26 Apr 8 at 6:41 Overall, I'm pleased with how the discussion has gone. It has been a few days since there were comments left though. Hopefully a little interest picks up during the primary phase. I am also very impressed by the other candidates. There are many very good candidates that I'd be happy to work with (or be unashamed to lose to). Primary Phase Updated April 17, 2015 The nomination phases has ended. I have advanced to the primary phase. The purpose of this phase is to narrow the list of 30 candidates to 10. Those 10 will be the ones that can be voted on to be the next moderators. In the primary phase, users can vote nominees up or down. As part of the election process, one of the other candidates created several tools to help users see nominee activity. He also created a live vote monitor . It was exciting to hang out in the Election Chat room and watch votes roll in. The position of 10th was highly contested toward the end of the primary. I was very pleased to see that everyone remained civil in the chat room too. I ended this phase with 1492 positive votes. This put me in 15th. Sadly, not enough to advance to the election phase. However, there are several excellent candidates that did advance. Good luck to them. Other Primary Data Part of the primary phase involves answering questions that users have posed during the nomination phase. These questions were voted upon and the highest were included in the questionnaire. My response was removed at the end of this phase because I did not advance. I was prepared with answers about my moderation style and my expectations for the position. One of the other tools that was created during this cycle showed nominee activity on the site over time. This was a quick way to compare nominees based on the number of activities they have performed on undeleted posts (because data related to deleted posts is removed from public view). It provided, at a glance, a way to see who is and is not active. Below are the charts of my activity that the tool created. The green vertical bar is account creation date. The orange/brown vertical bars are previous election windows. This is my activity chart for Stack Overflow itself. It shows that I've had an account since 2009, but didn't really start utilizing the site, more than occassionally, until 2013. Since then, it shows that a large majority of my activity onsite is in the review queues. There are posts, revisions, comments and more along the bottom, but most of my activity is in the review queues. My Meta Stack Overflow and Meta Stack Exchange charts show a fairly low activity level. Unfortunately, I think this is obscuring the posts that I do make simply because of the large scale on the y-axis. Finally, the last chart shows the combined activity across all three areas (SO, MSO, and MSE). It looks very similar to the Stack Overflow one because of the lower activity levels on the two Meta sites. Election Phase Updated April 21, 2015 The election is over. Three great candidates were elected. One thing I am like about Stack Exchange's elections is their usage of OpenSTV and the Meek STV method . The results of each round of the election cycle are available at opavote.org . Final Thoughts Like most elections, there are known and unknown winners before the election even begins. The response that Martijn Pieters received from the time he nominated through the final moments of the election all but guarenteed a victory for him. It is a well deserved victory. I hope that he is able to keep up his frequent, high quality, answers in the python tag. If not, his drop in activity will be felt. The surprise, to me, was Jeremy Banks . He ended in 9th place in the primaries. That doesn't mean he can't do the job. I've interacted with all three moderators on site and in chat prior to their victory. I've been very happy with those interactions. With their promotion, I look forward to working with them as a moderator on the Stack Exchange network in the future. Congratulations to the winners!"},{"tags":"Programming Projects","url":"zephyr-the-bot-that-watches-for-low-quality-vote-requests.html","title":"Zephyr - The bot that watches for low quality vote requests","text":"Introduction Stack Exchange receives thousands of questions per day across all of their sites. Not all of these are high quality posts. Fortunately, users of the Stack Exchange network are given tools to help keep that low quality stuff to a minimum. One of these tools is the chat network that spans the Stack Exchange sites. In the chat rooms, a convention has arisen to tag a message as cv-pls for questions that need to be closed for one reason or another. Over time, this evolved to include other tags such as: del-pls for a deletion request spam for notification that spam made it through the already impressive spam filters reopen for a reopen request a few others to cover specific flag types (eg. Not an answer, Very Low Quality or Offensive) Introducing Zephyr The problem with these is that the requests are only seen by users active in the specific room where it was posted. Other users across the network miss the request. Zephyr was built to resolve this problem. Zephyr monitors several rooms where these types of requests are frequent. These requests all all posted into a single chat room . This provides users with a single room to monitor to see requests for multiple questions and sites across the network. Here is an example of what Zephyr's chat activity looks like during a spam wave: How it works Zephyr utilizes the ChatExchange package to join and read the chat rooms. To do this, Zephyr required a dedicated account. I decided to run Zephyr with a dedicated account to completely separate the bot that would sit and watch multiple chat rooms 24/7 from my account. Zephyr maintains a small SQLite database of all the posts that it records. The idea behind this, is that eventually this data will be utilized to train other systems on unwanted content. This information is pulled via the API . Zephyr watches the chat rooms for specific string patterns . If these patterns are matched, a message is posted if should_post is True for the matched pattern. Overall, a nice simple application. It performs some pattern matching and a couple API calls. Other bots In addition to watching user activity, Zephyr also watches two other quality bots that patrol Stack Exchange for low quality content: SmokeDetector and Phamhilator . If either of these bots detect spam, Zephyr takes note of the information by recording it to the database, but not reposting. Since both of those bots post their reports, it didn't make sense for Zephyr to add a second (or third, if both of the others detected spam) message to the chat room. The information is recorded, though, to help future training for other systems. Updates Updated May 8, 2015 Over time Zephyr has been updated to include new rooms to monitor or new patterns to match. Those changes are small (and simple). There are, however, a few larger changes that I'd like to note below. Commands The other bots that Zephyr monitors respond to user input. Zephyr has very little that requires user interaction since all of it's posts are generated by user input. However, there have been times where I, as the bot owner, would like to be able to issue certain commands to it. My most common desire is to see a report of how many spam posts Zephyr has seen. Thus, Zephyr now responds to the command spamreport from me. It then prints out a nice summary of information. This information has been utilized in SmokeDetector to watch for commonly spammed domains. Upgrade from SQLite to MariaDB Zephyr was originally built against an SQLite database. This worked, but was getting slower as more data was being added. This slow down was beginning to affect performance. I started seeing this error more and more frequently: Traceback (most recent call last): File \"H:\\python-virtualenvs\\zephyr-se-voterequests\\lib\\site-packages\\sqlalchemy\\pool.py\", line 255, in _close_connection self._dialect.do_close(connection) File \"H:\\python-virtualenvs\\zephyr-se-voterequests\\lib\\site-packages\\sqlalchemy\\engine\\default.py\", line 418, in do_close dbapi_connection.close() ProgrammingError: SQLite objects created in a thread can only be used in that same thread.The object was created in thread id 4824 and this is thread id 4660 After spending a lot of time troubleshooting and not resolving it to my satisfaction, I decided to upgrade to a more robust database. I'd used MySQL/MariaDB before and I happened to have another application utilizing MariaDB at the moment so that is the solution I picked. The first step was transferring data. I learned that there isn't a decent utility to do a straight migration. So, I took these steps to transfer the data: Export table structures and data from SQLite Convert the SQLite dump to MySQL format. Though both systems use SQL, there are slight differences in dialect. I utilized this Python script as a starting point. It got me most of the way there, but not completely. Data clean up. Ugh. The dreaded part of the job for anyone who handles data. Fortunately, the script above did most of the work. I ended up fixing a couple stray back ticks that didn't convert properly, escaping a very extra quotation marks, and replacing a few \"smart quotes\" (of both the left and right variety). I wish data at the office job was this easy to clean... Import into MariaDB Since the transfer to MariaDB, I've noticed no performance degradation. The error about threads has been eliminated as well. Upgrade to utilize web sockets Originally, Zephyr used the watch method when monitoring a room. This method would long poll the room. It turns out that this is pretty unreliable. I'd get multiple errors through out the week, ranging from Connection Aborted errors to random 404 messages. The solution has been to switch to watch_socket . The only time I've had problems since this switch is when the Stack Exchange web sockets go down. This saves a lot of restarts to get everything up and running again."},{"tags":"Vipers","url":"thanks-for-all-the-fish.html","title":"Thanks for all the fish","text":"Goodbye Team Vipers A few days ago Team Vipers shut down. Exactly 6 years after I join the community and 5 years after taking over as owner, I decided to step aside. I announced my decision to my admin team in November and asked if anyone wanted to take over. A week later I made the announcement public. I copied that below. My hope was that someone would be willing to take over day to day operations and management. Unfortunately, no one was able to take on the responsibilities. I had to make the hard choice of shutting down the community. I am unable to provide the time or monetary commitment this community requires any longer. Team Vipers was a home and community for 6 years. I've met a lot of people playing games with members of the community. Several other sites were created by members of Team Vipers. They have indicated they'd be willing to absorb and welcome members from Vipers. It's been a great run. So long Vipers, and thanks for all the fish. Original Announcement On January 5th, 2015, I am stepping away from Vipers as the owner and administrator. Unfortunately, I no longer have time to maintain the servers in a way that benefits the community. I announced this to the admin team earlier this week. What do we do before January 5th? I am giving you this heads up to discuss the future of Vipers. If someone wants to step up and take everything on, I'll gladly offer help during the transition. If you have questions about what happens behind the scenes, I'll answer those as well. In fact, I'll start with a few questions that I think may help lead this discussion. Do we need multiple game servers? Short answer - No. If the community wants to focus on a specific server mode, then a single server is all that is needed. When I joined Vipers there were two servers: An Orange server and a vanilla server. For the first year-ish, the servers base grew and contracted around different mods. Orange turned to Crit Orange, a Randomizer appeared and then splintered into it's own community, the Nest grew and then shrank. Zombie popped up. Prophunt and Dodgeball came around multiple times. In all of this, there has been between one or two popular servers, with the rest being very niche. Those are for fun. Those are also frustrating to fix when updates break them. Do we need a dedicated server? Again, no. The benefit the dedicated server provided is IP address stability. Before the dedicated server, the game servers jumped around IPs every few months. With each move players were lost. Some found us months or years later when they randomly stumbled upon us. Others are just gone. More importantly though, is dealing with performance. If there is a hosting company that can provide high quality, low ping, servers at a cheap price - that is worth it. A word of advice, avoid EscapedTurkey. It is a one man operation and that person doesn't know what they are doing. If an issue crops up, it takes hours to days to resolve it. It is because of them that we purchased a dedicated server. One other note. I can not transfer the dedicated server to anyone. It's rented in my name and the hosting company doesn't allow transfers. So, no matter what happens, the IP addresses would have to change anyway. The upside of this long transition period is that you have plenty of time to advertise and set up the new servers. We could have in game announcements AND we can lock the existing servers with the new IP in the titles. It won't be an immediate cut over. It provides notification to the players. Do we need the forums? Yes. The forums are what kept me in Vipers before I ended up in the position I'm in now. Without the forums, there is no community. There are people that you see frequently on the servers, but rarely any meaningful interaction. With the forums we get stories of our members' lives. If, however, you are going to stick to playing Valve games, I highly recommend a change in forum software. Find something that supports integration with Steam (either natively or via plugin) and start over. With that integration, you can do so many more (and non-hackish) things. You can pull information from Steam or any other TF2 community that exposes an API. Banning a player on the servers and the forums is easy as both are based on the same thing. Additionally, requiring the Steam login should eliminate the spam (no random Chinese, Russian, Californian spam bots). In any case, I will provide a full database dump for you. If you stick with PHPBB, I think a fresh install of PHPBB would be a good thing, but you'll have all of the data and should be able to simply import it into the database. If you go with something else, you'll have the database and can attempt to use that software's conversion script. Results will vary on how that works. Does we make money on donations? No. Since July of [2014 to November 2014], the community has donated $20. Total. It has been 14 months since we reached our monthly goal. In the last year, we have broken $50 only 4 times. To prevent someone from misconstruing this as a plea for money, I've disabled donations. I don't want someone to start a misguided \"Save the Vipers\" campaign that you believe can succeed with donations. The way to \"save the vipers\" is to come together as a team and discuss your options. What happens if no one steps up? Someone in the community may step up, but if no one steps up by January, then Vipers has one last New Years hoo-rah and fades to the internet. If someone does step up, I am still available to help with initial set up. I'm still available to play games and to talk to. I'm not dropping off the face of the earth and I'm not rage quitting. This is very much a case of real life happens. I think that is going to cover your initial questions. If you have more, post them here and I'll answer them. It has been a great 6 years with everyone here. While I may be stepping down as owner, if someone keeps Vipers around, you'll keep me around as a regular. I've had fun talking to all of you and playing with (and against) all of you. Updated Steam Group Ownership Updated April 28, 2015 Ownership of the Steam Group for Team Vipers has been transferred back to Russell, the original founder of Vipers. The announcement was posted to the group earlier today. I've also transfered data from the Vipers blog to my personal one here and removed links to Vipers from most posts since they will all fail to resolve correctly. Unfortunately, it's not worth it to preserve small bits of history in this case and was just easier to remove the links. The best I can do it transfer the blog posts and copy snippets or screenshots here. Direct links to the forum discussions has been intentionally removed."},{"tags":"Programming Projects","url":"can-a-machine-be-taught-to-flag-comments-automatically.html","title":"Can a machine be taught to flag comments automatically","text":"Introduction This post was originally published by me on Meta Stack Overflow on December 14, 2014. I've republished it here so that I can easily update information related to recent developments. If you have questions or comments, I highly encourage you to visit the question on Meta Stack Overflow and post there. TL;DR: Yes it can. Background On June 27, 2014 Skynet awoke. It looked at Stack Overflow and thought \"Why are all these people being so chatty and talking about obsolete things? I should nuke them all!\" Fortunately, Skynet was a baby and only had access to my 100 comment flags a day. Prior to this activation date, the system was fed with 10,000 \"Good Comments\", \"Obsolete\" comments and \"Too Chatty\" comments. These comments were taken from the Stack Exchange Data Explorer . The \"Obsolete\" and \"Too Chatty\" comment types had to meet the following criteria: Total comment length of less than 100 characters Comment has a 0 score Had variations of the following phrases: Phrases '%mark%answer%' '%mark%accept%' '%accept%answer%' '%lease%accept%' '%mark%answer%' '%thank%you%' '%thx%you%' '%.....' '+1%' '-1%' \"Good Comments\" were assumed, initially, to be anything that didn't fall into the above criteria This provided a base of 30,000 comments that were roughly categorized into 3 distinct groups. Manually scanning the classifications took several weeks, and through this some of the groupings were changed to reflect a more appropriate classification. Not all comments less than 100 characters starting with \"Thank you\" are \"too chatty\", just as not all comments over 100 characters are good comments. I reclassified these comments as if I had encountered them on Stack Overflow. My next step was to train a classifier. I had initially assumed that I'd start with a Naive Bayes to get a baseline and then work to something more complicated from there. Perhaps, extract text features, user information, etc. and build a fancy classifier. My initial tests showed that the Naive Bayes was accurate 80-90% of the time with test data. I combined the classifier's certainty of classification with an acceptable threshold of when I'd allow a flag to be issued in my name. Tuning these threshold took a few weeks but eventually I determined the following thresholds were appropriate for my use: Type | Threshold | Flagging Enabled -------------------------------------------------- too chatty | 0.9997 | True obsolete | 0.99 | True good comment | 0.9999 | False When a comment is classified, if it exceeds the threshold for one of the above, it is recorded into my database for future retraining. If flagging is enabled, the API is utilized to issue an appropriate flag. Obviously, I don't want to flag good comments, but I do want to record them so that I can reuse the data in a later training step. Results What have the results of this experiment been? From my point of view, I'd venture that it's been successful. I have automatically flagged over 17,000 comments. As of December 17, 2014, the process has been running for 173 days. My comment flagging stats are currently: 26885 comments flagged 26714 deemed helpful 171 declined Started at (approximately): 9885 comments flagged 9847 deemed helpful 38 declined This gives me an overall accuracy of 99.36%. Down from 99.61% when no automated process was involved. There are pictures that help tell this story too. In this first one, we see that the rolling 10 day average for the number of declined flags has stayed below two flags a day. In October, there was a two week period where the rolling average was 0 and nearly a month long period where the system did not make any mistakes. Since November, the number of mistakes has climbed slightly. The biggest number of mistakes it has made was the opening day of Winter Bash 2014. Purely speculation, but I believe this was the moderators being protective of content and not wanting people to farm the Resolution hat . Of course, I don't know this. Another theory I have about this uptick since November is the adjustment to day light saving time. My process starts 10 minutes after UTC. It is possible that this earlier hour has caused my flags to be processed by a different moderator, or a moderator that is more awake/less hungry/in a different mood than previously at this point in the daily rotation cycle or because they lost their keys that day. Except for 3 days, since June 27th, the process has flagged 100 comments a day. In this chart, you can see the number of declined comment flags along the bottom. Finally, this chart shows the number of comments that the system wanted to act on (and a rolling 5 day average). When the system was brought online, it was acting on 700-800 comments a day (saving to my local database). Many of these were being classified as \"Good Comments\". You can see the day that I adjusted the threshold for \"Good Comments\" to be acted upon (saved). The drop in the number of comments the system saved is dramatic. Instead of saving 700-800 comments daily, the system now averages about 150 comments to save. Since I don't flag \"Good Comments\", I feel this is the appropriate action to take. Flagged but declined As shown above, I've had comments flags declined. Some of these obviously should have been and required a retraining or threshold adjustment on my part. Others, in my opinion, should have been removed as noise. Below is a small sampling of both types of comments. Recent comments that I feel are noise: yes thank you so much for you help it works sorry for the late reply Wow it works. Thank you very much! wow that works!Thanks so much for your advice! Ok, the works great, thank you so much! Thank you very much for your explanation, you rock dude !!! Here are some comments that were incorrectly flagged: @Spina: yes. Check my answer. You can simply point MONGO_URL to an invalid URL. Sorry, my error. I was: \"position\", not \"display\". Check it: jsfiddle.net/hvfku99c I believe UI.registerHelper is, being deprecated. Please check my updated answer. Other comments are flagged but then edited prior to a moderator seeing the comment. The edit adds information to the post, thus the declination is justified: Yes, I have indexes. Let me show my schema was edited to the much more useful: Yes, I have indexes for UUID and Permission. In fact rlationship is a variable length here (e)-[rp:Has_Pocket|Has_Document*0..]->d Here is the question i had posted first using FIleStorage issue was edited to include the link to the referenced post. It's also worth noting that despite getting flags declined, some comments do eventually disappear. This is due to either flags raised by other community members putting the comment back in front of a moderator or by simply accumulating enough community flags for the system to act automatically. In either case, the desired result of removing noise has been accomplished. Oh, derr. good point. Edited. You're right! Hopefully you see my point anyways. Lessons and Observations Replication to other sites would depend on site culture As a (fairly) non-subjective site, Stack Overflow made a good test case for this. On a site like Community Building , Pets , Parenting or other site that accepts subjective answers, \"too chatty\" would be much harder to classify. +/-1 has been discouraged The observation I made on my own that comments with this type of content were distracting has been noticed by others as well. This was actually a very nice validation of my own process and some of the results posted on that thread show many such comments continue to be noise. Of course, this change did also force users to modify their content and may have added new patterns that can be utilized in future training. Ability to automatically check flags would be great so that automated runs could be paused if it goes crazy The process of checking that my flagging history remains accurate is time consuming. The status of a flag can't be acquired via the API. I've submitted a feature request for this information to be added to the API. With this information, flagging can be paused or stopped if X number of flags are declined. Stack Overflow's volume of comments is a crutch. Due to the high volume of comments and limited number of comment flags my account has available, I can afford to be picky on which comments I want to act on. The classifier itself is about 85% accurate in determining the type of comment. However, I artificially increase my accuracy by only acting on comments that have a very high classifier certainty by forcing this certainty level to meet or surpass my threshold values from above. Smaller sites, with a lower volume, don't have the benefit of having enough comments to be this picky. It is on these sites that a more feature based classifier would be important. The human element is still unpredictable. My classifier was trained utilizing my idea of how comments should be flagged. Prior to automating this, I was not 100% accurate. Additionally, moderators are not 100% accurate in their processing of flags. Users disagree on how these rules should be implemented, but are willing to assist in keeping the site clean. With more than 175K comments a week, every little bit helps. Discussion As my title states, my original question was whether or not I can teach a machine how to flag comments as I would. The answer to that is yes. The next question is whether this type of system would be helpful in cleaning up comments across Stack Overflow. My system works only on new comments created around each new UTC. Once my 100 flags are hit (or the API tells me to stop), it shuts down for the day. Having something automated go through historical comments or that can run all day would be beneficial. Finally, now that I've admitted that I've been automatically flagging comments, can I continue to do so? Update This section has been updated multiple times since the original post. Most recently, it was updated May 3, 2015 As I mentioned in the introduction, this was originally published in December 2014. How is the system behaving now? It is performing very well. Process Changes In January 2015, another user was using a basic query to look for invalid comments. This caused a high number of moderator flags, many of which were declined. My process was caught in this mass decline. This resulted in 49 declined flags for a single day. This is, by far, the largest number of declined flags the process has generated in a day. It did, however, prompt a process change after consultation with the Stack Overflow moderators. The process will no longer flag comments newer than 48 hours old. This provides users with a two day window to see a comment before the system will flag it. This single change has provided a huge improvement in terms of flag acceptance. May 2015 (11 Months) After nearly a year of running, these are my flagging statistics: 39938 comments flagged 39659 deemed helpful 279 declined This provides a helpful rate of 99.3%. This is down just slightly from 99.36% in December. I attribute a large part of the dip to the issue mentioned above. Here is an updated chart showing the rolling 10 day average for number of declined flags. I've had several stretches of multi-week time frames with no declined flags. This is a busy chart, so I've narrowed it down to show just the last 90 days. From here you can see that in the past 90 days there have been only 10 declined flags. Sept 2015 (15 Months) It has been almost 15 months since the process started. In that time, the model has gotten more accurate. Since the last update in May, I've had only 3 declined comment flags: 52351 comments flagged 52069 deemed helpful 282 declined This provides a helpful rate of 99.46%. Here is an updated chart showing the rolling 10 day average for number of declined flags. The 90 day window is not even worth showing. It has three days where a single flag was declined. Summary of 2015 I processed comments 359 days out of the year. I missed three in January due to stopping it after a mass decline of flags (more later), I can't account for a missed day in July and August. I don't recall stopping it, but I missed July 3rd and August 19. I also missed December 28th due to a power issue. I flagged 35,960 comments. Of that, 111 were declined. By month, this is the break down of rejected flags. The blip at the end of November is due to new moderators being elected and adjusting to what other moderators consider \"good\" versus \"bad\" comments. I didn't see the spike in the April election which is interesting, but after a couple days in November it's back to normal. The January spike I mentioned above. Interesting note: The longest stretch in the year with no declined flags was from August 13th through November 24th."},{"tags":"Side Activities","url":"moderation-postion-on-moderators.html","title":"Moderation postion on Moderators","text":"Introduction I've been the owner of Vipers for almost 5 years. In that time I've help the community grown and get stronger. I've helped members begin their own communities and I've maintained the game servers the players utilize 24 hours a day. I've been concerned about low activity, excited about new events and updates, and watched the community ebb and flow through out the years. I've dealt with trolls, spammers, and new community members. I think I've gained valuable experience in managing a community. I'm also a member on Stack Overflow and over the years gotten to know others that enjoy the \"Meta\" aspect of Stack Exchange. That's the \"behind the scenes\", \"how does this site work?\" discussions and activities. Moderators A few days ago, I found a way to merge my experience with Vipers and the knowledge of the Stack Exchange platform. A new community started recently: Moderators . The idea behind this site is for people building, administering, managing or cultivating digital communities to have a place to ask others with similar experiences questions. I love it! One of the things a new site needs on the Stack Exchange network is a set of pro tempore moderators. While Stack Exchange believe in community moderation , there are still some things that require more access. These pro tempore moderators are those people. I nominated myself for one such position. The Nomination This is the nomination I presented to the community. In it, I discuss my experience with Vipers and some of our challenges. One in particular that I mention is the automated language filter we have on the game servers. I am an administrator/site owner of a medium sized gaming community that runs on a PHPBB3 board. We host multiple game servers as well. I've got a team of moderators that help keep the forum and game servers clean. I've run this site for 5 years, after taking it over from the original creator of the community who wanted to move on. In my time as admin, we've seen the number of participants on the forum increase. We've seen our game server population increase as well. I attribute this to getting the community involved in change discussions. One of our biggest changes occurred several years ago. Community members complained that our game servers would be over run with trolls at hours when moderators weren't available and spewing filth. The community wanted a cleaner game server experience. Users wanted these players gone immediately. Previous community leaders felt that trolling of this kind was part of the game and did nothing. After some discussions regarding what was and wasn't appropriate, we decided to be (for lack of a better term) \"family friendlier\". Certain 'extreme' phrases were no longer tolerated at all. A technical solution was built to automatically remove players that violated these rules. This solution allowed users to swear, but once it became excessive (again, defined by the community) they, too, were removed. The tool we have (PHPBB3) may not have the reputation, badges, or increasing privileges used here on Stack Exchange but for my community that has not been a negative. Engaging with the community in discussions and letting the members provide input that me and my team utilize has been extremely beneficial. I have no experience moderating a Stack Exchange site. I don't feel that's a down side though. I can provide the \"outside\" perspective in a Stack Exchange heavy group. That does mean, though, that I'd depend on and expect the community to provide feedback on how moderation in being handled. Much like my existing gaming community, input from the community to the moderation team is important and the moderation team should be listening to that input. Appointment Updated on August 12, 2014 I have been appointed to a moderator position on Moderators. I now \"moderator the moderators\", as the running joke has been on meta and the chatroom. I am looking forward to helping this community grow and to providing my experience of community management outside of Stack Exchange to this position. Renaming to Community Building Updated on December 4, 2014 Almost since the beginning of Moderators, there have been conversations about whether the name is limiting our scope. We aren't a community for only moderation questions. We are a community about how to build communities and moderation happens to be a part of community building. (How many times can you say \"community\" before it sounds weird?) Discussions began in August about possible name changes. These discussions continued into October as we worked on the scope of our site. Finally, on December 2, we received our new name: Community Building . It's the same great site but with a much more relevant name."},{"tags":"Vipers","url":"how-to-appeal-a-ban-effectively.html","title":"How to appeal a ban effectively","text":"Introduction In last week's post , I showed three ways that appealing bans on the Vipers forums would fail. Everyone makes mistakes and we try to recognize that when someone comes to the forums and makes a good appeal. We usually offer an unadvertised \"last chance\". This is the chance to prove you've changed. If you fail, you are gone for good. If, however, you have changed behaviors, this allows you to play on the Vipers servers again. To get this chance though, you have to make an appeal. Super secret formula for getting unbanned The secret to a successful appeal is to do all of the following in your appeal thread: Post in complete sentences, using mostly correct english. I'm not going to mark you off for simple spelling mistakes, but I'm not going to read your post if it looks like you typed it from your phone to your teenage buddy. Stay polite. If you lose your cool, it's much less likely we are going to want to work with you. Remember, this is a game. You not being able to play on one server is not the end of the world. Explain why you think your ban was inappropriate. Make this short and to the point. Don't lie. I have logs. I know how to read logs. I even have ways of quickly searching through the logs for specific times, if you provide that information. If your story doesn't match what I see in the logs, I'm not going to engage with you. Answer questions from the admins. It's entirely possible you didn't provide a crucial bit of information or we need to wait for input from the banning admin. In either case, if an administrator asks you a question, you should probably answer it. When you do so, follow the first two bullet points above. Summary The sad thing is most of the appeals fail at step 1. These posts are to facilitate communication between the admin team and the player that wants to return to the server. In these appeals I try to educate a player on why they were banned in the first place. If it's difficult to understand what is being said, it's very hard to have this conversation."},{"tags":"Vipers","url":"how-not-to-appeal-a-ban.html","title":"How NOT to appeal a ban","text":"Introduction I've been running Vipers for about four and a half years. In that time I've seen my share of players get banned. The \"good\" ones realize they screwed up and come back when the ban has expired and never have an issue again. These types of players are common if they are first time offenders. The minute someone gets a second ban, the likelihood of them getting another ban shoots way up. Our Rules Vipers has five very simple rules. These are presented every time someone joins any of our game servers. They have to click \"Continue\" to move past them. We have a very simple structure for how bans work. Unless you are incredibly egregious in your behavior, or are out right cheating by using hacks of some kind, you get a 4 strike policy. The ban length for the first three increases for each, but you are allowed back. On the forth, we show you the door. Most of our bans are automated , thus are violating the very first rule. Bad appeals Playing on our servers is not a right. It is not a guarantee. We are not obligated to provide a gaming environment for you to spew your filth. If you can't meet our simple rules, you can leave. If you want to get back into the servers after your forth offense, you have to come to us and ask. You should think about how you get someone to do what you want in \"the real world\". Method 1: Freedom of Speech I think this one is my favorite. Last I checked, I'm not the government of the United States. I'm a private citizen providing a private server for others to play on. My server, my rules (technically, the community's rules, but you understand...) I am not bound by the First Amendment to allow you to say whatever you want. An example: Let's break down some of the flaws: First, and most importantly, we do not have a member named \"DarkWolf\", let alone an admin DarkWolf was banned, automatically, 2 minutes prior to the person making this appeal. The chat logs show that the person appealing even noticed the ban and made a comment. It was this comment that triggered the ban. Not seen in this image is the response when this was pointed out and the appeal denied. The response managed to hit all of the trigger words we ban for on the servers. Just in case we were reconsidering the denial, at this point we're not. Method 2: Admin Abuse Admins are evil and are holding a grudge! They hate me and ban for inappropriate language. Honest, I never said anything bad. It's amazing how quickly that tune changes when I can produce exact, timestamped, logs with their Steam ID attached to it. It's even more amusing when the admin being accused is \"Zephyr\", the automated process that watches for such language. An example: Let's break down this one too: Zephyr is a bot. It is not holding a grudge and it is not stalking you Swearing, unless very very excessive, isn't going to trigger a ban Chat logs show this is the 4th automated ban. In each ban, the user has managed to slip in multiple offensive phrases befor being automatically kicked Method 3: Ranting A rant isn't helpful to anyone. It doesn't endear the poster to the community. The administrators don't want to read through a rambling, unformated, exposition. An example: Our break down: Several logic errors and contradictions to their own arguments Doesn't actually mention they are looking to be unbanned, just that they want to complain about the rules. (It's mentioned in a later post that they'd like to be unbanned) Chat logs clearly indicate the player was fond of certain slurs. This is the reason for the bans Method 4: Threats to the server Threats to attack the server in some way are not going to get you unbanned. There are only two possible outcomes to this appeal type. Either you are successful and the server is off line (now how are you going to play even if unbanned) or we ignore your tantrum and leave you banned. Break down: I have no reason to unban you. You've threatened the servers and made an ultimatum that effectively holds us hostage: \"Either I get to play with you, or I'll take down your servers\". You offer no proof of your claims (unsurprisingly, none existed) Summary Above I've posted three of the most common methods people use to appeal their ban on the Viper servers. Every time one of these types of appeals is posted, it is rejected and the player is told they aren't welcome. All I (and the community) ask is a little bit of respect. You've already proven that you can't follow our rules if we reach this point. If you have truly changed, why don't you demonstrate some of that change while requesting the ban be removed."},{"tags":"Vipers","url":"mann-vs-vipers-beaten.html","title":"Mann Vs Vipers Beaten!","text":"The Team Vipers' Mann Vs Machine modification - Mann Vs Vipers - that was completed in August has finally been beaten. Last night a starving group of players finally put down Wave 9. The players were: MooMooCow CutieVamp Batman Starfox Healthy Cyanide Zhiv The team struggled against the Admin bots - InsaneMosquito, Venom and SchooledYa. I've been told we are terrifying to play against and making us MvM bots with even higher stats made it even more terrifying. (I approve.) Here's a video of Wave 9 from the victory. The winning strategy appears to be the one that works against us most of the time. Separate the medic (me), from the Heavy (Venom) and the Soldier (SchooledYa). Together, we're hard to stop, but apart we can each be picked on for a while until there is no health left. The bots must have learned that weakness as well (I don't approve.) Congratulations to the winners!"},{"tags":"Side Activities","url":"i-had-a-secret-addiction-to-the-tf2-idling-economy-but-i'm-better-now-(honest).html","title":"I had a secret addiction to the TF2 idling economy but I'm better now (honest)","text":"Introduction On July 11, 2013, Valve released a patch to Team Fortress 2 to limit idling. Idling is the process of launching Team Fortress and then letting it run for a few hours and collecting item drops. It used to be simple to do by utilizing the -textmode game parameter . This would \"launch\" the game without graphics. From there, you just let it run and come back in a few hours with 8-10 new items (your weekly limit). The idea behind this was to then trade or craft these into metal. The metal could be used to trade for keys, hats, etc. and all you to be \"rich\". The quotes are there because I do realize that being rich in a virtual game does not make one rich in the real world. But, since the Mann-Conomy Update introduced this massive meta game to Team Fortress, I've tried to stay out of it. I failed. I didn't just idle one account. I idled 17 accounts. That is between 136 and 170 items a week. That's between 68 to 85 scrap metal a week. That's between 7.55 and 9.44 refined metal a week. That is between 3 and 4 keys a week (depending on who I trade with). Keys are the backbone of the economy. Get enough of those and you can trade for anything else. I honestly don't know what my goal was. I wasn't interested in the fancy hats (ooooh...shiny pixels). I didn't open crates with the keys. I did utilize the keys to get Tour of Duty tickets for me and some friends. That's honorable, right? How did this happen? This all started with the bot I built for Vipers to handle the raffles . I realized that I could automate much more than trading with players. I could automate trading between bots. I could automate crafting - which is relatively time consuming, especially when you have 136 items to go through. I could automate trading with established scrap bank bots. These bots will take any two weapon drops and give you one scrap, even if the two items can't be crafted together. The idea is that they get enough weapons that eventually they'll be able to craft it down to metal. The set up To idle 17 accounts I needed to figure out what my computer could handle. I had to figure out how to run multiple instances of Steam and Team Fortress at a time. Enter an application I'd utilized before: Sandboxie . This application provides isolated sandboxes for applications to run in. Normally Steam won't run more than once on a machine. But, if you launch it via Sandboxie, the host OS (and other Sandboxie environments) can't see that Steam is running in another instance. A bit of experiementation showed that I could handle 6 accounts idling at a time and have the computer remain just barely usable. I created new accounts and split them into which day of the week they'd be run. I had three days out of the week designated for idling. A batch script was built to launch the Sandboxie environments of the day that was to idle. Then it'd launch Team Fortress in each of those environments. Then I'd go to bed. When I woke up the next morning, I'd shut down the environments. I'd repeat this for two more nights. At the end of the third night, I had all the items I could get for the week. Accounts could not be free accounts. This meant that I needed to either buy an item for each account, or trade for an \"Upgrade to Premium Gift\" . Getting items to a single account The next step in the process was to convert all the items to metal. Normally, this would take place by either logging into each account and crafting there and then trading everything to a master account, or trading everything first then crafting. In either case, 17 accounts is a lot to handle and trading/crafting is rather boring. My solution was to modify the raffle bot. I'd designate one account as a master account. This would be the account that received items. All others would dump items to it. I'd log into the account that was receiving items and initiate a trade with each other bot in turn. I'd issue a command add all and that bot would dump it's inventory into trade. Making it through all the bots would take 5 minutes. Previously it would take me an hour or more to log into each account and manually add items to the trade windows and then confirm the trade on both sides. yawn Crafting The next step was crafting items to metal. The rule was that any two weapons utilized by the same class could be crafted to scrap metal. That sounds simple enough. After lots of trial and error, I'd built a set of commands that would select compatable weapons and craft them together. This particular aspect wasn't documented by Steam (or the revese engineered SteamKit2 library I was utilizing). Crafting would take about 15 seconds to get all compatible items to scrap and then combining 9 scrap to get a refined metal. This saved inventory space. With the simple command craftall , all those new weapons would be crafted into metal and then combined into refined metal. This would be done in less than a minute. Previously, I'd have to initiate each crafting session, add all items per craft, wait for the craft to complete and then repeat. This was another 15-30 minutes saved. Left overs After crafting, there would still be left over weapons. There were weapons that had run out of same class items to craft with. The solution was to trade these away in groups of two to ScrapBank.me and get scrap metal back. This could be further combined to refined metal again wasting space. Metal to Keys At this point, human intervention was required again. I had to convert my metal to keys. I did this via trading. The easy way to do this was to find someone running a keybot, that would take metal for a key. Many exist, but all of them overcharge. The other option was to look for a trader that was getting rid of a bulk set of keys and then trade them. In either case, I'd end the week with 3-4 keys. I'd trade these for Tour of Duty tickets and go play a game with friends. The actual work required on my part was starting the idling for the night, shutting down idling in the morning, starting the automated trade, crafting and trade left overs processes and finally trading metal for keys. What used to take me hours to do each week, I could not do in less than an hour (the bulk of which ended up being trading metal for keys). What changed? Valve released a patch to limit how effective idling was. Their \"Mann-Conomy\" was seeing rapid inflation. The price of keys (which they sold for physical money) in metal was rapidly increasing. From the time I started this to the time I ended it, it jumped from 2-3 refined per key to 9-10 per key, with no sign of stopping. Casual players were complaining they couldn't get enough items to trade for this stuff. The patch also required that you click to confirm each new received item. I'm ok. Honest. I tried for a few days to find an effective work around. I didn't. When I wasn't able to find one, I transfered all items to the master account and went through the process of converting to keys. With that, I purchased the final set of Tour of Duty tickets. The idle bots were shut down. It's been a month now. The bots are still down. I'm still around. Looking back on this, I'm happy Valve broke this. It wasn't doing anything for me other than providing me with something to do: \"Need to idle tonight\", \"Need to craft and trade today\", etc. Now I have that time back. I am pleased with the technical challeges I over came to get this done though. Crafting was the biggest, but I think I'm most proud of the automated transfer of items to the master. Since the bots had to communicate via Steam and not via a local application, working out how I was going to do that took some time. That, my friends, was my addiction to the meta game of a hat similator in a first person shooter. I'm over it now and looking forward to some other project."},{"tags":"Vipers","url":"give-some-refined-win-some-prizes.html","title":"Give some refined, Win some prizes","text":"Introduction Financially, Vipers is supported by donations from the community. When the community doesn't cover the cost, I end up covering the difference. This isn't my favorite thing to do in the world, but we've been pretty successful in the past. In recent months, though, we've been coming up short more frequently. This has motivated me (and the rest of the admin team to find ways to cover costs). Now we have one. Welcome to the new raffle bot I've built a Raffle bot based on SteamBot , which is the base of scrap.tf . Entries to raffles will be one entry per refined metal. You can have an unlimited number of entries. I will convert the refined metal to various prizes (with the goal being keys most of the time). Then we'll have the system select a set of winners from the entries. A user can only win once per raffle, so even if you you have a gigantic number of entries, your odds of winning more than one position are zero. Only one win per steam id. Using the prices from backpack.tf , the bot will determine the \"value\" of the items within the trade. How does this off set costs? Items that are received that are of high value or any additional keys we can get based on raffle entries will be sold on various TF2 trading markets. The profits from these trades will be used to cover some community costs. Keep high value items for future raffles? This question was added based on feed back from the community. It was added on March 20, 2013 The original plan was to sell any such items. However, due to community feedback, I've changed my mind. We will utilize \"high value\" items as prizes for future raffles. These future raffles will not be announced until any running raffles are complete. It is also possible that such a raffle will run separately from the planned monthly ones. Our first winners Updated on April 3, 2013 Our first raffle completed at Midnight on April 1st. I was surprised by the number of entries we received. I'm even more surprised that the second one has begun and is already three quarters of the way to the number of entries it took a month to receive. People want those keys, and I saw mention of those Bill's Hats too. The number of entries we received allowed us to completely cover the community costs that donations didn't cover. Thank you to all our players that entered! Our first winners are: Cashprizes: Winner of 10 keys Popinfresh: Winner of 7 keys Iamthebaron: Winner of 4 refined metal That Guy From That Thing: Winner of 1 refined metal"},{"tags":"Vipers","url":"homing-projectiles-are-awesome!.html","title":"Homing projectiles are awesome!","text":"Stupid soldier spam The appeal of the crit server is fast game play, overpowered shots, and nearly instant death if you aren't paying attention. The down side is the soldier spam. Lots of it. It's not unusual to have a team of soldiers spamming rockets. This is part of the reason we stuck a class limit on Soldiers. Pyro is a common way to counter a soldier firing at long range. The problem with pyro is that it has limited long range weapons in return. Unless you can sneak up on an enemy (not easy with spam and some of the maps), the pyro is stuck taking pot shots with either the Flare gun or the shotgun. Two weeks ago, I added a plugin to the server that made Pyro much more effective at helping the team without needing to advance to the front line constantly. Reflectiles Reflected Projectiles - Reflectiles, if you will - becoming homing projectiles when a Pyro air blasts them away. These newly tracking projectiles will track an opposing team member and hunt them down. If the player being tracked dies before the projectile hits them, the projectile will select a new target. Well, that seems unfair. How do you defend against a homing projectile as a soldier? It is called Team Fortress 2. You have team mates. Utilize them. That homing projectile can be reflected again by a Pyro on your team. Each time a projectile is reflected it gets just a bit faster. Source Code Updated May 20, 2015 with link to GitHub instead of the old SVN, my apologies for missing that link when migrating to this blog It is important to note that this version hasn't been updated in a LONG time but was still functioning when Vipers shut down. If it doesn't work, the first thing to try is updating SourceMod's gamedata. This was the fix every other time it didn't work The source code is released on Github. The repository is: https://github.com/AWegnerGitHub/Vipers-Server-Plugins"},{"tags":"Vipers","url":"monitoring-language-on-the-game-servers.html","title":"Monitoring Language on the game servers","text":"Introduction My admin tool of choice for the TF2 servers is HLSW . It's decent at allowing me to manage a server without ever needing to log to the server. My biggest complaint about it is that I can only watch the game chat of one server at a time. Sometimes, it's helpful to see an ongoing conversation to resolve minor problems before they become big ones. For example, claims of \"hacking\" usually turn out to be completely baseless. But, if multiple users (and more importantly, multiple trusted users) suddenly start mentioning a hacker, I can step in and resolve the problem without entering the server. HLSW is good for this. A hacker is confined to one server. The biggest problem is when there are reports of lag across all of the game servers. Vipers has a dedicated machine that runs 5 game servers. If all five suddenly report lag, there is a problem somewhere. With HLSW, though, I can't see all of the servers at once. Thus, I've built a tool... Chat Monitor All chat that occurs on the servers is logged. I've used this to resolve complaints of unfair bans and reports of hackers. I built in a hook to these logs from the application template to quickly pull known aliases of users. It's been invaluable in solving problems of \"what happened\" on the servers. I've expanded it's usefulness. Now I can load a single page and see all chat activity occurring on all active game servers on a single screen. It provides, at a glance, a quick way to see if there are problems on the servers. It also allows me to step back from picking which server I think will be \"bad\" and monitor that. Now I can monitor all of them at once. Inappropriate words As a community, we've chosen to set a higher standard for our players. As such, we have a restriction on a total of 3 words and a few derivatives of those words. This system is in place because the community stepped forward and wanted to clean up the experience on the servers a bit. The problem with these higher standards is that we don't have admins on the game servers (or watch chat logs) 24 hours a day. Thus, while admins sleep, a troll can wander through and spew garbage. Unless a user reports this behavior, we will never be aware of it. I've built a system to handle this automatically. The system will monitor chat logs across all servers. If a user hits the threshold for banning, they will be removed from the server and banned for a day. The logic to the system is this: Automated removal logic Inappropriate terms are configured with a \"weight\". This \"weight\" will be used to calculate whether or not a user surpasses a threshold set for being banned. System monitors chat logs for configured terms. If a term is found, the offending message is saved. The term weight is added to the user's current threshold value. If this is the user's first time saying one of these terms, they start at 0 and this weight is added. If user exceeds the threshold, the system issues a ban to Sourcebans. The user is then kicked from the game server. The ban length will be 1 day. The system keeps messages for a total of 5 minutes. If a message is older than that, the system forgets it. Currently, the three inappropriate terms all have a threshold of 1 . This means they saying the words results in a ban. Homophobic, racist remarks aren't welcome on the Viper servers. We can't prevent it, but we can deal with offences swiftly. The 5 minute window is added because the community requested that excessive swearing also be limited. We don't want to outright ban it, but they don't want a swear filled rant to occur after every match. Thus, I built in the 5 minute window and the thresholds. The system is configured to catch common swear words, but the words have a low weight. It'd take repeated spamming of the words in a 5 minute window to reach the threshold and be removed from the server. Updated removal logic Updated May 17, 2012 The automated system has been active for almost a month. I'm finding that the system has been removing the same set of players every other day. They aren't learning. This is despite the message they are shown when removed from the server. I've made a change to the logic in how long a ban will last. It provides a 4 strike system: First offence: Weight of term(s) said times 1. This means, for most cases, they are issued a single day ban. Second offence: Weight of term(s) said times 3. This means, for most cases, they are issued a three day ban. Third offence: Weight of terms said times 21. This means, for most cases, they are issued a three week ban. Forth offence: Permanent removal from the game servers. The community has been very enthusiastic about how quickly users of inappropriate terms are removed. I've seen a few minor complaints about the permanent removal of users on the forth offence. I've told the community that if a user protests the ban and if they can show they've learned our rules, I will provide one additional chance after the user has waited a minimum of a month from from last time they were banned. If they return to their previous activities, they will be re-banned and they will not be able to return in the future. Update at shutdown Updated January 20, 2015 In January 2015, Team Vipers shut down . With that shutdown, all chat monitoring also shut down. The system was active from April 23, 2012 to January 4, 2015, for a total of 987 days. In that time, 4457 bans were issued for inappropriate language. That is over 4 users a day being removed from our player base because they couldn't maintain a respectful attitude. I consider that a success. I believe Viper community members did too."},{"tags":"Technical Solutions","url":"connect-python-to-osi-soft-pi.html","title":"Connect Python to OSI Soft PI","text":"OSI PI is a historian database. I had a task to connect a python application to this database. I asked a question on Stack Overflow about whether this was a simple problem to solve. After two weeks I still hadn't gotten a viable response, so I had to build by own solution. I did reach out to the vendor first for help. Their response back was not helpful. Looks like pyodbc is written against ODBC 3.x. The OSI PI ODBC driver is using ODBC 2.0. The python ODBC driver manager will convert most ODBC 3 calls on the fly to ODBC 2 ones. Anything added to 3, however, will obviously fail. You would need to find some way to make sure that your only using 2.0 compliant ODBC calls. Currently their is not a PI ODBC driver that is compliant with ODBC 3.0. So, it looks like the vendor doesn't support Python (odd, they are named \"PI\", but I digress). Additionally, the drivers provided by the company initially didn't work. The code below shows how I was able to finally connect python to OSI PI. It may not be the most elegant, but it functions for the purposes of my application. Initially I was attempting to connect using the pyodbc module. Unfortunately, OSI PI would return a message like this: pyodbc . Error : ( ' IM002 ' , \"[IM002] [OSI][PI ODBC][PI]PI-API Error <pilg_getdefserverinfo> 0 (0) (SQLDriverConnectW); [01000] [Microsoft][ODBC Driver Manager] The driver doesn't support the version of ODBC behavior that the application requested (see SQLSetEnvAttr). (0)\" ) They vendor mentioned that using OLEDB instead may prove more fruitful. Thus, the code below is how I got connected using the vendor provided OLDEB driver. The downside is that I also had to do this all through COM objects using win32com . I'm not knocking the module, because it is extremely useful and I've done some great things with it. from win32com.client import Dispatch oConn = Dispatch ( 'ADODB.Connection' ) oRS = Dispatch ( 'ADODB.RecordSet' ) oConn . ConnectionString = \"Provider=PIOLEDB;Data Source=<server>;User ID=<username>;database=<database>;Password=<password>\" oConn . Open () if oConn . State == 0 : print \"We've connected to the database.\" db_cmd = \"\"\"SELECT tag FROM pipoint WHERE tag LIKE 'TAG0001%'\"\"\" oRS . ActiveConnection = oConn oRS . Open ( db_cmd ) while not oRS . EOF : #print oRS.Fields.Item(\"tag\").Value # Ability to print by a field name print oRS . Fields . Item ( 0 ) . Value # Ability to print by a field location oRS . MoveNext () oRS . Close () oRS = None else : print \"Not connected\" if oConn . State == 0 : oConn . Close () oConn = None I followed up on my Stack Overflow post about 2 months after posting my solution with the following note: Just following up on this after using it for a couple months. This is still the only way I've found to do this with python, but it seems to be very slow when I need to run a large number of queries. I suspect it is because I have to open/close the database connection for each query, but OSI PI/ADODB complains if I do not. Performance has not reached a point where I am forced to rewrite this yet. If/when I do I will follow up again. In the meantime others using this solution should be aware that it is slow when running many queries."},{"tags":"Vipers","url":"a-new-more-fair-rtd.html","title":"A new, more fair, RTD","text":"The old RTD Viper's own LinuxLover, aka pheadxdll on the SourceMod forums , wrote the original version of the Roll the Dice plugin. It has provided countless hours of fun for players. After all, who doesn't love getting Toxic while standing near an enemy spawn and hearing the rage as they die immediately. It's usually worth the instant death that follows when the effect wears off ten seconds later. There were problems though. The biggest was that the chances of getting a Good vs Bad roll were not equal. Instead, you had a roughly equal chance of getting any roll. There were 14 possible rolls. You had a 1 out of 14 chance of getting a specific effect. However, 9 of those effects were negative. Thus, you had a much higher chance of getting a negative effect vs a positive one. The other major problem was that it was very difficult to add new effects. Finally, with the released mod not being updated, and LinuxLover departing Vipers to handle his own community on the Randomizer server, it was next to impossible to get changes made. NOTE : LinuxLover released a new version of RTD (the 0.4 branch) sometime after we had forked the version we had. The new version on SourceMod contains many of the same features we have. It does not, however, contain all of them. The new RTD The logic for how rolls are determined has been changed. There is now a list of Good Effects and a list of Bad Effects. When someone rolls, the first thing that is done is determine whether or not we are going to have a Good or Bad effect. That is a 50/50 chance. Then it randomly selects one of the effects that are active and appropriate for the player's current class that falls under the winning category. This should even out the Good vs Bad complaints. Another change that I've added is that we can now more easily add effects. Some new effects have been added: Powerplay: When Uber isn't enough...you need Kritz and Uber. Freeze Bullets: You've been shot! You should run away, but you can't. You're frozen in place for the next ten seconds, if you're lucky. Fire Bullets: A bullet wound isn't enough. You need to be on fire too. No crits: Haha! You are on an all crits server and you just lost your crits. Go sit at the little kid table. Valve Rockets: We've included something that may be slightly overkill. You tell me: +9900% damage +9000% clip size +75% firing speed +500 health on kill 10 seconds of crits on kill +200% speed +60% reload time Because sometimes overkill is needed. We've been testing performance of this over the last few months. Today is the day that everyone can get it without an admin being around. For those that haven't seen it, here is one of our first tests back in November The Headless Horseless Headmann: That's right, you can be the Halloween nightmare. Update: Homing Projectiles: Is that soldier aimbotting?! Nope. His rockets are just following you where ever you go. Boom! Oh, sniper arrows and pyro flares are probably something to avoid too. Finally, I added in the ability to log rolls to a database so that we can find fancy stats and ensure it's rolling fairly. I'll update this post in a few months with some of our gathered stats. Source Code Updated May 20, 2015 with link to GitHub instead of the old SVN, my apologies for missing that link when migrating to this blog It is important to note that this version hasn't been updated in a LONG time and all effects may not work any more. Specifically, homing probably doesn't work because the Sidewinder extension had been broken by a Valve update in mid-2014. Other effects should continue to work The source code is released on Github. It requires several common modules which are all available on the SourceMod forums. The repository is: https://github.com/AWegnerGitHub/Vipers-Server-Plugins Statistics This section was updated in January 2015 after the shutdown Good vs Bad I don't have exact stats on Good vs Bad rolls before the rewrite, but we started logging all rolls with the rewrite. This is the split of Good vs Bad. I am very happy with an almost exactly 50/50 split over nearly a million RTDs. Class with the most RTDs Solider was the most popular class on the crit server. The images below show number of times our soldiers got certain rolls. Note that God Mode is low because the community decided having both God Mode and Powerplay was redundant. They decided to keep Power play. God Mode was disabled about a year after this version was released. Homing is low because it wasn't implemented until about 18 months after the initial release. Class with the least RTDs Medics rolled RTD the least number of times. This means sense for two reasons. First, they were not a common class on the crit server. When nearly everything is one hit, one kill, it's very hard to build uber. Second, when you do try to build Uber, ruining it with a 50/50 shot at getting a bad roll isn't good for the team. The low God Mode and Homing results are the same here as they were for the soldier."},{"tags":"Vipers","url":"a-special-kind-of-troll.html","title":"A special kind of troll","text":"What happened? Over the Christmas holiday, the Team Vipers forums were hit by \"multiple\" users who decided it was a good idea to use our community as a battle ground. A Viper member stepped in to \"help\" resolve the problem by attempting to play mediator. (Notice the air quotes. These are important later) The battling users turned on this member and escalated the matter to threats of violence. When the member stopped responding, the users got bored and began spamming the board. Due to a combination of it being Christmas and the admins doing Christmas-y things with their families, this went on for hours. There were several rounds of clean ups, but the battle continued over nearly a 48 hour period. Why is this troll different? After Christmas festivities had ended and I had time to sit down in front of a computer to investigate what happened, I began piecing together a time line of events. It turns out that all of these \"users\" were coming from the same IP address. On top of that, they shared the same IP address as the member that attempted to \"help\". Odd... A little more background The member that was \"helping\" had recently been approached by the admin team in an effort to clarify some questions about their age. One of the admins had been presented evidence that the user was under the age of 16, which we all know if the minimum age to be a Vipers member. The user took this as a threat that we were going to remove them from the community entirely via bans on the servers and forums. They posted a rant on their \"sub-community\". To mean this seems much more like a place where people that didn't like our rules go to complain about us in an unproductive manner. I'm still not sure why they set up this community in the first place, because they have always expressed a very keen interest in Vipers. Follow up with the member When I started seeing a pattern between the battling users and the member, I sent a Steam message to the member asking if they were around so that we could chat. The conversation began with me asking about the rant they posted and whether or not they were aware that we allowed players under 16 to play on the servers, but not be members. They said they were not aware of this and seemed pleased they'd still be around. I thanked them for their attempt to help over the holiday. Then I mentioned that it was odd that all of the users were coming from the same IP address. It was agreed that was odd. When I said that there was another user that used that IP in the past, they stopped responding. A few hours later I got a message saying they were back and around to talk again but they'd closed the Steam chat window and lost the conversation. A few minutes later they began complaining that their own forum had been spammed. I'll admit I was curious, so I went to take a look. There were multiple \"users\" having a very similar battle across the other forum. The member had posted messages as the voice of the community that it had to stop or they'd issue bans. I was sick of the game at this point. I mentioned that I knew they were all of the users involved. I'd issued bans to the user accounts on the forum. I revoked their Viper membership and banned their main account. I also made a note that they could appeal the ban in 6 months time. I figured they'd forget about us. It takes a special kind of troll to spam their own site to try and convince someone you aren't a spammer. After effect of the bans At this point the member was less than pleased. There were vague threats made against me, the server, and anything they could think of that would offend me. I simply ignored the user on Steam. This didn't sit well, apparently. The user attempted a denial of service attack on the game servers, which our hosting provider mitigated. Part of this mitigation involves sending me a list of IP addresses that were attempting to connect to the server at the time of the attack, and part of the traffic that automatically triggered their counter measures. Unsurprisingly, there were only a few IP addresses. Geolocation says they are all in the same city. One of those IP addresses is the IP of the user. What is going to change? The biggest problem I see out of all of this is that we didn't have active admins during the initial wave of spam. I can't fault the admin team, after all it was Christmas. However, there have been a few requests recently for a couple more game server admins. I feel this incident just confirms that we need a couple more. In the coming days, we'll announce who our new admins will be. The member that has been removed has been banned from the servers and the forums. We've also destroyed two new user accounts they created since the spam wave. This is business as usual. I did offer them the chance to appear at the end of June. I suspect they will not. The appeal This section was added on July 7th, 2011 after the user appealed their ban It seems I was wrong. They filed an appeal to their ban. Of course it took place over another holiday (4th of July). Fortunately, our two European admins were still around while the rest of us were out enjoying some fireworks. The appeal was filed on the forums, as is usual. Community members stepped in to experess their unhappiness that the user would consider returning after what they did over Christmas. One of the admins that were around put a message in the admin area and asked for feedback from other admins. When I returned home after some amazing fireworks, I found I had an email from our hosting provider. They'd blocked another attempted denial of service attack. I logged into the forums to see what was going on and noticed that the appeal thread was not going well. The user were getting more and more upset at the community members who were rallying against their return. It eventually ended with a note saying that they hope Vipers never recovers from the DDOS. At this point one of the other admins locked the thread. A check of the IP address the user was utilizing to post the messages and the list of IPs involved in the attempted DDOS showed that the IP was on the list, and again they were all in the same city (and the same city as in December). Their appeal was rejected. They were informed they would not be welcome back. After this appeal, I don't believe anything will need to change on our side. The appeal process concluded faster than normal. Attempts to DDOS the community server makes the decision rather easy for the admins."},{"tags":"Technical Solutions","url":"multiple-ip-addresses-on-the-same-physical-network-card.html","title":"Multiple IP addresses on the same physical network card","text":"There are times when a server can be allocated more than one IP Address even though it contains only one physical network card. To associate these IP addresses with the server some manipulation of networking settings will need to be performed. The steps outlined in this walk-through are for RedHat based systems. This tutorial is for statically assigned IP Addresses (as a server generally will have). For this walk through we are going to add one additional IP address to eth0 . Navigate to cd /etc/sysconfig/network-scripts Copy ifcfg-eth0 to ifcfg-eth0:0 cp ifcfg-eth0 ifcfg-eth0:0 Now we need to modify the new file slightly so that it gets it's own IP address. Open ifcfg-eth0:0 in your favorite editor DEVICE=eth0:0 <-- Change this to match the new eth0:0 file we just created BOOTPROTO=none BROADCAST=x.x.x.x <-- This is the broad cast address for the subnet the new IP is on DNS1=x.x.x.x <-- This is the main DNS server you are using (example: 64.120.14.26) GATEWAY=x.x.x.x <-- This is the gateway address for the subnet the new IP is on HWADDR=<DO NOT CHANGE> <-- Don't change this from what is existing. The Hardware address is the same as the physical one IPADDR=x.x.x.x <-- This is your new IP address NETMASK=x.x.x.x <-- This is the netmask for the subnet the new IP is on ONBOOT=yes <-- Leave to yes OPTIONS=layer2=1 TYPE=Ethernet PREFIX=29 DEFROUTE=yes NAME=\"System eth0:0\" <-- Change to reflect new name of device Save your file with the new settings. Now we need to restart the networking service: service network restart When the network components come back up you should see your new device in the ifconfig command. To add more IPs, copy and replace values as specified above."},{"tags":"Technical Solutions","url":"fixing-myisam-crashed-tables.html","title":"Fixing MYISAM Crashed Tables","text":"For various reasons, MyISAM tables are known to crash. When this happens, the following message will be displayed: INVALID SQL: 145 : Table '{something}' is marked as crashed and should be repaired I've found this error occurs when MySQL is unexpectedly shut down - whether from a power failure to the entire server or if MySQL itself has issues and you use the kill command to stop it. Unexpected shut downs, especially while these tables are being used, do not make MyISAM tables happy. To fix this, you need the ability to stop MySQL in a controlled manner, and you need to know where the database files are stored. locate *.MYI This will return where all the MYI files are stored. In this example, I am using /var/lib/mysql/mysql/ Go to the directory of the crashed table using the cd command. Next, stop MySQL. This is to ensure the tables are not accessed while we perform our repair functions. If you don't perform this step, the repair may not succeed. service mysqld stop Next we are going to perform two repair functions. The first one may take a while depending on the size of your tables. myisamchk -r --force --safe-recover *.MYI The second repair step is used to ensure all table states are updated correctly and repair any minor indexing issues. It is likely that this step is not needed after performing the previous step, but it should only take a few seconds now. myisamchk --force --fast --update-state *.MYI Finally, restart MySQL service mysqld start"},{"tags":"Vipers","url":"automated-template-for-membership-applications.html","title":"Automated template for membership applications","text":"Introduction Not to long ago, applications for new membership in Team Vipers consisted of someone wandering onto the site, posting a few words and an administrator saying they were accepted as a member. This worked when we were a small community. We aren't small any more. We have 5 servers and hundreds of players a day. Each server has their own sub-community. There are players joining the forums than entire groups of people have never met because they play exclusively on one game server. Admins were also inconsistent in how (or if) they voted. Some admins didn't realize they could have a say, thinking it was a privilege granted to only the senior administrators. We've built a system to resolve many of these problems and to make the administration side easier. What's new? The new system presents all users with an application template. They fill out the form and the system handles the rest. The New Users and Applications subforum has been modified so that no one, except the bot, can create topics. They topics will only be created when the form is submitted. When a user applies to join Vipers, we will automatically include relevant information about the user as we know them: HLStats information: This helps us get a sense of where they play and how often they play. It will help admins identify users they should recognize based on the servers they frequent (because we all know certains servers are better than others ;) cough Vanilla Nest vs Crits cough ) Ban information: This will check if the user has any recorded bans in Sourcebans . It's important to know if the user has been banned previously. Known aliases: Pulling information from our chat logs and Valve's profile page, we can build a list of known aliases. This helps identify users that frequently change names but have been around a while. There may be other features we add in the future as well. Updated January 2012 I've removed HLStats from the servers and removed it from new application information. We have added a check of a user's Steam Reputation instead. After a user applies, they are put into a two week hold. During this week it is expected they will stick around the forums and learn about the community they just applied to. Even better would be that they had done this before applying. While this two week hold is in place, the administration team will be able to cast they votes in a separate sub-forum. They can hold administration specific discussions - usually details that are important for admins to know, but don't need to be public. Once voting is complete, if they become a member, the system automatically grants appropriate forum and server related permissions. Voting rules Three admin \"Yes\" votes and zero \"No\" votes grants the user membership immediately after the two week window has expired One or two \"No\" votes places a message on the user's application that the administrators are still considering the application Three or more \"No\" votes rejects the application Possible Responses Accepted by admin team If you reach a total of three or more admin \"Yes\" votes, and do not get three or more admin \"No\" votes, you will be accepted as a member of Vipers and automatically have your forum access modified. You will receive a message similar to this: Denied by admin team If you reach three or more \"No\" votes your application will be denied. This will occur even if you receive more \"Yes\" votes than \"No\" votes. Vipers is not a majority rule community. The decision has been made that if three admins do not feel comfortable accepting your application, you will not be granted membership. You will receive a message similar to this: Denied due to lack of votes To be accepted, your application requires a minimum of three \"Yes\" votes. If this can not be reached (and you also can't reach three \"No\" votes), your application will be rejected due to lack of votes from the admin team. This means that the admin team does not feel strongly either way about your application. Post on the forums. Play in the servers. Get to know our players and the community at large and then try again in a month. You will receive a message similar to this: Denied because of age Repeat after me: \"Age does not equal maturity.\" However, it has a very strong correlation. Over time we have learned that younger players tend to bring a lower maturity level that most of the community does not care for. As such, we've decided to set a minimium age requirement of 16. If a user indicates they are less than that, the system will reject their application with a message similar to this: Updated January 2010 This process has been in place for a few months now. It has gone very well. We've reduced the clutter in the applications forum. We've also seen the number of \"forgotten\" applications drop dramatically. Original Announcement The original announcement is posted here for future reference. Zephyr is an automated robot designed to improve our new member application process. The current process, which involves copying a template to a new thread and the potential for applications to become misplaced, is cumbersome and inefficient. The goal of Zephyr is to remove as much of the manual process as possible. Now, our new applicants will fill out an actual application online. The same information will be requested, but it will be in a more reliable format and will not require an applicant copy and paste anything between threads. This new application will be posted in the same forum as you're used to, and members will be freely able to comment and discuss the applicant via that thread. It will also display which date admin voting will be open, which is two full weeks after the original application post. This will hopefully cut down on any confusion related to the delay between application and voting. As another note, from now on Zephyr will be the only user capable of creating new threads in the New Member Application forum. As previously stated, members will be able to post comments on existing threads, but the only new threads will come from the application process. This will keep the forum cleaner and help prevent applications from becoming lost or forgotten about. Once again, Zephyr is an automated robot. It is not programmed to respond to comments or questions. Doing so will not get your question answered. As always, if you have questions or comments about the application process, Zephyr, or anything else, you're welcome to send them to any admin. We'll be more than happy to help. You may all bow to your Robotic Overlord now."}]}